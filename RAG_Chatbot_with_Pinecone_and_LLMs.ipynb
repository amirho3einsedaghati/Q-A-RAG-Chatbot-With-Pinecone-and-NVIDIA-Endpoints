{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50rGsWRD_13x",
        "outputId": "89268810-af7d-439a-f04c-6762c7a84996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: langchain-nvidia-ai-endpoints in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.3.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from langchain-nvidia-ai-endpoints) (3.11.14)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-nvidia-ai-endpoints) (0.3.47)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.18.3)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (2.10.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (2.27.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (3.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-nvidia-ai-endpoints) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone langchain-nvidia-ai-endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Ie_BMClUQIW0",
        "outputId": "7d3fd06b-11b8-4623-f547-609aee49ca4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "youtube"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bbceef87-33af-4eaf-96b6-9e675ef14bbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
              "      <td>Hi, welcome to the video.</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t3.0</td>\n",
              "      <td>So this is the fourth video in a Transformers</td>\n",
              "      <td>3.00</td>\n",
              "      <td>11.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t9.36</td>\n",
              "      <td>from Scratch mini series.</td>\n",
              "      <td>9.36</td>\n",
              "      <td>15.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t11.56</td>\n",
              "      <td>So if you haven't been following along,</td>\n",
              "      <td>11.56</td>\n",
              "      <td>18.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t15.84</td>\n",
              "      <td>we've essentially covered what you can see on ...</td>\n",
              "      <td>15.84</td>\n",
              "      <td>20.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbceef87-33af-4eaf-96b6-9e675ef14bbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bbceef87-33af-4eaf-96b6-9e675ef14bbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bbceef87-33af-4eaf-96b6-9e675ef14bbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f14ca50b-6fd7-41bf-8d4d-0e3a75b31c64\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f14ca50b-6fd7-41bf-8d4d-0e3a75b31c64')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f14ca50b-6fd7-41bf-8d4d-0e3a75b31c64 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               title                published  \\\n",
              "0  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "1  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "2  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "3  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "4  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "\n",
              "                            url     video_id                channel_id  \\\n",
              "0  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "1  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "2  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "3  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "4  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "\n",
              "                   id                                               text  \\\n",
              "0    35Pdoyi6ZoQ-t0.0                          Hi, welcome to the video.   \n",
              "1    35Pdoyi6ZoQ-t3.0      So this is the fourth video in a Transformers   \n",
              "2   35Pdoyi6ZoQ-t9.36                          from Scratch mini series.   \n",
              "3  35Pdoyi6ZoQ-t11.56            So if you haven't been following along,   \n",
              "4  35Pdoyi6ZoQ-t15.84  we've essentially covered what you can see on ...   \n",
              "\n",
              "   start    end  \n",
              "0   0.00   9.36  \n",
              "1   3.00  11.56  \n",
              "2   9.36  15.84  \n",
              "3  11.56  18.48  \n",
              "4  15.84  20.60  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "youtube = pd.read_json(\"hf://datasets/jamescalam/youtube-transcriptions/youtube-transcriptions.jsonl\", lines=True)\n",
        "youtube.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TiHslu0wbxQ",
        "outputId": "e3cb9942-0153-457b-ca5d-2d90963f261e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 208619 entries, 0 to 208618\n",
            "Data columns (total 9 columns):\n",
            " #   Column      Non-Null Count   Dtype  \n",
            "---  ------      --------------   -----  \n",
            " 0   title       208619 non-null  object \n",
            " 1   published   208619 non-null  object \n",
            " 2   url         208619 non-null  object \n",
            " 3   video_id    208619 non-null  object \n",
            " 4   channel_id  208619 non-null  object \n",
            " 5   id          208619 non-null  object \n",
            " 6   text        208619 non-null  object \n",
            " 7   start       208619 non-null  float64\n",
            " 8   end         208619 non-null  float64\n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 14.3+ MB\n"
          ]
        }
      ],
      "source": [
        "youtube.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ko2EyO-JxZGb",
        "outputId": "2229e55f-9c2a-43f8-de02-872683477964"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"filtered_youtube\",\n  \"rows\": 630,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Training and Testing an Italian BERT - Transformers From Scratch #4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2021-07-06 13:00:03 UTC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://youtu.be/35Pdoyi6ZoQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"35Pdoyi6ZoQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"UCv83tO5cePwHMt1952IVVHw\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 630,\n        \"samples\": [\n          \"35Pdoyi6ZoQ-t1509.72\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 583,\n        \"samples\": [\n          \"OK, so no, still not good.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 529.7972799420606,\n        \"min\": 0.0,\n        \"max\": 1835.96,\n        \"num_unique_values\": 630,\n        \"samples\": [\n          1509.72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 529.0089384007132,\n        \"min\": 9.36,\n        \"max\": 1839.76,\n        \"num_unique_values\": 629,\n        \"samples\": [\n          1508.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "filtered_youtube"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-467377db-c5c3-4321-a8ad-71689b56fea8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
              "      <td>Hi, welcome to the video.</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t3.0</td>\n",
              "      <td>So this is the fourth video in a Transformers</td>\n",
              "      <td>3.00</td>\n",
              "      <td>11.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t9.36</td>\n",
              "      <td>from Scratch mini series.</td>\n",
              "      <td>9.36</td>\n",
              "      <td>15.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t11.56</td>\n",
              "      <td>So if you haven't been following along,</td>\n",
              "      <td>11.56</td>\n",
              "      <td>18.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t15.84</td>\n",
              "      <td>we've essentially covered what you can see on ...</td>\n",
              "      <td>15.84</td>\n",
              "      <td>20.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t1818.96</td>\n",
              "      <td>And then what we'll be able to do is actually ...</td>\n",
              "      <td>1818.96</td>\n",
              "      <td>1831.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t1828.5600000000002</td>\n",
              "      <td>So, yeah, I think good result.</td>\n",
              "      <td>1828.56</td>\n",
              "      <td>1833.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t1831.5600000000002</td>\n",
              "      <td>I'm pretty happy with that.</td>\n",
              "      <td>1831.56</td>\n",
              "      <td>1835.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>628</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t1833.3600000000001</td>\n",
              "      <td>And thank you for watching.</td>\n",
              "      <td>1833.36</td>\n",
              "      <td>1839.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>629</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t1835.96</td>\n",
              "      <td></td>\n",
              "      <td>1835.96</td>\n",
              "      <td>1839.76</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>630 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-467377db-c5c3-4321-a8ad-71689b56fea8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-467377db-c5c3-4321-a8ad-71689b56fea8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-467377db-c5c3-4321-a8ad-71689b56fea8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb32cd7a-b3c4-4a08-bcc4-6796e61e1957\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb32cd7a-b3c4-4a08-bcc4-6796e61e1957')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb32cd7a-b3c4-4a08-bcc4-6796e61e1957 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2d733dfd-191d-42a7-bf36-e36e8d404251\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('filtered_youtube')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2d733dfd-191d-42a7-bf36-e36e8d404251 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('filtered_youtube');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 title  \\\n",
              "0    Training and Testing an Italian BERT - Transfo...   \n",
              "1    Training and Testing an Italian BERT - Transfo...   \n",
              "2    Training and Testing an Italian BERT - Transfo...   \n",
              "3    Training and Testing an Italian BERT - Transfo...   \n",
              "4    Training and Testing an Italian BERT - Transfo...   \n",
              "..                                                 ...   \n",
              "625  Training and Testing an Italian BERT - Transfo...   \n",
              "626  Training and Testing an Italian BERT - Transfo...   \n",
              "627  Training and Testing an Italian BERT - Transfo...   \n",
              "628  Training and Testing an Italian BERT - Transfo...   \n",
              "629  Training and Testing an Italian BERT - Transfo...   \n",
              "\n",
              "                   published                           url     video_id  \\\n",
              "0    2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "1    2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "2    2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "3    2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "4    2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "..                       ...                           ...          ...   \n",
              "625  2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "626  2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "627  2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "628  2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "629  2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "\n",
              "                   channel_id                               id  \\\n",
              "0    UCv83tO5cePwHMt1952IVVHw                 35Pdoyi6ZoQ-t0.0   \n",
              "1    UCv83tO5cePwHMt1952IVVHw                 35Pdoyi6ZoQ-t3.0   \n",
              "2    UCv83tO5cePwHMt1952IVVHw                35Pdoyi6ZoQ-t9.36   \n",
              "3    UCv83tO5cePwHMt1952IVVHw               35Pdoyi6ZoQ-t11.56   \n",
              "4    UCv83tO5cePwHMt1952IVVHw               35Pdoyi6ZoQ-t15.84   \n",
              "..                        ...                              ...   \n",
              "625  UCv83tO5cePwHMt1952IVVHw             35Pdoyi6ZoQ-t1818.96   \n",
              "626  UCv83tO5cePwHMt1952IVVHw  35Pdoyi6ZoQ-t1828.5600000000002   \n",
              "627  UCv83tO5cePwHMt1952IVVHw  35Pdoyi6ZoQ-t1831.5600000000002   \n",
              "628  UCv83tO5cePwHMt1952IVVHw  35Pdoyi6ZoQ-t1833.3600000000001   \n",
              "629  UCv83tO5cePwHMt1952IVVHw             35Pdoyi6ZoQ-t1835.96   \n",
              "\n",
              "                                                  text    start      end  \n",
              "0                            Hi, welcome to the video.     0.00     9.36  \n",
              "1        So this is the fourth video in a Transformers     3.00    11.56  \n",
              "2                            from Scratch mini series.     9.36    15.84  \n",
              "3              So if you haven't been following along,    11.56    18.48  \n",
              "4    we've essentially covered what you can see on ...    15.84    20.60  \n",
              "..                                                 ...      ...      ...  \n",
              "625  And then what we'll be able to do is actually ...  1818.96  1831.56  \n",
              "626                     So, yeah, I think good result.  1828.56  1833.36  \n",
              "627                        I'm pretty happy with that.  1831.56  1835.96  \n",
              "628                        And thank you for watching.  1833.36  1839.76  \n",
              "629                                                     1835.96  1839.76  \n",
              "\n",
              "[630 rows x 9 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: I wanna fillter all rows containing 35Pdoyi6ZoQ in the id column for example \"35Pdoyi6ZoQ-t0.0\"\n",
        "\n",
        "\n",
        "filtered_youtube = youtube[youtube['id'].str.contains('35Pdoyi6ZoQ')]\n",
        "filtered_youtube\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5nzvIOms_-1I",
        "outputId": "edf13dd3-f0c3-4bdc-f71d-bcaf797b2df0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi, welcome to the video.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "youtube.iloc[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhpxUphqAolS",
        "outputId": "59282a34-396b-46fa-ccea-49b50f2ebd27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(208619, 9)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "youtube.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iiH5xCKASBI",
        "outputId": "ca42520e-5136-4451-f0d6-319e86731963"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(207919)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "youtube['title'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ltUVFsDTAlky",
        "outputId": "040f921e-e3ae-4843-edd8-c02908cd45ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"youtube[~youtube['title']\",\n  \"rows\": 700,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"What Is Neuralink??! - TDBS 22 April 2017\",\n          \"[ML News] Hugging Face course | GAN Theft Auto | AI Programming Puzzles | PyTorch 1.9 Released\",\n          \"I TRAINED AN AI TO SOLVE 2+2 (w/ Live Coding)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 699,\n        \"samples\": [\n          \"2017-04-23 11:37:29\",\n          \"2021-02-19T16:11:18Z\",\n          \"2020-08-06T16:30:14Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"https://youtu.be/ToZyCA0uvoI\",\n          \"https://youtu.be/6_q9DbX35kk\",\n          \"https://youtu.be/udS2OPohs_s\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"ToZyCA0uvoI\",\n          \"6_q9DbX35kk\",\n          \"udS2OPohs_s\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"UCr8O8l5cCX85Oem1d18EezQ\",\n          \"UCZHmQk67mSJgfCCTn7xBfew\",\n          \"UCobqgqE4i5Kf7wrxRxhToQA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"ToZyCA0uvoI-t0.0\",\n          \"6_q9DbX35kk-t0.0\",\n          \"udS2OPohs_s-t0.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 685,\n        \"samples\": [\n          \"Howdy-dowdy.\",\n          \"What is going on everybody and welcome back to part three of our deep learning with?\",\n          \"Testing baby, one two three. Monday, you know what that means? Push-ups.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.67470046594988,\n        \"min\": 0.0,\n        \"max\": 120.0,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.0,\n          0.32,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.503232798304187,\n        \"min\": 2.18,\n        \"max\": 122.0,\n        \"num_unique_values\": 341,\n        \"samples\": [\n          17.28,\n          6.0,\n          11.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-28db6653-4635-4259-89d6-ba857aa169ae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
              "      <td>Hi, welcome to the video.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>Choosing Indexes for Similarity Search (Faiss ...</td>\n",
              "      <td>2021-08-09 15:04:10 UTC</td>\n",
              "      <td>https://youtu.be/B7wmo_NImgM</td>\n",
              "      <td>B7wmo_NImgM</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>B7wmo_NImgM-t0.0</td>\n",
              "      <td>Hi, welcome to the video.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>Training BERT #4 - Train With Next Sentence Pr...</td>\n",
              "      <td>2021-05-27 16:15:39 UTC</td>\n",
              "      <td>https://youtu.be/x1lAcT3xl5M</td>\n",
              "      <td>x1lAcT3xl5M</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>x1lAcT3xl5M-t0.0</td>\n",
              "      <td>Hi and welcome to the video.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2044</th>\n",
              "      <td>Build NLP Pipelines with HuggingFace Datasets</td>\n",
              "      <td>2021-09-23 13:30:07 UTC</td>\n",
              "      <td>https://youtu.be/r-zQQ16wTCA</td>\n",
              "      <td>r-zQQ16wTCA</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>r-zQQ16wTCA-t0.0</td>\n",
              "      <td>Hi, welcome to this video. We're going to have...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2478</th>\n",
              "      <td>How-to do Sentiment Analysis with Flair in Python</td>\n",
              "      <td>2020-12-04 14:00:03 UTC</td>\n",
              "      <td>https://youtu.be/DFtP1THE8fE</td>\n",
              "      <td>DFtP1THE8fE</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>DFtP1THE8fE-t0.0</td>\n",
              "      <td>Hi, welcome to this video on sentiment analysi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206636</th>\n",
              "      <td>Is Google Translate Sexist? Gender Stereotypes...</td>\n",
              "      <td>2021-03-23T15:59:17Z</td>\n",
              "      <td>https://youtu.be/J7CrtblmMnU</td>\n",
              "      <td>J7CrtblmMnU</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>J7CrtblmMnU-t0.0</td>\n",
              "      <td>So, you might have seen this tweet.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206816</th>\n",
              "      <td>The Man behind Stable Diffusion</td>\n",
              "      <td>2022-08-13T10:52:40Z</td>\n",
              "      <td>https://youtu.be/YQ2QtKcK2dA</td>\n",
              "      <td>YQ2QtKcK2dA</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>YQ2QtKcK2dA-t0.0</td>\n",
              "      <td>This is a mud, a mud is very rich, and he want...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207250</th>\n",
              "      <td>Neural Networks from Scratch - P.3 The Dot Pro...</td>\n",
              "      <td>2020-04-24T14:27:00Z</td>\n",
              "      <td>https://youtu.be/tMrbN67U9d4</td>\n",
              "      <td>tMrbN67U9d4</td>\n",
              "      <td>UCfzlCWGWYyIQ0aLC5w48gBQ</td>\n",
              "      <td>tMrbN67U9d4-t0.0</td>\n",
              "      <td>What's going on everybody and welcome to part ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207505</th>\n",
              "      <td>Extracting Training Data from Large Language M...</td>\n",
              "      <td>2020-12-26T19:42:56Z</td>\n",
              "      <td>https://youtu.be/plK2WVdLTOY</td>\n",
              "      <td>plK2WVdLTOY</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>plK2WVdLTOY-t0.0</td>\n",
              "      <td>Hi there. Today, we're looking at extracting t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208052</th>\n",
              "      <td>GLOM: How to represent part-whole hierarchies ...</td>\n",
              "      <td>2021-02-27T15:47:03Z</td>\n",
              "      <td>https://youtu.be/cllFzkvrYmE</td>\n",
              "      <td>cllFzkvrYmE</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>cllFzkvrYmE-t0.0</td>\n",
              "      <td>Hi, there. Today, we'll look at how to represe...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28db6653-4635-4259-89d6-ba857aa169ae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-28db6653-4635-4259-89d6-ba857aa169ae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-28db6653-4635-4259-89d6-ba857aa169ae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11f782a0-63d5-48ed-889c-e009178ed8e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11f782a0-63d5-48ed-889c-e009178ed8e2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11f782a0-63d5-48ed-889c-e009178ed8e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                    title  \\\n",
              "0       Training and Testing an Italian BERT - Transfo...   \n",
              "630     Choosing Indexes for Similarity Search (Faiss ...   \n",
              "979     Training BERT #4 - Train With Next Sentence Pr...   \n",
              "2044        Build NLP Pipelines with HuggingFace Datasets   \n",
              "2478    How-to do Sentiment Analysis with Flair in Python   \n",
              "...                                                   ...   \n",
              "206636  Is Google Translate Sexist? Gender Stereotypes...   \n",
              "206816                    The Man behind Stable Diffusion   \n",
              "207250  Neural Networks from Scratch - P.3 The Dot Pro...   \n",
              "207505  Extracting Training Data from Large Language M...   \n",
              "208052  GLOM: How to represent part-whole hierarchies ...   \n",
              "\n",
              "                      published                           url     video_id  \\\n",
              "0       2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "630     2021-08-09 15:04:10 UTC  https://youtu.be/B7wmo_NImgM  B7wmo_NImgM   \n",
              "979     2021-05-27 16:15:39 UTC  https://youtu.be/x1lAcT3xl5M  x1lAcT3xl5M   \n",
              "2044    2021-09-23 13:30:07 UTC  https://youtu.be/r-zQQ16wTCA  r-zQQ16wTCA   \n",
              "2478    2020-12-04 14:00:03 UTC  https://youtu.be/DFtP1THE8fE  DFtP1THE8fE   \n",
              "...                         ...                           ...          ...   \n",
              "206636     2021-03-23T15:59:17Z  https://youtu.be/J7CrtblmMnU  J7CrtblmMnU   \n",
              "206816     2022-08-13T10:52:40Z  https://youtu.be/YQ2QtKcK2dA  YQ2QtKcK2dA   \n",
              "207250     2020-04-24T14:27:00Z  https://youtu.be/tMrbN67U9d4  tMrbN67U9d4   \n",
              "207505     2020-12-26T19:42:56Z  https://youtu.be/plK2WVdLTOY  plK2WVdLTOY   \n",
              "208052     2021-02-27T15:47:03Z  https://youtu.be/cllFzkvrYmE  cllFzkvrYmE   \n",
              "\n",
              "                      channel_id                id  \\\n",
              "0       UCv83tO5cePwHMt1952IVVHw  35Pdoyi6ZoQ-t0.0   \n",
              "630     UCv83tO5cePwHMt1952IVVHw  B7wmo_NImgM-t0.0   \n",
              "979     UCv83tO5cePwHMt1952IVVHw  x1lAcT3xl5M-t0.0   \n",
              "2044    UCv83tO5cePwHMt1952IVVHw  r-zQQ16wTCA-t0.0   \n",
              "2478    UCv83tO5cePwHMt1952IVVHw  DFtP1THE8fE-t0.0   \n",
              "...                          ...               ...   \n",
              "206636  UCZHmQk67mSJgfCCTn7xBfew  J7CrtblmMnU-t0.0   \n",
              "206816  UCZHmQk67mSJgfCCTn7xBfew  YQ2QtKcK2dA-t0.0   \n",
              "207250  UCfzlCWGWYyIQ0aLC5w48gBQ  tMrbN67U9d4-t0.0   \n",
              "207505  UCZHmQk67mSJgfCCTn7xBfew  plK2WVdLTOY-t0.0   \n",
              "208052  UCZHmQk67mSJgfCCTn7xBfew  cllFzkvrYmE-t0.0   \n",
              "\n",
              "                                                     text  start    end  \n",
              "0                               Hi, welcome to the video.    0.0   9.36  \n",
              "630                             Hi, welcome to the video.    0.0   6.32  \n",
              "979                          Hi and welcome to the video.    0.0   4.00  \n",
              "2044    Hi, welcome to this video. We're going to have...    0.0  10.80  \n",
              "2478    Hi, welcome to this video on sentiment analysi...    0.0  14.32  \n",
              "...                                                   ...    ...    ...  \n",
              "206636                So, you might have seen this tweet.    0.0   6.88  \n",
              "206816  This is a mud, a mud is very rich, and he want...    0.0  12.10  \n",
              "207250  What's going on everybody and welcome to part ...    0.0  13.96  \n",
              "207505  Hi there. Today, we're looking at extracting t...    0.0  14.20  \n",
              "208052  Hi, there. Today, we'll look at how to represe...    0.0  13.56  \n",
              "\n",
              "[700 rows x 9 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "youtube[~youtube['title'].duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Hq-w4F7fAz6a",
        "outputId": "64464e06-5bb3-4c4e-ef74-9533cc056298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 382 ms, sys: 18.3 ms, total: 400 ms\n",
            "Wall time: 403 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"grouped_youtube\",\n  \"rows\": 700,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"DeepMind&#39;s AlphaFold 2 Explained! AI Breakthrough in Protein Folding! What we know (&amp; what we don&#39;t)\",\n          \"Short Term vs Long Term Satisfaction\",\n          \"NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"It will change everything. DeepMind solves 50 year old grand challenge. The game has changed. DeepMind's latest AI breakthrough achieves historic new milestone, helps solve how diseases invade cells, improve protein folding prediction. AI breakthrough it also wipes your butt automatically. It is the newest DeepMind big publication. Actually, it's not a publication yet. But so what happened and I'm sure you've heard this is that every year there is this competition of protein folding prediction. So proteins are the structures that fold in a given way. And we'll go into that in a bit. But basically every year there is this competition. And the results of this year's competition came out. And they looked something like this. Namely, every entry here you see is a team participating in that competition of protein folding prediction. And there is one team which is DeepMind's system alpha fold two, which completely dominates all the others to the point where the problem is now considered to be solved. Now solved in this case, simply means that you're past a certain number in this in this test set. And if you're past that certain number, your predictions are useful enough so that other scientists can basically take them and base work on them. So that's what it means for this protein folding problem to be solved. Now we don't have much information on alpha fold two yet other than it's really good. And like a blog post and a bunch of advertisement videos by DeepMind, they are writing a paper on it. But today I want to go into this blog post, maybe parse out what we can gather from that blog post. And I also want to go actually through the alpha fold one paper. So as you can see, the performance here increased drastically with alpha fold two, but you know, guesses are high that the system is going to be somewhat similar to alpha fold one of which we do have a paper. So today we'll go into alpha fold one, we'll go into some speculations of alpha fold two, I can already give you my speculation, it's transformers, it's attention, that all of a sudden made this big jump together with probably a few other improvements to the alpha fold one system. Basically, transformers continuing to dominate the entire field. So where do we start? It's probably best, by the way, if this is not a great meme template, I don't know what is just saying, just saying. Yeah, so let's actually start with the problem itself. I realize if you're here, you're probably a machine learning person, might not know too much about protein folding. So these things here are computer representations of proteins. They don't really look that way, but sort of similar. A protein essentially is a chain of amino acids. So an amino acid, where do we have this right here? Amino acids are these what they're called basic building blocks of life, since the proteins, proteins are what make the cell do things. So protein are sort of the workers in the cell, they are used as signaling molecules, receptors, they are parts of your muscles, that actually the parts that move are proteins. So they they are all the work doers. Whenever something needs to work in a cell, do mechanical or work, proteins are involved. And amino acids are the building blocks of proteins. So each amino acid has an has a given a certain common structure. And there are 21 of them. So all the proteins in the world are simply made out of chains of these 21 amino acids. And these chains they are formed in. So there's always this sort of body that can link up to other bodies of amino acids. It's very similar, if you maybe know how DNA is structured, is a very similar concept, except in DNA, there are four different bases. Here, there are 21 amino acids. And each amino acid is a little bit different in each amino acid has like a tail that hangs off. So the tail can be no look like this, or it can look like this, like as with a side chain, are there is there one where it's like, maybe a cyclic one, I'm not sure maybe it can look out here, or it can have sort of no tail at all. I think that's the case for glycine. So the important part is depending on these on this tail, the properties, the chemical properties of the amino acids are different. And then what happens next is really interesting. Once this amino acid chain is built in a in this. So this is the central dogma of modern biology is that you have DNA. And DNA is translated to RNA, sorry. And then it's translated to. So it's read off copied to RNA, which is sort of a DNA clone. And then the RNA is translated into the amino acid chain. And there's always three, three pieces of DNA mapped to one amino acid. This is very much it's like a compiler. Notably, the interesting part is that these steps right here, this compilation steps are done by proteins. So there are proteins that do these things. So nature in a very real sense is its own compiler. So this here you can see as like the binary. And this here is like the source code. But what happens once you build this chain of amino acid, and you set it out into the cell because of these different properties of these side chains, they're also called residues, this chain begins to fold. And so this is, if you know a bit of chemistry, you might know that these are these are sort of atoms that are linked with covalent bonds in this case. And it can be that part of this chain is rather like electrically negatively charged. And here part of this chain might be like electrically positively charged in a given place over a given other place. And it also depends on the surrounding medium, of course. And that means that in this case, for example, these two things will attract. And so if you release this amino acid chain, what you're going to get is sort of a bend, where now the the chain sort of bends and these two this chain right here, this tail goes like here, this tail goes like here, I'm sorry, if there is no, if there is no, if there is no, I don't even know what to call it, pyrene rings or something like this. There isn't an amino acid with that, I apologize. But the point is that these two things attract and sort of form this shape. And this shape is very important. We know that proteins and proteins consist of, it can be hundreds, thousands, tens of thousands of these amino acids in a chain, the proteins function is, interestingly, largely determined by its structure by its 3d structure, not necessarily by the actual amino acid. So technically, you can substitute amino acids for each other. So this amino acid here can be could be substituted for another amino acid that maybe isn't the same, but is has the same properties of its side chain, such that if the structure is still the same, the protein would perform the same function. So that that is, is very special property of proteins, namely their 3d structure, largely determines their function. So for example, in this step here, when you read off the RNA to the DNA, as you know, the RNA is sorry, the DNA is like this double strand of connected base pairs. And in order to replicate the DNA or to read it off, there is a there more or let's call it there's also this step of DNA replication, right where you copy the DNA in mitosis. In order to do that, you need to split off the two strands, you need to split it up, because you want to get like a protein needs to get here to actually read it off. For that there is a protein, a specific protein that will insert right here to split up the DNA, which is called a helicase. And that really is very important how that protein is shaped. So the shape needs to be actually such that it kind of removes these bonds from each other. So the shape is very, very important for a protein. And conceivably, you could build a helicase from many, many different amino acid sequences, as long as it has the same shape. Now, I think something like something like fundamental like a helicase is probably conserved in the evolutionary tree. But I hope you get the point, the shape is super duper important. Now, the shape isn't just arbitrary, there are so the amino acid chain is called the primary structure. And then the first thing that happens is that two very distinct kind of sub shapes appear. So often repeating shapes, these things I think are called alpha helix, helices, or helix, this is a helix. And this here is I don't know what's in English, it's probably called a strand or something like this. These are like long sheets, like, I think they're called beta strands. And these things form these are often repeated sequences. And then the third, the tertiary structure is when the whole thing starts to kind of fold on itself and so on and give itself the the final structure. So this is part I guess, of the RNA polymerase, which is the molecule that reads DNA and outputs RNA. And there are many, many, many proteins. Now, since the shape is so important, it is vital that we know of it, right. And technically, technically, this is what why this problem is 50 years old, I guess, they say it's a 50 year old problem. I think that's due to the fact that 50 years ago, a Nobel laureate said the following, since a protein is fully determined by its amino acid chain, and since the you know, I mean, acid chain determines the structure that is going to go because of these kind of chemical properties, it should be possible to read in the amino acid sequence or read in the DNA sequence, we know what amino acid sequence results, and output the shape of a protein. However, this is an extremely complicated problem, it turned out to be because they're very subtle interactions, they're not always the same, it depends, right, like somewhere out here, there could be some amino acid with like some weird chain that, you know, everything folds on itself all the time. So at some point, these get in contact, and they change is kind of the local properties here. So this is a very, very difficult problem to solve. And people have have sort of tried to do this. And now apparently, deep mind the first system that does this to such a satisfaction that it's beneficial. Alright, now I lost my train of thought. Yeah, so the shape prediction, what happened so far is what you'd have to do is you'd have to sort of do this, determine this experimentally. So you'd have to take these proteins and crystallize them, and then like shoot x rays at them, and then infer the structure, you can you can do that from crystallized proteins, because I think it's due to crystals are like very regular accumulations of proteins. So if you look at a snowflake, that is, if we knew nothing about the water molecule, that it's like H2O, if we knew nothing of that, we could just look at a snowflake and determine this structure, this this these specific angles here from the snowflake. We would just look at the snowflakes. And if someone tells us, look, that's all the same material, that's all water, we could infer what the water molecule looks like, just by analyzing snowflakes, because they're crystals. And the pretty much the same here is you build you make crystals out of these materials, you shoot x rays at them, and then you sort of reason over the patterns that come out. This is very, very difficult, very expensive. And so to solve this problem computationally, is super important. I will get to this graphic in a minute. This is sort of the only thing we know about alpha fold two is this graphic right now, because they have not yet released the the paper or any descriptions of the model, as I said, but what we'll do is we'll go into alpha fold one. So this is alpha fold one. And alpha fold one was participating in the same competition two years ago and was already dominant there, but not yet dominant to the point of having quote unquote, solved the problem just better than other systems. So this is the basic structure of alpha fold one. So what do you what do you have right here? Let's let's give us ourselves an overview. So the overview is the following. There are two different stages to this algorithm. Stage one is over here, and stage two is over here. Maybe it's easiest to start with stage two. So the output of stage one is this thing right here, a distance and torsion distribution prediction. So this this this matrix here that's kind of tilted on its side, I believe there are more down here, right? Okay. So what you do right here is you you take an amino acid sequence and you line it up right here, you line it up. This is the amino acid sequence is a bit harder if there's like a split. But let's just say a protein is actually there can't be a split. Sorry, that's in the amino acids. I'm dumb. So a protein is sing a single chain of these amino acids. There can be multiple sort of parts to a bigger protein conglomerate. But there is this chain, you line it up here and here. So now we're building sort of a pairwise matrix between the sequence and itself. Okay. And this pairwise matrix is going to be a distance matrix. So what we are going to do is we're going to input some features about this sequence of amino acids, right, that's what we get as an input. And we're going to predict for any pair, right, so we have the sequence. And we're going to predict for any pair, how far are they apart? So of course, here, the answer is always kind of zero, they're zero apart. But you might say, you know, these two are five apart. And these two here are seven apart. But these two here are only one apart. So it's reasonable, you know, that the final structure the these two are close together. We don't worry about close together right now, we just worry about for each two will predict how far they are apart. Okay, so this is you can view this as you know, a machine learning problem, right, you have an input, see, you have a sequence, and you simply want to predict the distance matrix. So here you can see that in fact, you can see the top and bottom one is the predicted and one is the real, I don't even remember which one's which, you can see that this system does a pretty good job at that there are minute differences. If you really go look like down here, you can see a bit of a difference. Over here, there is a bit of a difference. But in general, this system does a pretty good job. So this is the output of stage one is this matrix, it's a bunch of other, it's like also the torsion angles and so on. But the main thing is you predict the distances between those two. That's what you take as a input to stage two. So what stage two does is stage two builds a model of this molecule. And the model is sort of a differentiable geometrical model. So they say they where is it this I don't get these nature papers like they're split into two parts, but then they are they largely say the same things. I am absolutely confused by them. So we're going to jump around a fair bit. They say we parameterize protein structures by the backbone torsion angles of all residues and build a differentiable model of protein geometry to compute the coordinates for all residues. And thus the interresidue distances. So what they do is essentially, they build a computer model of these amino acids. And these are parameterized by the torsion angles. Now the torsion angle is simply the angle between any two of them. So this would be like a torsion angle of 180 degrees. And then if it folds like this, it will be torsion angle of 90 degrees and so on. And you need two torsion angles because you're in 3d. But essentially, the torsion angles determine the structure of the protein. So it's one way of parameterizing it. So they built a differentiable model, a differentiable model of protein geometry. Okay, now the important thing is that they don't do any learning with this differentiable model. The purpose of this differentiable model is such that what you can do now, if you have a differentiable model, you can run gradient descent. So imagine they pretty much lay it out right here. So they have the x, x is x is the output of your differentiable geometry, right of your torsion angles, let's just call it this Greek letter phi, psi, whatever. If x is the output, and now x goes into your loss function, so x goes into your loss function, and the loss function simply compares x to the predicted x, okay, so the loss function will take in x, and it will compare it to the x that you predicted from from this thing here. Okay, so we start off with a flat chain, maybe, actually, I think we start off with some initialization, because they also predict the torsion angles directly, right here, they're predicted torsion angles direction, and that's what we initialize from. But let's just say we initialize from the flat chain. And then, because this is differentiable, we do so your your L, your L is x minus x prime, okay. And what we do is we derive the loss with respect to the angle to the torsion angle. So what and we can do this since this is differentiable. So now we know how do we need to change the angle, which is this thing right here, in order to make the loss smaller, right? And maybe it says you need actually you need to turn it down, right? Make the angle smaller. And we do that, okay, cool. Now it's only 90 degrees. And then we do it again, and again, and again. And you can see that by changing all the angles, such that this loss is smaller, we end up through steps, step, step, step, step, step, we, we in our computer model, we sort of replicate this process that happens in nature, where what we feed in is how far any two amino acids should be apart. And by running gradient descent, just gradient descent on the torsion angles, we figure out what do the angles need to be in order to make this happen? Okay, so first, we predict all the distances, and then we figure out how do we need to set the angles such that these distances are fulfilled? These are not true distances, these are predicted distances, right? So everything depends on how well we can predict these distances. But once we have them, we can sort of replicate in our computers the process as it happens in nature, except in nature, the the whole folding is dependent on these all these chemical interactions and so on. And now we do none of this, we simply look see how do we need to fold in order to make these distances in our computer model, like these like the distance between this and this, and this and this, any two distances may agree with the distances that we have predicted right here. And you can see that over time, this as you run gradient descent, this goes up, this this TM score was up the root mean square distance goes down between then you of course can compare it if you have a test set with stuff that people have already figured out, you can analyze these metrics and see that indeed, you do get the correct folding. It's also pretty interesting that so here in blue and red, I believe you have Yeah, exactly. So the the helix in blue, and the strands in red. So in this case, you from if you have this folded structure, partially folded structure, you can already see that these sort of substructures emerge like this is a helix, right, as you can see, and then you sort of made this may be a strand and so on. There are ways to heuristically classify that. And you can see that if you look at the database, right, you can see that this here is the strand, these are helices, and this is a strand and these are here, this is a strand and so on. And you can see that the model here is what the model thinks at the beginning, it doesn't get many things correct, though it does some, but then over time, it sort of refines its guesses until at the end, it's pretty much, you know, equal to what the to what the database to what the true sample is. And here is simply the distribution of, I guess, confidence about these things, and the the torsion angles right here. So it, as you can see, this two step process is the key here to do that. Now, AlphaFold2 conceivably probably changes this a little bit. But again, we're not sure. The step one right here is a deep learning system. So step two is simply a gradient descent procedure that you run at inference time, right? This at training, you can you can just do step one. So step one is, is the machine learning bit. So the goal is to output this distance, this distance tensor right here. And there are more things than distances, as we said, there are torsion angles, and so on. But ultimately, you want to output this distance matrix. And how do they do it, you can already see it's a deep neural network. So you want to build a input data point, let's say, of l by l, which is sequence length by sequence length. So you want to collect some features, you don't know the distances yet, right? But you can collect some features that are either either pairwise features between these two things, right? So here, maybe this is I don't know, leucine, and this is what's a different amino acid, glycine. And in here, you want to put features, maybe it can be features for that position, right? Maybe leucine here is at the 100th position in the in this particular protein, and this is at the 90th position. So we want to put in some features that of that that you can derive from a data set, you can put in correlation statistics in general between these two amino acids, you can even put in just single features. So you have these tiled l by one features, which is just features for the sequence itself, not pairwise features. But what you do is you simply replicate them along along any given dimension right here, you always put the same features, this is very common in convnets. And you can even do a scalar feature. So there are some scalar features. And what you would do is you would simply fill an entire plane with that scalar feature, all the same number, it's just easier to do it like this, because it fits into the convolutional architecture well. So you want to provide all kinds of features and the features they provide are, you know, plentiful, and a lot of them do introduce some domain tools, domain expertise, and so on. But once they have that, they simply take that sort of image with many, many channels, and they predict this image if you want. So it's just an image to image translation problem. And they do this via a convolutional neural network. As you can see, there are 220 residual convolutional blocks. Now, I assume that most of the viewers of this video are familiar what convolutional neural networks are, if not, I'm deeply sorry, but we'll not go into that. But you can see they sort of they tile this tensor right here, and they tile it differently from from from instance to instance. So they tile it, they in the training procedure, they always tile it differently. That's a form of data augmentation. But ultimately, you slide over this image with this 64 by 64 ConvNet, and you produce the image on the right, you can see an inherent weakness of these approaches, namely, that this thing can only ever look at 64 amino acids at a time. So now that can, that can be the same if you're on the diagonal of this, let's say, let's say this is not 64 by 64, but three by three, right? If you're on the diagonal, you would only consider three amino acids and their interactions with each other, right, any to any interactions with each other. If you're off the diagonal, what you would consider is maybe these three amino acids and these three amino acids, and you would only consider you consider features for maybe for those three, but interactions only in between, like the these not interactions actually within the same amino acids. So you're the thing that you can look at any point in time is going to be very limited, right? And these so these distances that you get out here, they necessarily cannot directly depend on, let's say this amino acid right here, you always have this limited view of your protein, that sort of local now, people argue that that's actually enough, if you look at maybe the green connections right here, in order to establish them, what's most important is the vicinity of these of this amino acid and the immediate vicinity of this amino acid. And of course, the interaction between those two vicinities, but it is quite conceivable that this green thing down here being so close will actually sort of push the two apart and sort of do this interaction, which, in my understanding would not be covered by a system like this. And that's where alpha fold two, I believe is is one point where it makes the big gains that it does. Now the features that go in here, as I said, they are, they're quite plentiful. One of the more interesting features is this MSA these multiple sequence alignment, and I believe they're they're up right here. Yeah, sequences. So here they introduce them in recent years, the accuracy of structural predictions has improved through the use of evolutionary covariation data that are found in sets of related sequences. sequences that are similar to the target sequence are found by searching large data sets of protein sequences derived from DNA sequencing and aligned to the target sequence to generate a multiple sequence alignment. Correlated changes in the positions of two amino acid residues across the sequences of MSA can be used to infer which residues might be in contact. So what what this I've searched out one of the papers right here and this is from a paper called improved contact prediction proteins using pseudo likelihoods to infer POTS models. The entire basis here is that here is your chain of amino acid that you're considering. And this is you, this is the human and they actually have one like a very similar graphic in their blog post, but we'll draw this ourselves. I'll just kind of sort of copy it. And what you do is you go and look into your database, right? This this is the amino acid sequence and each amino acid can actually be abbreviated by a single letter since there are 21. And luckily, the holy alphabet creators have given us what 26. So that fits. So each of these can be done by like s, y, c, m, d, and so on. Can be then you go look into your database and your database is of sort of all of life. And you go look for similar sequences. And there are tools that you can very quickly see through databases and get out similar sequences to yours and that those are sequences that are overlapping in amino acid sequence, right? So you could find in the fish, this is an alpha, this is not a fish. In the fish, there is a similar sequence right here in the iron, like this is okay. In the whatever this is, this might be a horsey. No, this is not a horse. Let's make an alligator out of this. So in the alligator raw does the alligator have there might be a sequence and so you get the point my drawing skills are to to be criticized in another video. So you search for all of these similar sequences just by by amino acid sequence. And from the correlations, you can derive something for example, I've already told you that sometimes you can substitute an amino acid in the sort of function of the protein isn't really affected. And this may be what you can see right here. So in the human, this is maybe a D, but or sorry, maybe this here, it's a C. But in the in the let's call this an M. In the fish, it's a C too. But you know, in the alligator, it's a P and in the cockroach, it's K and so on. You can see that maybe if the alignment is good, right, this is sort of from the same protein or from a protein that does maybe the same thing in these life forms because life is continuous. Often these things are preserved or slightly modified. So here, there are variations that happen in life, right mutations, variations. And so we can safely maybe assume that you know, a K, whether there's a K or a P or a C in this particular point, it doesn't really matter, the shape doesn't seem to be too affected. Okay, that's so that's step one. And now, so this might be this this protein, this amino acid right here, you see, whether it's this chain, or whether it's this chain, maybe doesn't really matter for the function of the protein. However, if you look at two proteins that are in contact, what needs to happen? So if my protein here has this chain, and the other protein has has sort of is in contact, that means there is like a chemical interaction between the two, okay. So now if a mutation happens, if a mutation happens, and the protein is still functioning the same way, but the mutation happened, let's say, it's now this right here, that must mean the shape is still the same sort of, and that must mean that probably, if one of them changed, the other one probably changed, sort of analogously at the same time, because structure is preserved function is preserved. So structure is preserved. And since structures determined by chemical interactions, one of the parts changed, that means probably the other part has changed as well. So maybe now this is sort of this chain right here. So what you would expect to see in the statistics is that if one changes, the other one changes accordingly. So there can be variations, right, there can be mutations. But if the mutation happens in one of them, a corresponding mutation should happen in the other one as well. Otherwise, the protein would be nonfunctional and the organism would sort of die. Not always, but you know, this is kind of a statistics game. And this is what you see here, like the fish has an S like the human and an H right here, but the alligator has an F and a W right here. And then in the cockroach, you see the S and the H again, and so on. And here down here, you see the F and the W again. And this is an indication that these, the correlation here is an indication that these two things might be in contact with each other. Now, there have been systems, for example, in this paper right here, that directly go from these statistics to contact predictions, and so on. Alpha fold simply takes in this stuff as features. So this right here, all of this, there can be I think they derive 488 features from this. So this goes down here. I think they say it again, as I said, this is confused, like here, article stops references, article starts again, thanks. And they like say almost the same things. It's just a little bit more detailed, it's not longer. So here, they derive 484 features from these multiple sequence alignment for each residue pair, right. So in our big tensor right here, right here, each dot each thing right here already now has 400. So each one of these already has 484 features. And then some more, right, this is already this is from the MSA, but then more features. So they incorporate lots of features right here. Where are we at here, incorporate lots of features. In addition, we provide the network with features that explicitly represent gaps and deletions. They also represent scalar features, and so on. So here you can see they have scalar features, sequence length features, amino acid type profiles, HH blitz profiles, these are all sort of these comp bio tools, these genetic tools. And so on. You also have sequence length features. These are these 484 features and so on. So these are all akin, there are some positional, one of these acts as positional encodings, so on. So lots of features, input convolutional network output, the distance matrix. And that's that, right. So there you have the inputs, the distance matrix from the distance matrix, you can run gradient descent to get the protein structure at inference time. And they make some pretty cool points. Not only do they compare the distance matrices, but they here is the not only the single prediction for the distance, but they of course, output a probability distribution, they've been all of these distances, they output a probability distribution. And you can see that the black line in these histograms. So this is, this is for a particular thing. This is for this, this red line, this red row right here, it's the extraction. So it's for one of the amino acid, the distribution of probabilities of distance bins with each of the other ones. So this is number 29. And we look at the distance between number 29 and 123, and so on. The black line represent, the represents, I think, eight angstroms, which is generally considered the barrier for being in contact or not being in contact. And here, it's colored in blue, if not in contact and in green, if in contact, and the red bar represents the true distance. And you can see this is pretty accurate. So whenever the network predicts blue, usually the red line is on the right of the black line. And if the network predicts, no, sorry, this green and blue is the ground truth. So whenever it's blue, the network's distribution is usually shifted towards the right. And whenever it's green, the network's distribution is shifted towards the left. There are some failure cases, as you can see right here, the network predicts a higher distance than the, than the, the truth, right. You can also see what's pretty interesting is that the most accurate predictions sort of the highest confidence, the smallest variation in distribution are around here, which is exactly around. So 29 would be in the middle right here. And that's where we find the most accurate predictions, of course, since local local distances are much more easier. And then as you go farther away, you get less sure. And this is a cool thing. So here you can see model prediction versus true distance fits fairly well. But you can also see that here they plot the standard deviation of their prediction. And you can see that the the means are very close, but the higher the sort of standard deviation, the less sure the model is. So there seems to be a there seems to be like a built in confidence metric, right. So you can see the distance error it makes here are bigger. And also its standard deviation is bigger at the same time, which means that you can sort of look at the standard deviation of this distribution right here. And that is an estimate for how sure how confident the model is in its prediction. And apparently, that's something that in alpha fold to the the model relies upon very, very crucially. So here you these are just the on the bottom, you see one of these residual blocks here, more distance matrices, they do a lot of analysis in this article, which is pretty cool. So you can go into it fairly far. They also have look at what the network pays attention to. And it makes a lot of sense, like it pays attention to kind of these these helices, and then these interactions between the helices and the parts where it's close in close contact with, and so on. But now we want to go into alpha fold to alpha fold to now the what we have isn't much we have this graphic right here, which is also in the article, it's probably better we go to the blog post to the blog post is like a fluff piece, saying we, they are going to publish a paper. But of course, they don't have it yet, because we've just gotten the results. Yeah, they have they have these these cool these videos were like, ah, so good. As I said, I've, like, there's so many Twitter threads with, I'm not usually up for the hype, but this is the best thing and so on. And everyone's everyone's hyping. And I thought, is it really up to me to be the grumpy one here? But then I couldn't find anything to be grumpy about. So this is what we what we get. Let's see, it's it's deep mind. I expect them to not fully maybe release the code, maybe they will. But in alpha fold one, they've released like half the code, which is already pretty cool. So there are open source implementations based on that. So, again, nothing to be grumpy about. Alright, so what can we what can we say, they say, a folded, a folded protein can be thought of as a spatial graph. And then this is kind of a new word they introduced. But ultimately, it's simply this distance matrix that we've seen before is a representation of that spatial graph, right? It's simply a graph of nodes and the edges say whether or not they're in contact or respectively how far they are apart, where the residues are nodes and edges connect the residues in close proximity. This graph is important for understanding the physical interactions within proteins as well as their evolutionary history. For the latest version of alpha fold used at CAS 14, that's this challenge, we created an attention based neural network system trained end to end that attempts to interpret the structure of this graph while reasoning over the implicit graph that it's building. I look this, it sounds like this, this is fluff, maybe, I don't know, but this here, attention based, okay, so I'm going to guess for sure that they've replaced this convnet with an with a transformer style with an attention, attention layer or multiple attention layers. They say it uses evolutionary evolutionarily related sequences, multiple sequence alignment and the representation of amino acid residue pairs to refine this graph. This is this is what we've already seen. So use these other sequences plus like a lot of stats that you can gather from the data sets on amino acid pairs in order to develop this, this graph and the graph is distance, the distance matrix, or other things we'll see in just a second. They say by iterating this process, system develops strong predictions of the underlying physical structure of the protein and is able to determine highly accurate structures in a matter of days. Additionally, alpha fall can predict which parts of each predicted protein structure are reliable in the future using an internal confidence measure. Again, this is something that we've already sort of seen in alpha fold one that there is sort of an internal confidence measure. And the part here is they say by iterating this process, which could mean that it's no longer just this two stage approach, but it could be an actually fully cycling approach that sort of goes back to the neural network to refine the structure that it's building with the gradient descent procedure. It's entirely possible. So this is the graphic of alpha fold two, you can see at the very beginning, you have protein sequence. And at first, you have this embed and outer embed and outer sum, which I'm going to guess this is just kind of features for pairs or individual amino acids. This this is correlation statistics from your data set, it can be, you know, chemical properties, whatever it just a bunch of features that you can attach to each of these amino acids in the sequence, right. The other path here is this genetic search and embed. So this is what we've already seen with the MSA embedding, I saw I told you they have the same graphic. So there's human, there's fishy, there's rabbit, and you simply search for sequences in your database, it could even be from other humans, right, that are similar. And from that from those, you can also derive features. So here is where I'm a bit confused. You can see they build up this again, this square matrix right here. I mean, this, it already screamed attention before, right. So I'm going to guess they no longer limit themselves to the maybe, maybe to the 64 by 64. Maybe they do something bigger. Maybe they use local attention, who knows, but I'm going to guess they use attention to. And these, this here is simply given by an attention layer of some sort to go into the next to just this is basically, I would guess this is a big transformer right here. The interesting part is that it appears to interact much like much like the original transformer, maybe encoder decoder. Here, they pass information around. So this top thing isn't amino acid sequence to amino acid sequence like to itself, but it appears to be a matrix that you build up between the amino acid sequence and these sequences you built. So I would guess that they are no longer, let's say, happy with simply inputting the features of these algorithms that go over these other sequences. But now they also want to sort of put these features through through steps of transformations. So again, I would guess this is an attention layer. And how can we interpret this matrix? As you can see, this matrix relates individual amino acids in the sequence to other species. So I would guess that this square here represents something like how important is this particular location in the chain, which is a purple thingy in the human? How important is that in the in the in the chicken? Or how related is that to the chicken at that particular position? Or as a whole? I don't know, probably DeepMind doesn't know, like they probably just ship these features in here, right? And then they just ship it through transformers, they pass information around, I don't know whether it's just in this direction. And then in this direction, or whether there's like an arrow right here, conceivably, but in any case, it seems like they've replaced what was a ConvNet. So no longer friends with ConvNet new best friend is transformer. And then at the end, you see what they get out is these pairwise distances again. Now, it's also not really clear, because I would expect maybe an arrow going like this, if they again, use these pairwise distances to predict the structure, I don't know, okay. Or if that's just a side output, I would guess they still actually use the pairwise distances. And the confidence score, again, you can it might be something very similar that we've saw again, being the sort of standard deviation on the predicted distances, but they could also refine that. And then the last thing is, I don't know if this iterative process is simply referring to there being multiple layers of this attention and passing around. So the passing around will simply be like, you stack the representations on top of each other. I don't know if this is the iterative procedure, or if there is actually like the structure module actually sort of builds the structure and then goes back. And then you consult the neural network again, and then you build some more of the structure, and so on. I can't tell right now, it's quite conceivable that they they do like that the search here is not only gradient descent, but is actually informed by the neural network. So you sort of go back and refine though I don't know, there doesn't seem to be any features in the neural networks that would represent that would represent whatever you could read from a partially built 3d model. So, you know, the boring guess is that the part two is very is is a lot of the same, but there could also be substantial improvements in that part. Alright, I hope this was this was sort of a good overview. So, as I said, the paper isn't out yet. If you want to cite this, I guess you can you can refer to the blog post and here they say, until we've published a paper on this work, please cite high accuracy protein structure prediction using deep learning by these people. I just want to say that I just want to highlight shout out to to Anna, who was educated right here. She was an intern. So in a way, I'm actually saying that this is my discovery and I take full responsibility for it. You're welcome world. Shout out to Anna. Very nice job. Good work. Good work to all of these people. And yeah, I hope that was enough. If I got something horribly wrong, please tell me in the \",\n          \"What's going on y'all? Episode 103 of the Daniel Burke Show. Sorry it's been so long since the last one. I know I said I sort of wanted to do these weekly or fortnightly or something. I'm still trying to work out that routine. No excuses though. We're gonna do one today. And what's the topic of the day? Well since it's a Friday, I figured I want to do something fun. Because Friday, F, fun, you know, you know how it goes. Everyone's happy on a Friday. I'm happy every day, well I try to be at least. Actually, speaking of that, I had an article published recently on Thought Catalog. It's called How I Stop Feeling Depressed. So although I'm positive and optimistic and outgoing, fun, whatever, 99.999999% of the time, I still have those days where I feel down and sort of not wanting to do anything. So I wrote an article on what I do to stop feeling down, stop feeling depressed. And Thought Catalog, the editors from Thought Catalog, reached out and republished it on their website. So I was pretty happy with that. It's the first piece of writing I've actually had republished somewhere. Either the Medium or my blog or something like that. So I'll link it in the description or in the show notes so you can all check it out. And show some love, show some hate, I don't mind, as long as you read it. But what's today's episode about? Yes, Friday, it's fun. We're gonna make it nice and quick. I figured, let's do 5 minutes of fire. Alright, how are we gonna do this? Well, I've got my iPad here, which is a timer, and I've got a topic that I've sort of been thinking about. And my dog's interrupting me here, trying to get through my door. I'll be with you in a second. So I'll record this, I've gotta be quick because she wants me to feed her or pat her or something like that. So I'll be with you in a second, Seven. Yes, my dog's named Seven. But that's another story for another time. So, today, 5 minutes of fire, what's the topic? Short term vs. long term satisfaction. So let's get started. Without any further ado, I'm gonna start the timer. Podcast listeners, you won't be able to see it, but YouTube viewers, you can watch this. 5 minutes, let's go. So, I've been thinking about this for a few days, and I read somewhere once, and I've heard from a lot of people that if you think about something for a few days or a bit longer, you've gotta write something about it, or you've gotta, these are from creators by the way, from people I look up to. And if you're thinking about the same thing for a long enough period, you've gotta write something about it, or you've gotta make something about it to get out of your head. And this is what I'm doing today. So I've been thinking about short term satisfaction vs. long term gratification, or either way around, short term gratification vs. long term satisfaction. You can put it either way. Now, why? Well, because I've been trying to rework my life for the last few months. If you've been following along with the previous episodes, you'll know that, but if not, just take that into consideration. So I've been trying to work out, okay, where am I getting short term satisfaction, and where am I getting long term satisfaction? How can I find a balance in the middle? So for me, I think short term satisfaction is something like if you were to buy something, like you get that really gratifying, satisfying feeling of buying something new. Or if you do it, for me, it's working out. Like I find working out easy now. It's easier for me to work out than it is for me to not work out during the day. I get a really good feeling, I get a pump, I've been doing it for 6, 7, 8 years now, so I'm quite good at it. I've still got a lot to learn, but for me, it's a short term satisfaction. Like I get 45 minutes and I get a really good feeling. What I'm trying to get more of, or trying to think about how I can implement it more in my life is long term goals, like long term satisfaction. Now, what is long term satisfaction? Well, it could be working on a project for an extended period of time, or going to university, doing a 4 or 5 year course, or something like that. It's these longer periods, starting a business, something that takes a longer period of time to actually get that satisfying feeling from. And why do I want that? Well, I've always worked best with a long term goal in mind. And how I mean is like, you have a long term goal and then you'll be working towards that, but in the short term, you'll be incredibly fierce. You'll be working as fast as you can in the short term to hit your short term goals, but incredibly patient over the long term. So what's that look like for me? I'm just going to use myself as an example. So a long term goal for me could be, I want to start my own business or build my own company. And a lot of people think, like including myself, like there's a shortcut to everything, right? Like I could build it in a week or in a day or something like that. And if you don't reach it in that time period, well, you get unsatisfied. And I've been there in the past, don't get me wrong. I've definitely sort of tried to do something a lot quicker than what I thought I could do and become deeply unhappy. So now I'm sort of trying to delay gratification for a longer term as well as balancing it out with short term gratification. I haven't quite worked out the full balance yet, but ways that I've sort of found that are best to do that is realize what your long term goal is. So say, or you don't have to have a long term goal, right? Realize like what you're sort of working towards in the grand scheme of things. And maybe if you want to get fit, right? You want to be active and healthy. I had an email the other day from someone asking, what's a 21 day fix? And I don't like that concept of, I relate everything to fitness and nutrition because it's what I know best, right? And so I got an email asking me, what's a 21 day fix to jumpstart my diet and jumpstart my body and etc. Jumpstart my weight loss process. And I replied, think about instead of thinking about the 21 day fix, think about the 21,000 day fix. So what can you do for the rest of your life that's sustainable, right? And for me, thinking about that is scary at the same time as liberating. And it's exciting, exciting and scary at the same time. And where am I going with all this? Well, what's a practical example? Because right now I've been talking so much meta. So how can you fix this? How can you, I don't know, put some steps into action, right? What are some action steps that you can actually do rather than me just sort of rattling off this whole spiel about how I'm trying to live my life. So what I've learned from others and read and what not and practiced myself is a reward system. So look at your big picture. Look at your big goals. What do you really want to achieve? And if you don't have them, that's okay. These things come over time. It's only after trying a whole bunch of things that you realize what you actually want to do. No one's born with a passion. So think about what the big picture is and then break it down into small little chunks. And this time is about to go off, but I'm going to pause it so it doesn't make a sound. So yeah, think about what the big overarching goal is. Like for me, it's I want to help the world move more and eat better, right? So that's a very unspecific goal, but that's the big overarching goal, right? The overarching one doesn't have to be very specific in my opinion, but in the short term, make them ultra specific. Like, okay, I'm going to do this today. I'm going to work out this. I'm going to post a video on eating better. I'm going to write a post about how I lost weight. Like that's what I've done just recently actually. So short term goals, really specific, time-based, and I don't know, something you can complete within a shorter timeframe. Let's say less than a month, less than a month or three months. Long term goals, you're looking at three to five years or an unspecific. Like have that sort of as the end goal and the pathway you're going to build it with these short term goals. So the final, the big long term, the big moonshot will be, I don't know, the gate at the end of the road. And then each of the short term goals will be the road that you're building towards to get into that gate. Now, that was five minutes of fire. What's the daily challenge? Think about your short term and long term goals. I've got to go work out with some friends. I've got to get some of that short term gratification. But over the next few weeks and months, I've got to keep working towards the overall picture, which is helping everyone move more and helping people eat better. But that was a new experiment for this episode, episode 103, The Daniel Burke Show. You can check me out anywhere, mrdeyburke.com. All my social media handles are at Mr. D Burke. \",\n          \"Hi there, today we're looking at high performance large scale image recognition without normalization by Andrew Brock, Soham Dey, Samuel L. Smith, and Karen Simonian of DeepMind. This is otherwise known as NF nets, normalizer free networks. So the point of this paper is to build networks, in this case, specifically convolutional residual style networks that have no batch normalization built in and we'll get to why in, you know, during looking at this paper. But without the batch normalization, usually these networks are performing not as well, or cannot scale to larger batch sizes. However, this paper right here builds networks that can scale to large batch sizes and are more efficient than previous state of the art methods. So if you compare them to something like an efficient net, and I called it, I called it, you shouldn't call your model efficient net, because a more efficient model is going to come around. So NF net are now officially efficient or net, okay. Yes, you can see right here to reach the same accuracy as an efficient B seven, you need, I think they say they have an over 8.7 x speed up, if you look at the training latency, and that's going to be important while looking at these experiments in a second. And if you train for as long as the efficient net B seven, you can reach a higher performance, this is image net top one accuracy. And this model is a new state of the art without additional training data. And it is also a new state of the art transfer learning. And it is the currently ranked number two, behind a method that uses semi supervised pre training with extra data. So in the kind of global leaderboard, it's number two, but it is number one in various categories, the image net has now become, you know, like speedrunning, there is there's glitchless, and the equivalent is like additional training data less, and so on. In any case, we'll go through the paper, we'll discuss what the tricks are to get the normalizer free networks to work, I do have also a fair bit of, let's say, criticism against this paper right here. But in general, it's a pretty cool paper, the code is available, of course, link to the code, you can try it out yourselves. And that's, you know, it's pretty cool that the code is available. All right. If you like content like this, as always, don't hesitate to share it out, consider subscribing, let's dive in. What's the problem with batch norm, batch norm? As you might know, I've done a video on batch norm, but essentially, what it says is that if you have a data point that goes through a network, you know, it will experience various transformations as it goes down the layers. However, some of these transformations are quite unfortunate if you built the network a little bit in a wrong way. So what might happen is that your initial data distribution might be, you know, in machine learning, it's good practice to center the data and around the mean and kind of, you know, scale it to unit variants or something like this. But then as you progress through the layers, and especially if you have something like relu layers, they only extract the positive part of the signal. So with time, it can happen that the intermediate representation right here, for example, is, you know, something like this. So it's very skewed, it's not centered, and so on. And the current methods we have in machine learning, they just work better if your data is sort of well behaved as a nice condition number is centered and so on. So what batch norm does is every layer it comes in, it looks at the current batch of data, the current mini batch, and it centers and rescales it. So what it would do is it would transform this data by a simple standardization procedure into a well behaved data set. Of course, remembering the transformation for a back prop, and then feeding that data to the next layer. That's batch norm. And it has several disadvantages. So the disadvantages of batch norm, this paper identifies three batch normalization has three significant practical disadvantages. First, it is a surprisingly expensive computational primitive, which incurs memory overhead, okay, which is, you know, you need to compute these means, and these scalings, and you need to remember them for the back prop. All right, second of all, sorry, significantly increases the time required to evaluate the gradient in some networks. I mean, there is Yeah, there is some back prop you have to do through all of this standardization. Second, it introduces a discrepancy between the behavior of the model during training and at inference time, which is also true because at inference time, you don't want this kind of batch dependence, you want to be able to feed a batch of data to the next layer, you want to be able to feed a single data point, and the result should always be the same irrespective of the other data. And people usually do this by so at training time, you simply calculate this mean shift right here and the scaling that you have to do. And what you would do is you'd have kind of a database, a special buffer where you save these things for every batch. And then at test time, you simply look at your buffer, you kind of build a mean a moving average over your training data, and you'll simply use those shifts and variants. So you have a discrepancy between training data, which just looks at the current batch and inference, which looks at your mean your average over the last few batches. And third of all, and this is the so this introduces hidden hyper parameters that have to be tuned, which is kind of how fast the mean decays in your database. And third, most importantly, so most importantly, batch normalization breaks the independence between training examples in the mini batch. So not you, it now matters which other examples are in the batch. And that has two consequences. So the first consequence is that batch size matters. So batch size matters in batch normalization. If you have a large batch, you can compute these means of the data, they are a much better approximation to the true mean of the current data set at this particular representation, then a small batch. So if you just have three examples, the mean is going to be a very noisy approximation. Whereas if you have a large batch, it's a good approximation. So batch size matters for batch norm. And second of all, so distributed training, distributed training, yeah, distributed training becomes extremely cumbersome. Because if you do, for example, data parallelism, which means that here you have your batch of data. And we know for some applications that large batches are pretty favorable for training, they stabilize training, you can do larger step sizes, and so on. So what people do is they split the batch, they shard one batch into, let's say, three different parts. And they have the network on three different machines. So the same network is on three different machines. And what you would like to do is you would like to forward propagate all of these batches through the network, sorry, this whole batch in three different shards through the network, and then back propagate and sort of communicate the gradients around. But now imagine if you have a batch norm layer. So if you have a batch norm layer right here, it's going to be the same here. And it's going to be the same here. What you would have to do technically is you have to forward propagate the signal right here to the batch norm layer. And then you'd have to communicate these batch statistics between the batch norm layers, because otherwise, you don't have the mean and the variance over your whole batch that you feed in, right, you can opt to not do this computation. But then again, you run into the problem that usually these, the number of samples in the shard is fairly small, and you have a bad approximation. So batch norm just kind of makes certain things complicated, right. And this interdependence of training data points is one of those things, and they call it the most important things. So they say this third property has negative range of negative consequences. Practitioners have found that batch normalized networks often difficult to replicate precisely on different hardware. Batch norm is a very common term in training. So let's say you have a batch normalization, the cause of subtle implementation errors. Okay, well, yeah, especially during distributed training. And then it cannot be used for some tasks since the interaction between training examples in a batch enables the network to cheat certain loss functions. So this is, let's say you have a like a time series prediction, right. And in a time series prediction, so you have your your time series. So what you usually do is you say, well, this is my input. And this is my goal. And then, and this is my input. And this is my goal. So it's kind of it's like language modeling if you do that. So you want to slice one sequence into many training samples. So you do like overlapping training samples are like, this is the input. And this is the goal. Now, imagine you have those two in the same batch, then technically, the this training sample here could just kind of by means of the batch statistic aggregation, information can actually flow because this here technically is part of the input of one training data point, but it's the label for the other training data point. So there can be information leakage in that. So you shouldn't use batch norm or anything that connects the training samples to each other in these particular cases, it's kind of an edge case. And you can, you can probably get around it by just having a big data set and shuffling a lot, but still, so they say they solve all of these things. Specifically, they say, we propose adaptive gradient clipping, which clips gradients based on their unit wise ratio of gradient norms to parameter norms. And we demonstrate that AGC allows us to train normalizer free networks with larger batch sizes and stronger data augmentations. So their method of of circumventing batch norm of building networks that don't have batch norm anymore, is going to be this adaptive gradient clipping, it's going to be in combination with earlier work from an earlier paper that they've done. And but this paper introduces specifically that active gradient clipping, you're going to see it's a pretty simple idea, it should be implementable in pretty much any network out there. And it has the potential to become kind of a staple component in deep learning, if it turns out to actually work as well, as they say in the paper. They say we design a family of normalizer free resnets called NF nets, which set the new state of the art validation accuracies on ImageNet for a range of training latencies. Okay, so they repeat these things from what I said in the intro. And they also say achieve substantially higher validation accuracy than batch normalized networks when fine tuning on ImageNet after pre training, so they also have a good transfer accuracy. Now, my first problem with this is that the two things here are kind of not very related. So the gradient clipping is an actual, let's say, a contribution, it's a new method, they suggest that they measure it absolutely cool. But then they go around, and they do like giant architecture searches for how could we replace the ConvNet block and so on, to come up with these NF nets, which is also cool. But it is not clear to me that these two things are necessarily as connected as they make it to be, of course, they would say, well, since it's normalizer free, we can build up but I don't see why you couldn't just do like better architecture search for classic batch norms networks. So it seems like and then you don't you don't know where the gains actually come from, like whether or not you need the gradient clipping or whether the contribution here is actually to figure out a kind of a better ResNet architecture. You know, who, who knows? In any case, they the structure of the paper is the follows, they first go, what does batch norm do? What does it do well? And then how can we replace all of the things that it does well, by our own stuff and then not need batch norm anymore. So they identify four things, batch normalization downscales the residual branch. So in a ResNet, you usually have an input. And then you put that through a series of layers to the output. But first, you add the input again, so you add the two. And this and this is, so this part is called the residual branch, it's kind of, so this is the identity function, I've done a video on ResNets, if you want to learn more about that on residual networks. And batch norm will downscale the residual branch implicitly. And that just means that the signal strength is more in favor of this identity function, which is the entire point of ResNets, which is the whole point of the entire point of ResNets, which makes training more stable. Second, batch normalization eliminates mean shift. And that's the thing we said before, that for example, if you have relu's or something like this, they only retain the positive part of the signal, which leads down the network to quite a shift in the mean of the data. And batch norm eliminates that. Third, batch normalization has a regularizing effect by means of the, the batch statistics are noisy, which, you know, we said is a problem for inference. Yes, but it is also has a regularizing effect during training. And lastly, batch normalization allows efficient large batch training. So it smoothens the loss landscape. And this increases the largest stable learning rate. Okay, so we want to get we want to get to a point where we get all these benefits, but don't need batch norm anymore. So first, they introduce their old paper and their old paper, it's not that old, I think it's so it is this one here, you can see it's also this year, it's an it's it's an iClear paper. And there, they built these normalizer free ResNets, these NF ResNets, not to be confused with NF Nets, which this paper introduces, okay. So the normalizer free ResNets already tried to build normalizer free ResNets, they manage, they manage to build, you know, networks that train, but they don't beat the efficient net efficiency yet. What they do, specifically, is they just pay attention a lot to scaling. So they introduce, for example, these parameters, alpha and beta. And what they do is, essentially, in every single block in the neural network, they try to very carefully predict how this block will change the variance of the data. And then they build constants here. So this is, is this alpha or is this beta, I think this is alpha goes after and beta goes before, they build constants, alpha and beta, these are constants that are made particularly for the architecture. So if this is like a conv layer, they pay attention, and they make these constants such that the variance kind of stays constant as you go down the network. So it's very much like people build deep learning frameworks, where you know, for every operation, you have to define a gradient, and then you can chain them together. Here, for every block, they, you know, carefully think about how it affects the variance of a signal. And then they design appropriate scalings to bring that variance back. And if you do that consistently, and it's it is quite hard, right? And they have to do a lot of things, for example, also kind of a, a variant of weight standardization and so on. But if you do this, then you can train quite large batch sizes. So normalizer free resnets match the test set accuracies achieved by batch normalized pre activation resnets on ImageNet, a batch size 124. They also significantly outperform their batch normalized counterparts when the batch size is very small, but they perform worse than batch normalized networks for large batch sizes. Crucially, they do not match the performance of state of the art networks like efficient nets. And this paper is going to fix this. All right. The main way, or one way, the thing the paper introduces is this adaptive gradient clipping. Now what is gradient clipping? So usually, usually, right, you have a parameter, it sits here in the parameter space, and then you get a gradient and you follow that gradient, like over here, down here, over here, down here during training. Now sometimes, sometimes you have a batch of data that just tells it to make a huge jump. And this, these huge jumps are often the cause for training instability. Because, for example, if you use SGD with momentum, that thing will get into your momentum term and just skew the training over here, it will screw with your atom buffers and even plain SGD, it's not really good if you take giant jumps. So gradient clipping simply says whenever a gradient of any parameter is larger than a size, let's say, this size here, we'll simply clip it, that's, we'll scale it. So that's the maximum length. So if it is, if it is, you know, if it's a good gradient, we're surely going to see it again. But if it's a bad gradient, we want to limit its impact. The problem is that it's very sensitive to this parameter right here. And the reason is, it's not adaptive. So what do they mean by adaptive? What they do is the following, it's almost the same. So as you can see, G is the gradient. So this part right here is the same, you want to scale the gradient, but you want to not only clip the gradient to its own norm, but you want to clip the gradient to the ratio to this ratio right here. So the ratio is going to be how large the gradient is, versus how large the weight that the gradient acts upon is. So if you have a small weight, if you have like a small weight, and you suggest a small change to it, fine. But if you suggest a big change to the weight, then it's like, I'd rather sorry, I probably should draw this like this. So small change, fine, large change, not so fine. However, if you already start with a large weight, then you know, large changes might be appropriate, because that's the general scale of that weight. It is though, it is an approximation, right? It is not, it is not a it is not the end all, it's simply a good heuristic, because you can make cases where just comparing these norms don't mean everything. So if your weight is this, and you have kind of a gradient that's really large, that goes into this direction, you know, that might be bad, because you kind of scale the gradient by a factor of three right here. But if I take the same length gradient, and just put it into the other direction, you've not scaled the weight at all, basically, but it's the same length of gradient. So just looking at norms isn't everything, but it seems to be a good heuristic. And with that heuristic, a lot of the problems of batch norm fall away. So they do ablations right here, where you can see that, for example, if you compare batch norm networks, the normalizer free resonance from the last paper and the normalizer free ResNet, plus this adaptive gradient clipping, you can see that after a certain batch size, the non AGC network simply collapses while the ones while the batch norm one and the gradient clipping one prevail. So this seems to be the recipe to go to higher batch sizes, pretty, pretty cool. But over here, you can see, here is a different thing here, it's top one accuracy versus clipping threshold. So where where do you set? Of course, there is still this parameter here. And they complain that it's very finicky with the if you don't do adaptive gradient clipping, so I expect this to not be as crucial if you do non adaptive gradient clipping. However, here, you can see that it has a crucial dependence on the batch size of all things. So you can see at small batch sizes, you can get away with clipping at a pretty large threshold. But then at large batch sizes, you can see you have to, you have to keep the threshold pretty low because if you clip it higher, then it's you know, it collapses. Now, I was told that one of the problems with batch norm is this dependence of training data points amount like to each other. And I kind of expected this paper to fix it, but it doesn't in a very subtle way. So here is how here is how the gradient clipping works. I told you right here, if the gradients too large, we're going to clip it, right? Pretty simple. If it's too large, you know, just clip it down. But what is a gradient? A gradient is actually composed of the batch of data that you feed through, right? So you feed a batch of data through a network, da da da da da. And then you have a weight somewhere here. And the gradient that you get for the weight, so maybe the weight is here in weight space, the gradient you get for the weight is an S sum. So your gradient for your weight of f of x is going to be so this is a large x, this is all the data is going to be a sum over your data points of the gradient now with respect to that, because your loss, sorry, this is a loss function that your loss is a sum. So your gradient is the gradient of a sum of loss functions. And these are interchangeable. Don't come at me math people, not always, but in this case, I guess. So I hope you can you can sort of see that your gradient is going to be a sum over data points or a mean over data points. And that means that it's not actually one gradient, this one gradient is made up by many, many data points pulling that weight in different directions. And the gradient you end up with is simply the average over or the sum over all these gradients that the individual weights put it. So if you now think in this in terms of gradient clipping, you can see that gradient clipping, and you think that during the data, data feeding process during the training process, every data point is an sort of an estimate of the whole data set. That means that your gradient is going to be noisy. That's the point of SGD. What happens to noise if you average it over a bunch of iid samples, it gets smaller in relation to the signal, right? If you have if you input the whole data set, you have no noise, you have a perfect gradient, at least over your training data, as you make the batch smaller and smaller, you have more noise. So if you clip on the final gradient, as opposed to the individual data points, and I've checked in the code, they first do the sum or the average, then they do the clipping. If you do that, that means now the effect of the clipping is going to be dependent on the batch size. And it means that you implicitly interconnect your training data, because if you have a noisy process, right, so if this is your, this is your base noisy process, and you average, you'd always sample two things from that from the noisy process, it has this much noise, you're going to get something that has less noise, because it's the average of two things. Now, if you average over 1000 samples, you're going to get something that has very little noise, right, every now and then it has a bit of noise. What you want to do with the gradient clipping is you want to limit the impact of bad training data points, training data points that just tell you to go a lot into a bad direction. What does that mean? If I have one bad training data point in my batch of four, that is going to spike the gradient a lot, like right here. So my gradient clipping can be pretty high. If I want to clip if I want to limit the impact of that bad data point, if I have a bad data point, my gradient is going to spike pretty heavily, and therefore my clipping threshold should be high. However, if I have one bad training data point in 1024, it's only going to spike the the total gradient a little bit. And therefore, in order to filter out my bad training data points, I need that threshold at a much lower level, right. And therefore, I'm going to, you know, filter out that one here. Now. So that's what I mean, it makes the training data points implicitly dependent on the others in the batch as batch norm does, it just doesn't do it explicitly. But still, there is a dependence on the batch, which I guess you could solve by doing the clipping before you do the averaging. But it's not as easily implemented in the frameworks that we have. By the way, if you do, and if that gets you a better network, cite the channel. Yep, on the way to become the first cited YouTube channel in a machine learning research paper. I could be wrong, though. I mean, I've looked at the code, I could it could be that they do it before. I don't know. Okay, so that's the deal with clipping and my issues with the fact that this does still depend on the batch. So we haven't, we haven't saw actually solve the dependence on the batch yet. We have probably solved the computational issue, they say, you know, for calculating batch norm, it takes a while, and it takes lots of compute this here, it doesn't, it still needs compute. However, probably not that much since you can still you can just do it during the backward phase, right? You don't need anything during the forward phase for doing this clipping, you simply during the backward phase, you need to normalize clip, and you're good. So we can take that one. And then my third criticism right here is that they say the third or the second criticism on batch norm is that it has different train timed behavior as test time behavior, which we discussed, which is true. But then what does their network contain? Dropout dropout? What's the property of dropout? It has a different behavior at train and at test time. Like, so, you know, don't. It's, it's okay, we get that batch norm has these limitations, but your paper doesn't necessarily make them better. It just kind of shifts them to different to different things. Okay, enough rant. So the second part of the paper goes into architecture building. So I actually don't want to touch this as much. But what they do is they say, well, now we go about building a beast architecture that just outperforms everything else. And I'm not sure what it has to do with normalizer free networks. Like, this is something you can do with or without batch norm, but they come up with this new architecture right here, this new block. Let me scroll to the end these new two blocks for resnets. So the right one is where you do not have a kind of a down or up sampling. And this one is where you do. But, you know, they have done a lot of search. And you can see here are the beta and alpha parameters to make this normalizer free. But, you know, doing architecture search, you can do that by yourself, like you don't need the normal, maybe you need the normalizer free, but they don't make it clear that these two things are so intimately connected. And then they get the model they get up here. And, you know, there is quite a bit of evidence in the paper that sorry, this one, there's quite a bit of evidence in the paper that this adaptive gradient clipping actually has some nice properties. Yeah, it allows you to go larger, larger batch size and so on. But again, it's it's a bit unclear what gains come from the normalizer free what gains come from the adaptive gradient clipping and what gains simply come from the fact that they have better architectures. So their whole point in architecture search is that efficiency net what it tries to do is it tries to achieve an accuracy with as little as little flops as possible. However, modern accelerators cannot necessarily make use of those, you know, savings in flops, because you know, they have certain constraints. And therefore, this network right here, it focuses explicitly on training latency, which means that if you use current hardware, which means GPUs or TPUs, how fast is training. So for a given time of training, how much accuracy do you get in there, since it's particularly built for that, as you can see, it beats efficient net by a lot. However, if you look at this in terms of flops, they have a graph down here. So if you look at this in terms of flops versus accuracy, as you can see, it aligns with efficient net. So the kind of line here is pretty, as you can see, like it's pretty straight. It's as if you were to scale up the efficient net architecture for a bit more in terms of flops. So this is better in terms of so this is more optimized for current hardware, this kind of of networks. Yeah, so that is pretty much it they do do a lot of ablations comparisons. And it's not like I don't believe that the adaptive gradient clipping is, you know, does nothing or that, you know, clearly they also they always do experiments, they compare the normalizer free res nets with the batch on res net. So they try to isolate the individual parts. Still, I'm not sure how I feel about papers that have, you know, a lot of different things in one paper. And then they get state of the art, you never exactly know why that is. And the last thing I want to mention that's cool about this paper is appendix E, appendix E show you that appendix E is negative results. And this is really cool. So here is a list of all the stuff they tried that didn't work. And you know, it's one page, but still, it is very, very good, even if it's only to see that other researchers try a whole lot of stuff and fail as well. So I invite you to check out the paper, I've linked the code, you can take the code, it's in jacks, which \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "grouped_youtube"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f5cf98ea-1b81-4375-8a47-fd6656fd4eae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$5 MILLION AI for FREE</td>\n",
              "      <td>Imagine an AI where all in the same model you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 week out from SPARTAN Race + How to bake a s...</td>\n",
              "      <td>What's going on guys welcome back to another S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10-minute Bodyweight Back &amp;amp; Shoulders Bedr...</td>\n",
              "      <td>You know in the coming months, leadership at e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10-minute Lower Body Bodyweight Workout for Be...</td>\n",
              "      <td>Oh, I'm building this hat. Look how good it is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10-minute Morning Wake Up Stretching Routine f...</td>\n",
              "      <td>Good morning! We are up to day 14 of Reps For ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>git for research basics: fundamentals, commits...</td>\n",
              "      <td>Hi there. Today we're taking a look at Git and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>iMAML: Meta-Learning with Implicit Gradients (...</td>\n",
              "      <td>Hi there, today we're looking at meta-learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>mixup: Beyond Empirical Risk Minimization (Pap...</td>\n",
              "      <td>Hi there, today we'll look at Mix-up beyond em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>âˆž-former: Infinite Memory Transformer (aka Inf...</td>\n",
              "      <td>Hello there, today we'll look at Infinityforme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>ðŸ¤— Hugging Face just released *Diffusers* - for...</td>\n",
              "      <td>Today we're going to have a look at a new libr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5cf98ea-1b81-4375-8a47-fd6656fd4eae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5cf98ea-1b81-4375-8a47-fd6656fd4eae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5cf98ea-1b81-4375-8a47-fd6656fd4eae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f2c6c0af-2d2a-478e-92ff-114f3b80128b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f2c6c0af-2d2a-478e-92ff-114f3b80128b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f2c6c0af-2d2a-478e-92ff-114f3b80128b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c2191fb5-10be-414e-8989-e88dc1be0272\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('grouped_youtube')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c2191fb5-10be-414e-8989-e88dc1be0272 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('grouped_youtube');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 title  \\\n",
              "0                               $5 MILLION AI for FREE   \n",
              "1    1 week out from SPARTAN Race + How to bake a s...   \n",
              "2    10-minute Bodyweight Back &amp; Shoulders Bedr...   \n",
              "3    10-minute Lower Body Bodyweight Workout for Be...   \n",
              "4    10-minute Morning Wake Up Stretching Routine f...   \n",
              "..                                                 ...   \n",
              "695  git for research basics: fundamentals, commits...   \n",
              "696  iMAML: Meta-Learning with Implicit Gradients (...   \n",
              "697  mixup: Beyond Empirical Risk Minimization (Pap...   \n",
              "698  âˆž-former: Infinite Memory Transformer (aka Inf...   \n",
              "699  ðŸ¤— Hugging Face just released *Diffusers* - for...   \n",
              "\n",
              "                                                  text  \n",
              "0    Imagine an AI where all in the same model you ...  \n",
              "1    What's going on guys welcome back to another S...  \n",
              "2    You know in the coming months, leadership at e...  \n",
              "3    Oh, I'm building this hat. Look how good it is...  \n",
              "4    Good morning! We are up to day 14 of Reps For ...  \n",
              "..                                                 ...  \n",
              "695  Hi there. Today we're taking a look at Git and...  \n",
              "696  Hi there, today we're looking at meta-learning...  \n",
              "697  Hi there, today we'll look at Mix-up beyond em...  \n",
              "698  Hello there, today we'll look at Infinityforme...  \n",
              "699  Today we're going to have a look at a new libr...  \n",
              "\n",
              "[700 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# I want to group by the title column and apply a function on that to concatenate\n",
        "# the different chunks of a text for each videos.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def concatenate_chunks(group):\n",
        "    concatenated_text = \" \".join(group['text'])\n",
        "    return pd.Series({'text': concatenated_text})\n",
        "\n",
        "grouped_youtube = youtube.groupby('title').apply(concatenate_chunks).reset_index()\n",
        "grouped_youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "id": "i6UOsG_MUBoj",
        "outputId": "ebcb4e9e-366c-4009-e577-44af5804a74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 4s, sys: 248 ms, total: 1min 4s\n",
            "Wall time: 1min 11s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"grouped_youtube\",\n  \"rows\": 700,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"DeepMind&#39;s AlphaFold 2 Explained! AI Breakthrough in Protein Folding! What we know (&amp; what we don&#39;t)\",\n          \"Short Term vs Long Term Satisfaction\",\n          \"NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"It will change everything. DeepMind solves 50 year old grand challenge. The game has changed. DeepMind's latest AI breakthrough achieves historic new milestone, helps solve how diseases invade cells, improve protein folding prediction. AI breakthrough it also wipes your butt automatically. It is the newest DeepMind big publication. Actually, it's not a publication yet. But so what happened and I'm sure you've heard this is that every year there is this competition of protein folding prediction. So proteins are the structures that fold in a given way. And we'll go into that in a bit. But basically every year there is this competition. And the results of this year's competition came out. And they looked something like this. Namely, every entry here you see is a team participating in that competition of protein folding prediction. And there is one team which is DeepMind's system alpha fold two, which completely dominates all the others to the point where the problem is now considered to be solved. Now solved in this case, simply means that you're past a certain number in this in this test set. And if you're past that certain number, your predictions are useful enough so that other scientists can basically take them and base work on them. So that's what it means for this protein folding problem to be solved. Now we don't have much information on alpha fold two yet other than it's really good. And like a blog post and a bunch of advertisement videos by DeepMind, they are writing a paper on it. But today I want to go into this blog post, maybe parse out what we can gather from that blog post. And I also want to go actually through the alpha fold one paper. So as you can see, the performance here increased drastically with alpha fold two, but you know, guesses are high that the system is going to be somewhat similar to alpha fold one of which we do have a paper. So today we'll go into alpha fold one, we'll go into some speculations of alpha fold two, I can already give you my speculation, it's transformers, it's attention, that all of a sudden made this big jump together with probably a few other improvements to the alpha fold one system. Basically, transformers continuing to dominate the entire field. So where do we start? It's probably best, by the way, if this is not a great meme template, I don't know what is just saying, just saying. Yeah, so let's actually start with the problem itself. I realize if you're here, you're probably a machine learning person, might not know too much about protein folding. So these things here are computer representations of proteins. They don't really look that way, but sort of similar. A protein essentially is a chain of amino acids. So an amino acid, where do we have this right here? Amino acids are these what they're called basic building blocks of life, since the proteins, proteins are what make the cell do things. So protein are sort of the workers in the cell, they are used as signaling molecules, receptors, they are parts of your muscles, that actually the parts that move are proteins. So they they are all the work doers. Whenever something needs to work in a cell, do mechanical or work, proteins are involved. And amino acids are the building blocks of proteins. So each amino acid has an has a given a certain common structure. And there are 21 of them. So all the proteins in the world are simply made out of chains of these 21 amino acids. And these chains they are formed in. So there's always this sort of body that can link up to other bodies of amino acids. It's very similar, if you maybe know how DNA is structured, is a very similar concept, except in DNA, there are four different bases. Here, there are 21 amino acids. And each amino acid is a little bit different in each amino acid has like a tail that hangs off. So the tail can be no look like this, or it can look like this, like as with a side chain, are there is there one where it's like, maybe a cyclic one, I'm not sure maybe it can look out here, or it can have sort of no tail at all. I think that's the case for glycine. So the important part is depending on these on this tail, the properties, the chemical properties of the amino acids are different. And then what happens next is really interesting. Once this amino acid chain is built in a in this. So this is the central dogma of modern biology is that you have DNA. And DNA is translated to RNA, sorry. And then it's translated to. So it's read off copied to RNA, which is sort of a DNA clone. And then the RNA is translated into the amino acid chain. And there's always three, three pieces of DNA mapped to one amino acid. This is very much it's like a compiler. Notably, the interesting part is that these steps right here, this compilation steps are done by proteins. So there are proteins that do these things. So nature in a very real sense is its own compiler. So this here you can see as like the binary. And this here is like the source code. But what happens once you build this chain of amino acid, and you set it out into the cell because of these different properties of these side chains, they're also called residues, this chain begins to fold. And so this is, if you know a bit of chemistry, you might know that these are these are sort of atoms that are linked with covalent bonds in this case. And it can be that part of this chain is rather like electrically negatively charged. And here part of this chain might be like electrically positively charged in a given place over a given other place. And it also depends on the surrounding medium, of course. And that means that in this case, for example, these two things will attract. And so if you release this amino acid chain, what you're going to get is sort of a bend, where now the the chain sort of bends and these two this chain right here, this tail goes like here, this tail goes like here, I'm sorry, if there is no, if there is no, if there is no, I don't even know what to call it, pyrene rings or something like this. There isn't an amino acid with that, I apologize. But the point is that these two things attract and sort of form this shape. And this shape is very important. We know that proteins and proteins consist of, it can be hundreds, thousands, tens of thousands of these amino acids in a chain, the proteins function is, interestingly, largely determined by its structure by its 3d structure, not necessarily by the actual amino acid. So technically, you can substitute amino acids for each other. So this amino acid here can be could be substituted for another amino acid that maybe isn't the same, but is has the same properties of its side chain, such that if the structure is still the same, the protein would perform the same function. So that that is, is very special property of proteins, namely their 3d structure, largely determines their function. So for example, in this step here, when you read off the RNA to the DNA, as you know, the RNA is sorry, the DNA is like this double strand of connected base pairs. And in order to replicate the DNA or to read it off, there is a there more or let's call it there's also this step of DNA replication, right where you copy the DNA in mitosis. In order to do that, you need to split off the two strands, you need to split it up, because you want to get like a protein needs to get here to actually read it off. For that there is a protein, a specific protein that will insert right here to split up the DNA, which is called a helicase. And that really is very important how that protein is shaped. So the shape needs to be actually such that it kind of removes these bonds from each other. So the shape is very, very important for a protein. And conceivably, you could build a helicase from many, many different amino acid sequences, as long as it has the same shape. Now, I think something like something like fundamental like a helicase is probably conserved in the evolutionary tree. But I hope you get the point, the shape is super duper important. Now, the shape isn't just arbitrary, there are so the amino acid chain is called the primary structure. And then the first thing that happens is that two very distinct kind of sub shapes appear. So often repeating shapes, these things I think are called alpha helix, helices, or helix, this is a helix. And this here is I don't know what's in English, it's probably called a strand or something like this. These are like long sheets, like, I think they're called beta strands. And these things form these are often repeated sequences. And then the third, the tertiary structure is when the whole thing starts to kind of fold on itself and so on and give itself the the final structure. So this is part I guess, of the RNA polymerase, which is the molecule that reads DNA and outputs RNA. And there are many, many, many proteins. Now, since the shape is so important, it is vital that we know of it, right. And technically, technically, this is what why this problem is 50 years old, I guess, they say it's a 50 year old problem. I think that's due to the fact that 50 years ago, a Nobel laureate said the following, since a protein is fully determined by its amino acid chain, and since the you know, I mean, acid chain determines the structure that is going to go because of these kind of chemical properties, it should be possible to read in the amino acid sequence or read in the DNA sequence, we know what amino acid sequence results, and output the shape of a protein. However, this is an extremely complicated problem, it turned out to be because they're very subtle interactions, they're not always the same, it depends, right, like somewhere out here, there could be some amino acid with like some weird chain that, you know, everything folds on itself all the time. So at some point, these get in contact, and they change is kind of the local properties here. So this is a very, very difficult problem to solve. And people have have sort of tried to do this. And now apparently, deep mind the first system that does this to such a satisfaction that it's beneficial. Alright, now I lost my train of thought. Yeah, so the shape prediction, what happened so far is what you'd have to do is you'd have to sort of do this, determine this experimentally. So you'd have to take these proteins and crystallize them, and then like shoot x rays at them, and then infer the structure, you can you can do that from crystallized proteins, because I think it's due to crystals are like very regular accumulations of proteins. So if you look at a snowflake, that is, if we knew nothing about the water molecule, that it's like H2O, if we knew nothing of that, we could just look at a snowflake and determine this structure, this this these specific angles here from the snowflake. We would just look at the snowflakes. And if someone tells us, look, that's all the same material, that's all water, we could infer what the water molecule looks like, just by analyzing snowflakes, because they're crystals. And the pretty much the same here is you build you make crystals out of these materials, you shoot x rays at them, and then you sort of reason over the patterns that come out. This is very, very difficult, very expensive. And so to solve this problem computationally, is super important. I will get to this graphic in a minute. This is sort of the only thing we know about alpha fold two is this graphic right now, because they have not yet released the the paper or any descriptions of the model, as I said, but what we'll do is we'll go into alpha fold one. So this is alpha fold one. And alpha fold one was participating in the same competition two years ago and was already dominant there, but not yet dominant to the point of having quote unquote, solved the problem just better than other systems. So this is the basic structure of alpha fold one. So what do you what do you have right here? Let's let's give us ourselves an overview. So the overview is the following. There are two different stages to this algorithm. Stage one is over here, and stage two is over here. Maybe it's easiest to start with stage two. So the output of stage one is this thing right here, a distance and torsion distribution prediction. So this this this matrix here that's kind of tilted on its side, I believe there are more down here, right? Okay. So what you do right here is you you take an amino acid sequence and you line it up right here, you line it up. This is the amino acid sequence is a bit harder if there's like a split. But let's just say a protein is actually there can't be a split. Sorry, that's in the amino acids. I'm dumb. So a protein is sing a single chain of these amino acids. There can be multiple sort of parts to a bigger protein conglomerate. But there is this chain, you line it up here and here. So now we're building sort of a pairwise matrix between the sequence and itself. Okay. And this pairwise matrix is going to be a distance matrix. So what we are going to do is we're going to input some features about this sequence of amino acids, right, that's what we get as an input. And we're going to predict for any pair, right, so we have the sequence. And we're going to predict for any pair, how far are they apart? So of course, here, the answer is always kind of zero, they're zero apart. But you might say, you know, these two are five apart. And these two here are seven apart. But these two here are only one apart. So it's reasonable, you know, that the final structure the these two are close together. We don't worry about close together right now, we just worry about for each two will predict how far they are apart. Okay, so this is you can view this as you know, a machine learning problem, right, you have an input, see, you have a sequence, and you simply want to predict the distance matrix. So here you can see that in fact, you can see the top and bottom one is the predicted and one is the real, I don't even remember which one's which, you can see that this system does a pretty good job at that there are minute differences. If you really go look like down here, you can see a bit of a difference. Over here, there is a bit of a difference. But in general, this system does a pretty good job. So this is the output of stage one is this matrix, it's a bunch of other, it's like also the torsion angles and so on. But the main thing is you predict the distances between those two. That's what you take as a input to stage two. So what stage two does is stage two builds a model of this molecule. And the model is sort of a differentiable geometrical model. So they say they where is it this I don't get these nature papers like they're split into two parts, but then they are they largely say the same things. I am absolutely confused by them. So we're going to jump around a fair bit. They say we parameterize protein structures by the backbone torsion angles of all residues and build a differentiable model of protein geometry to compute the coordinates for all residues. And thus the interresidue distances. So what they do is essentially, they build a computer model of these amino acids. And these are parameterized by the torsion angles. Now the torsion angle is simply the angle between any two of them. So this would be like a torsion angle of 180 degrees. And then if it folds like this, it will be torsion angle of 90 degrees and so on. And you need two torsion angles because you're in 3d. But essentially, the torsion angles determine the structure of the protein. So it's one way of parameterizing it. So they built a differentiable model, a differentiable model of protein geometry. Okay, now the important thing is that they don't do any learning with this differentiable model. The purpose of this differentiable model is such that what you can do now, if you have a differentiable model, you can run gradient descent. So imagine they pretty much lay it out right here. So they have the x, x is x is the output of your differentiable geometry, right of your torsion angles, let's just call it this Greek letter phi, psi, whatever. If x is the output, and now x goes into your loss function, so x goes into your loss function, and the loss function simply compares x to the predicted x, okay, so the loss function will take in x, and it will compare it to the x that you predicted from from this thing here. Okay, so we start off with a flat chain, maybe, actually, I think we start off with some initialization, because they also predict the torsion angles directly, right here, they're predicted torsion angles direction, and that's what we initialize from. But let's just say we initialize from the flat chain. And then, because this is differentiable, we do so your your L, your L is x minus x prime, okay. And what we do is we derive the loss with respect to the angle to the torsion angle. So what and we can do this since this is differentiable. So now we know how do we need to change the angle, which is this thing right here, in order to make the loss smaller, right? And maybe it says you need actually you need to turn it down, right? Make the angle smaller. And we do that, okay, cool. Now it's only 90 degrees. And then we do it again, and again, and again. And you can see that by changing all the angles, such that this loss is smaller, we end up through steps, step, step, step, step, step, we, we in our computer model, we sort of replicate this process that happens in nature, where what we feed in is how far any two amino acids should be apart. And by running gradient descent, just gradient descent on the torsion angles, we figure out what do the angles need to be in order to make this happen? Okay, so first, we predict all the distances, and then we figure out how do we need to set the angles such that these distances are fulfilled? These are not true distances, these are predicted distances, right? So everything depends on how well we can predict these distances. But once we have them, we can sort of replicate in our computers the process as it happens in nature, except in nature, the the whole folding is dependent on these all these chemical interactions and so on. And now we do none of this, we simply look see how do we need to fold in order to make these distances in our computer model, like these like the distance between this and this, and this and this, any two distances may agree with the distances that we have predicted right here. And you can see that over time, this as you run gradient descent, this goes up, this this TM score was up the root mean square distance goes down between then you of course can compare it if you have a test set with stuff that people have already figured out, you can analyze these metrics and see that indeed, you do get the correct folding. It's also pretty interesting that so here in blue and red, I believe you have Yeah, exactly. So the the helix in blue, and the strands in red. So in this case, you from if you have this folded structure, partially folded structure, you can already see that these sort of substructures emerge like this is a helix, right, as you can see, and then you sort of made this may be a strand and so on. There are ways to heuristically classify that. And you can see that if you look at the database, right, you can see that this here is the strand, these are helices, and this is a strand and these are here, this is a strand and so on. And you can see that the model here is what the model thinks at the beginning, it doesn't get many things correct, though it does some, but then over time, it sort of refines its guesses until at the end, it's pretty much, you know, equal to what the to what the database to what the true sample is. And here is simply the distribution of, I guess, confidence about these things, and the the torsion angles right here. So it, as you can see, this two step process is the key here to do that. Now, AlphaFold2 conceivably probably changes this a little bit. But again, we're not sure. The step one right here is a deep learning system. So step two is simply a gradient descent procedure that you run at inference time, right? This at training, you can you can just do step one. So step one is, is the machine learning bit. So the goal is to output this distance, this distance tensor right here. And there are more things than distances, as we said, there are torsion angles, and so on. But ultimately, you want to output this distance matrix. And how do they do it, you can already see it's a deep neural network. So you want to build a input data point, let's say, of l by l, which is sequence length by sequence length. So you want to collect some features, you don't know the distances yet, right? But you can collect some features that are either either pairwise features between these two things, right? So here, maybe this is I don't know, leucine, and this is what's a different amino acid, glycine. And in here, you want to put features, maybe it can be features for that position, right? Maybe leucine here is at the 100th position in the in this particular protein, and this is at the 90th position. So we want to put in some features that of that that you can derive from a data set, you can put in correlation statistics in general between these two amino acids, you can even put in just single features. So you have these tiled l by one features, which is just features for the sequence itself, not pairwise features. But what you do is you simply replicate them along along any given dimension right here, you always put the same features, this is very common in convnets. And you can even do a scalar feature. So there are some scalar features. And what you would do is you would simply fill an entire plane with that scalar feature, all the same number, it's just easier to do it like this, because it fits into the convolutional architecture well. So you want to provide all kinds of features and the features they provide are, you know, plentiful, and a lot of them do introduce some domain tools, domain expertise, and so on. But once they have that, they simply take that sort of image with many, many channels, and they predict this image if you want. So it's just an image to image translation problem. And they do this via a convolutional neural network. As you can see, there are 220 residual convolutional blocks. Now, I assume that most of the viewers of this video are familiar what convolutional neural networks are, if not, I'm deeply sorry, but we'll not go into that. But you can see they sort of they tile this tensor right here, and they tile it differently from from from instance to instance. So they tile it, they in the training procedure, they always tile it differently. That's a form of data augmentation. But ultimately, you slide over this image with this 64 by 64 ConvNet, and you produce the image on the right, you can see an inherent weakness of these approaches, namely, that this thing can only ever look at 64 amino acids at a time. So now that can, that can be the same if you're on the diagonal of this, let's say, let's say this is not 64 by 64, but three by three, right? If you're on the diagonal, you would only consider three amino acids and their interactions with each other, right, any to any interactions with each other. If you're off the diagonal, what you would consider is maybe these three amino acids and these three amino acids, and you would only consider you consider features for maybe for those three, but interactions only in between, like the these not interactions actually within the same amino acids. So you're the thing that you can look at any point in time is going to be very limited, right? And these so these distances that you get out here, they necessarily cannot directly depend on, let's say this amino acid right here, you always have this limited view of your protein, that sort of local now, people argue that that's actually enough, if you look at maybe the green connections right here, in order to establish them, what's most important is the vicinity of these of this amino acid and the immediate vicinity of this amino acid. And of course, the interaction between those two vicinities, but it is quite conceivable that this green thing down here being so close will actually sort of push the two apart and sort of do this interaction, which, in my understanding would not be covered by a system like this. And that's where alpha fold two, I believe is is one point where it makes the big gains that it does. Now the features that go in here, as I said, they are, they're quite plentiful. One of the more interesting features is this MSA these multiple sequence alignment, and I believe they're they're up right here. Yeah, sequences. So here they introduce them in recent years, the accuracy of structural predictions has improved through the use of evolutionary covariation data that are found in sets of related sequences. sequences that are similar to the target sequence are found by searching large data sets of protein sequences derived from DNA sequencing and aligned to the target sequence to generate a multiple sequence alignment. Correlated changes in the positions of two amino acid residues across the sequences of MSA can be used to infer which residues might be in contact. So what what this I've searched out one of the papers right here and this is from a paper called improved contact prediction proteins using pseudo likelihoods to infer POTS models. The entire basis here is that here is your chain of amino acid that you're considering. And this is you, this is the human and they actually have one like a very similar graphic in their blog post, but we'll draw this ourselves. I'll just kind of sort of copy it. And what you do is you go and look into your database, right? This this is the amino acid sequence and each amino acid can actually be abbreviated by a single letter since there are 21. And luckily, the holy alphabet creators have given us what 26. So that fits. So each of these can be done by like s, y, c, m, d, and so on. Can be then you go look into your database and your database is of sort of all of life. And you go look for similar sequences. And there are tools that you can very quickly see through databases and get out similar sequences to yours and that those are sequences that are overlapping in amino acid sequence, right? So you could find in the fish, this is an alpha, this is not a fish. In the fish, there is a similar sequence right here in the iron, like this is okay. In the whatever this is, this might be a horsey. No, this is not a horse. Let's make an alligator out of this. So in the alligator raw does the alligator have there might be a sequence and so you get the point my drawing skills are to to be criticized in another video. So you search for all of these similar sequences just by by amino acid sequence. And from the correlations, you can derive something for example, I've already told you that sometimes you can substitute an amino acid in the sort of function of the protein isn't really affected. And this may be what you can see right here. So in the human, this is maybe a D, but or sorry, maybe this here, it's a C. But in the in the let's call this an M. In the fish, it's a C too. But you know, in the alligator, it's a P and in the cockroach, it's K and so on. You can see that maybe if the alignment is good, right, this is sort of from the same protein or from a protein that does maybe the same thing in these life forms because life is continuous. Often these things are preserved or slightly modified. So here, there are variations that happen in life, right mutations, variations. And so we can safely maybe assume that you know, a K, whether there's a K or a P or a C in this particular point, it doesn't really matter, the shape doesn't seem to be too affected. Okay, that's so that's step one. And now, so this might be this this protein, this amino acid right here, you see, whether it's this chain, or whether it's this chain, maybe doesn't really matter for the function of the protein. However, if you look at two proteins that are in contact, what needs to happen? So if my protein here has this chain, and the other protein has has sort of is in contact, that means there is like a chemical interaction between the two, okay. So now if a mutation happens, if a mutation happens, and the protein is still functioning the same way, but the mutation happened, let's say, it's now this right here, that must mean the shape is still the same sort of, and that must mean that probably, if one of them changed, the other one probably changed, sort of analogously at the same time, because structure is preserved function is preserved. So structure is preserved. And since structures determined by chemical interactions, one of the parts changed, that means probably the other part has changed as well. So maybe now this is sort of this chain right here. So what you would expect to see in the statistics is that if one changes, the other one changes accordingly. So there can be variations, right, there can be mutations. But if the mutation happens in one of them, a corresponding mutation should happen in the other one as well. Otherwise, the protein would be nonfunctional and the organism would sort of die. Not always, but you know, this is kind of a statistics game. And this is what you see here, like the fish has an S like the human and an H right here, but the alligator has an F and a W right here. And then in the cockroach, you see the S and the H again, and so on. And here down here, you see the F and the W again. And this is an indication that these, the correlation here is an indication that these two things might be in contact with each other. Now, there have been systems, for example, in this paper right here, that directly go from these statistics to contact predictions, and so on. Alpha fold simply takes in this stuff as features. So this right here, all of this, there can be I think they derive 488 features from this. So this goes down here. I think they say it again, as I said, this is confused, like here, article stops references, article starts again, thanks. And they like say almost the same things. It's just a little bit more detailed, it's not longer. So here, they derive 484 features from these multiple sequence alignment for each residue pair, right. So in our big tensor right here, right here, each dot each thing right here already now has 400. So each one of these already has 484 features. And then some more, right, this is already this is from the MSA, but then more features. So they incorporate lots of features right here. Where are we at here, incorporate lots of features. In addition, we provide the network with features that explicitly represent gaps and deletions. They also represent scalar features, and so on. So here you can see they have scalar features, sequence length features, amino acid type profiles, HH blitz profiles, these are all sort of these comp bio tools, these genetic tools. And so on. You also have sequence length features. These are these 484 features and so on. So these are all akin, there are some positional, one of these acts as positional encodings, so on. So lots of features, input convolutional network output, the distance matrix. And that's that, right. So there you have the inputs, the distance matrix from the distance matrix, you can run gradient descent to get the protein structure at inference time. And they make some pretty cool points. Not only do they compare the distance matrices, but they here is the not only the single prediction for the distance, but they of course, output a probability distribution, they've been all of these distances, they output a probability distribution. And you can see that the black line in these histograms. So this is, this is for a particular thing. This is for this, this red line, this red row right here, it's the extraction. So it's for one of the amino acid, the distribution of probabilities of distance bins with each of the other ones. So this is number 29. And we look at the distance between number 29 and 123, and so on. The black line represent, the represents, I think, eight angstroms, which is generally considered the barrier for being in contact or not being in contact. And here, it's colored in blue, if not in contact and in green, if in contact, and the red bar represents the true distance. And you can see this is pretty accurate. So whenever the network predicts blue, usually the red line is on the right of the black line. And if the network predicts, no, sorry, this green and blue is the ground truth. So whenever it's blue, the network's distribution is usually shifted towards the right. And whenever it's green, the network's distribution is shifted towards the left. There are some failure cases, as you can see right here, the network predicts a higher distance than the, than the, the truth, right. You can also see what's pretty interesting is that the most accurate predictions sort of the highest confidence, the smallest variation in distribution are around here, which is exactly around. So 29 would be in the middle right here. And that's where we find the most accurate predictions, of course, since local local distances are much more easier. And then as you go farther away, you get less sure. And this is a cool thing. So here you can see model prediction versus true distance fits fairly well. But you can also see that here they plot the standard deviation of their prediction. And you can see that the the means are very close, but the higher the sort of standard deviation, the less sure the model is. So there seems to be a there seems to be like a built in confidence metric, right. So you can see the distance error it makes here are bigger. And also its standard deviation is bigger at the same time, which means that you can sort of look at the standard deviation of this distribution right here. And that is an estimate for how sure how confident the model is in its prediction. And apparently, that's something that in alpha fold to the the model relies upon very, very crucially. So here you these are just the on the bottom, you see one of these residual blocks here, more distance matrices, they do a lot of analysis in this article, which is pretty cool. So you can go into it fairly far. They also have look at what the network pays attention to. And it makes a lot of sense, like it pays attention to kind of these these helices, and then these interactions between the helices and the parts where it's close in close contact with, and so on. But now we want to go into alpha fold to alpha fold to now the what we have isn't much we have this graphic right here, which is also in the article, it's probably better we go to the blog post to the blog post is like a fluff piece, saying we, they are going to publish a paper. But of course, they don't have it yet, because we've just gotten the results. Yeah, they have they have these these cool these videos were like, ah, so good. As I said, I've, like, there's so many Twitter threads with, I'm not usually up for the hype, but this is the best thing and so on. And everyone's everyone's hyping. And I thought, is it really up to me to be the grumpy one here? But then I couldn't find anything to be grumpy about. So this is what we what we get. Let's see, it's it's deep mind. I expect them to not fully maybe release the code, maybe they will. But in alpha fold one, they've released like half the code, which is already pretty cool. So there are open source implementations based on that. So, again, nothing to be grumpy about. Alright, so what can we what can we say, they say, a folded, a folded protein can be thought of as a spatial graph. And then this is kind of a new word they introduced. But ultimately, it's simply this distance matrix that we've seen before is a representation of that spatial graph, right? It's simply a graph of nodes and the edges say whether or not they're in contact or respectively how far they are apart, where the residues are nodes and edges connect the residues in close proximity. This graph is important for understanding the physical interactions within proteins as well as their evolutionary history. For the latest version of alpha fold used at CAS 14, that's this challenge, we created an attention based neural network system trained end to end that attempts to interpret the structure of this graph while reasoning over the implicit graph that it's building. I look this, it sounds like this, this is fluff, maybe, I don't know, but this here, attention based, okay, so I'm going to guess for sure that they've replaced this convnet with an with a transformer style with an attention, attention layer or multiple attention layers. They say it uses evolutionary evolutionarily related sequences, multiple sequence alignment and the representation of amino acid residue pairs to refine this graph. This is this is what we've already seen. So use these other sequences plus like a lot of stats that you can gather from the data sets on amino acid pairs in order to develop this, this graph and the graph is distance, the distance matrix, or other things we'll see in just a second. They say by iterating this process, system develops strong predictions of the underlying physical structure of the protein and is able to determine highly accurate structures in a matter of days. Additionally, alpha fall can predict which parts of each predicted protein structure are reliable in the future using an internal confidence measure. Again, this is something that we've already sort of seen in alpha fold one that there is sort of an internal confidence measure. And the part here is they say by iterating this process, which could mean that it's no longer just this two stage approach, but it could be an actually fully cycling approach that sort of goes back to the neural network to refine the structure that it's building with the gradient descent procedure. It's entirely possible. So this is the graphic of alpha fold two, you can see at the very beginning, you have protein sequence. And at first, you have this embed and outer embed and outer sum, which I'm going to guess this is just kind of features for pairs or individual amino acids. This this is correlation statistics from your data set, it can be, you know, chemical properties, whatever it just a bunch of features that you can attach to each of these amino acids in the sequence, right. The other path here is this genetic search and embed. So this is what we've already seen with the MSA embedding, I saw I told you they have the same graphic. So there's human, there's fishy, there's rabbit, and you simply search for sequences in your database, it could even be from other humans, right, that are similar. And from that from those, you can also derive features. So here is where I'm a bit confused. You can see they build up this again, this square matrix right here. I mean, this, it already screamed attention before, right. So I'm going to guess they no longer limit themselves to the maybe, maybe to the 64 by 64. Maybe they do something bigger. Maybe they use local attention, who knows, but I'm going to guess they use attention to. And these, this here is simply given by an attention layer of some sort to go into the next to just this is basically, I would guess this is a big transformer right here. The interesting part is that it appears to interact much like much like the original transformer, maybe encoder decoder. Here, they pass information around. So this top thing isn't amino acid sequence to amino acid sequence like to itself, but it appears to be a matrix that you build up between the amino acid sequence and these sequences you built. So I would guess that they are no longer, let's say, happy with simply inputting the features of these algorithms that go over these other sequences. But now they also want to sort of put these features through through steps of transformations. So again, I would guess this is an attention layer. And how can we interpret this matrix? As you can see, this matrix relates individual amino acids in the sequence to other species. So I would guess that this square here represents something like how important is this particular location in the chain, which is a purple thingy in the human? How important is that in the in the in the chicken? Or how related is that to the chicken at that particular position? Or as a whole? I don't know, probably DeepMind doesn't know, like they probably just ship these features in here, right? And then they just ship it through transformers, they pass information around, I don't know whether it's just in this direction. And then in this direction, or whether there's like an arrow right here, conceivably, but in any case, it seems like they've replaced what was a ConvNet. So no longer friends with ConvNet new best friend is transformer. And then at the end, you see what they get out is these pairwise distances again. Now, it's also not really clear, because I would expect maybe an arrow going like this, if they again, use these pairwise distances to predict the structure, I don't know, okay. Or if that's just a side output, I would guess they still actually use the pairwise distances. And the confidence score, again, you can it might be something very similar that we've saw again, being the sort of standard deviation on the predicted distances, but they could also refine that. And then the last thing is, I don't know if this iterative process is simply referring to there being multiple layers of this attention and passing around. So the passing around will simply be like, you stack the representations on top of each other. I don't know if this is the iterative procedure, or if there is actually like the structure module actually sort of builds the structure and then goes back. And then you consult the neural network again, and then you build some more of the structure, and so on. I can't tell right now, it's quite conceivable that they they do like that the search here is not only gradient descent, but is actually informed by the neural network. So you sort of go back and refine though I don't know, there doesn't seem to be any features in the neural networks that would represent that would represent whatever you could read from a partially built 3d model. So, you know, the boring guess is that the part two is very is is a lot of the same, but there could also be substantial improvements in that part. Alright, I hope this was this was sort of a good overview. So, as I said, the paper isn't out yet. If you want to cite this, I guess you can you can refer to the blog post and here they say, until we've published a paper on this work, please cite high accuracy protein structure prediction using deep learning by these people. I just want to say that I just want to highlight shout out to to Anna, who was educated right here. She was an intern. So in a way, I'm actually saying that this is my discovery and I take full responsibility for it. You're welcome world. Shout out to Anna. Very nice job. Good work. Good work to all of these people. And yeah, I hope that was enough. If I got something horribly wrong, please tell me in the \",\n          \"What's going on y'all? Episode 103 of the Daniel Burke Show. Sorry it's been so long since the last one. I know I said I sort of wanted to do these weekly or fortnightly or something. I'm still trying to work out that routine. No excuses though. We're gonna do one today. And what's the topic of the day? Well since it's a Friday, I figured I want to do something fun. Because Friday, F, fun, you know, you know how it goes. Everyone's happy on a Friday. I'm happy every day, well I try to be at least. Actually, speaking of that, I had an article published recently on Thought Catalog. It's called How I Stop Feeling Depressed. So although I'm positive and optimistic and outgoing, fun, whatever, 99.999999% of the time, I still have those days where I feel down and sort of not wanting to do anything. So I wrote an article on what I do to stop feeling down, stop feeling depressed. And Thought Catalog, the editors from Thought Catalog, reached out and republished it on their website. So I was pretty happy with that. It's the first piece of writing I've actually had republished somewhere. Either the Medium or my blog or something like that. So I'll link it in the description or in the show notes so you can all check it out. And show some love, show some hate, I don't mind, as long as you read it. But what's today's episode about? Yes, Friday, it's fun. We're gonna make it nice and quick. I figured, let's do 5 minutes of fire. Alright, how are we gonna do this? Well, I've got my iPad here, which is a timer, and I've got a topic that I've sort of been thinking about. And my dog's interrupting me here, trying to get through my door. I'll be with you in a second. So I'll record this, I've gotta be quick because she wants me to feed her or pat her or something like that. So I'll be with you in a second, Seven. Yes, my dog's named Seven. But that's another story for another time. So, today, 5 minutes of fire, what's the topic? Short term vs. long term satisfaction. So let's get started. Without any further ado, I'm gonna start the timer. Podcast listeners, you won't be able to see it, but YouTube viewers, you can watch this. 5 minutes, let's go. So, I've been thinking about this for a few days, and I read somewhere once, and I've heard from a lot of people that if you think about something for a few days or a bit longer, you've gotta write something about it, or you've gotta, these are from creators by the way, from people I look up to. And if you're thinking about the same thing for a long enough period, you've gotta write something about it, or you've gotta make something about it to get out of your head. And this is what I'm doing today. So I've been thinking about short term satisfaction vs. long term gratification, or either way around, short term gratification vs. long term satisfaction. You can put it either way. Now, why? Well, because I've been trying to rework my life for the last few months. If you've been following along with the previous episodes, you'll know that, but if not, just take that into consideration. So I've been trying to work out, okay, where am I getting short term satisfaction, and where am I getting long term satisfaction? How can I find a balance in the middle? So for me, I think short term satisfaction is something like if you were to buy something, like you get that really gratifying, satisfying feeling of buying something new. Or if you do it, for me, it's working out. Like I find working out easy now. It's easier for me to work out than it is for me to not work out during the day. I get a really good feeling, I get a pump, I've been doing it for 6, 7, 8 years now, so I'm quite good at it. I've still got a lot to learn, but for me, it's a short term satisfaction. Like I get 45 minutes and I get a really good feeling. What I'm trying to get more of, or trying to think about how I can implement it more in my life is long term goals, like long term satisfaction. Now, what is long term satisfaction? Well, it could be working on a project for an extended period of time, or going to university, doing a 4 or 5 year course, or something like that. It's these longer periods, starting a business, something that takes a longer period of time to actually get that satisfying feeling from. And why do I want that? Well, I've always worked best with a long term goal in mind. And how I mean is like, you have a long term goal and then you'll be working towards that, but in the short term, you'll be incredibly fierce. You'll be working as fast as you can in the short term to hit your short term goals, but incredibly patient over the long term. So what's that look like for me? I'm just going to use myself as an example. So a long term goal for me could be, I want to start my own business or build my own company. And a lot of people think, like including myself, like there's a shortcut to everything, right? Like I could build it in a week or in a day or something like that. And if you don't reach it in that time period, well, you get unsatisfied. And I've been there in the past, don't get me wrong. I've definitely sort of tried to do something a lot quicker than what I thought I could do and become deeply unhappy. So now I'm sort of trying to delay gratification for a longer term as well as balancing it out with short term gratification. I haven't quite worked out the full balance yet, but ways that I've sort of found that are best to do that is realize what your long term goal is. So say, or you don't have to have a long term goal, right? Realize like what you're sort of working towards in the grand scheme of things. And maybe if you want to get fit, right? You want to be active and healthy. I had an email the other day from someone asking, what's a 21 day fix? And I don't like that concept of, I relate everything to fitness and nutrition because it's what I know best, right? And so I got an email asking me, what's a 21 day fix to jumpstart my diet and jumpstart my body and etc. Jumpstart my weight loss process. And I replied, think about instead of thinking about the 21 day fix, think about the 21,000 day fix. So what can you do for the rest of your life that's sustainable, right? And for me, thinking about that is scary at the same time as liberating. And it's exciting, exciting and scary at the same time. And where am I going with all this? Well, what's a practical example? Because right now I've been talking so much meta. So how can you fix this? How can you, I don't know, put some steps into action, right? What are some action steps that you can actually do rather than me just sort of rattling off this whole spiel about how I'm trying to live my life. So what I've learned from others and read and what not and practiced myself is a reward system. So look at your big picture. Look at your big goals. What do you really want to achieve? And if you don't have them, that's okay. These things come over time. It's only after trying a whole bunch of things that you realize what you actually want to do. No one's born with a passion. So think about what the big picture is and then break it down into small little chunks. And this time is about to go off, but I'm going to pause it so it doesn't make a sound. So yeah, think about what the big overarching goal is. Like for me, it's I want to help the world move more and eat better, right? So that's a very unspecific goal, but that's the big overarching goal, right? The overarching one doesn't have to be very specific in my opinion, but in the short term, make them ultra specific. Like, okay, I'm going to do this today. I'm going to work out this. I'm going to post a video on eating better. I'm going to write a post about how I lost weight. Like that's what I've done just recently actually. So short term goals, really specific, time-based, and I don't know, something you can complete within a shorter timeframe. Let's say less than a month, less than a month or three months. Long term goals, you're looking at three to five years or an unspecific. Like have that sort of as the end goal and the pathway you're going to build it with these short term goals. So the final, the big long term, the big moonshot will be, I don't know, the gate at the end of the road. And then each of the short term goals will be the road that you're building towards to get into that gate. Now, that was five minutes of fire. What's the daily challenge? Think about your short term and long term goals. I've got to go work out with some friends. I've got to get some of that short term gratification. But over the next few weeks and months, I've got to keep working towards the overall picture, which is helping everyone move more and helping people eat better. But that was a new experiment for this episode, episode 103, The Daniel Burke Show. You can check me out anywhere, mrdeyburke.com. All my social media handles are at Mr. D Burke. \",\n          \"Hi there, today we're looking at high performance large scale image recognition without normalization by Andrew Brock, Soham Dey, Samuel L. Smith, and Karen Simonian of DeepMind. This is otherwise known as NF nets, normalizer free networks. So the point of this paper is to build networks, in this case, specifically convolutional residual style networks that have no batch normalization built in and we'll get to why in, you know, during looking at this paper. But without the batch normalization, usually these networks are performing not as well, or cannot scale to larger batch sizes. However, this paper right here builds networks that can scale to large batch sizes and are more efficient than previous state of the art methods. So if you compare them to something like an efficient net, and I called it, I called it, you shouldn't call your model efficient net, because a more efficient model is going to come around. So NF net are now officially efficient or net, okay. Yes, you can see right here to reach the same accuracy as an efficient B seven, you need, I think they say they have an over 8.7 x speed up, if you look at the training latency, and that's going to be important while looking at these experiments in a second. And if you train for as long as the efficient net B seven, you can reach a higher performance, this is image net top one accuracy. And this model is a new state of the art without additional training data. And it is also a new state of the art transfer learning. And it is the currently ranked number two, behind a method that uses semi supervised pre training with extra data. So in the kind of global leaderboard, it's number two, but it is number one in various categories, the image net has now become, you know, like speedrunning, there is there's glitchless, and the equivalent is like additional training data less, and so on. In any case, we'll go through the paper, we'll discuss what the tricks are to get the normalizer free networks to work, I do have also a fair bit of, let's say, criticism against this paper right here. But in general, it's a pretty cool paper, the code is available, of course, link to the code, you can try it out yourselves. And that's, you know, it's pretty cool that the code is available. All right. If you like content like this, as always, don't hesitate to share it out, consider subscribing, let's dive in. What's the problem with batch norm, batch norm? As you might know, I've done a video on batch norm, but essentially, what it says is that if you have a data point that goes through a network, you know, it will experience various transformations as it goes down the layers. However, some of these transformations are quite unfortunate if you built the network a little bit in a wrong way. So what might happen is that your initial data distribution might be, you know, in machine learning, it's good practice to center the data and around the mean and kind of, you know, scale it to unit variants or something like this. But then as you progress through the layers, and especially if you have something like relu layers, they only extract the positive part of the signal. So with time, it can happen that the intermediate representation right here, for example, is, you know, something like this. So it's very skewed, it's not centered, and so on. And the current methods we have in machine learning, they just work better if your data is sort of well behaved as a nice condition number is centered and so on. So what batch norm does is every layer it comes in, it looks at the current batch of data, the current mini batch, and it centers and rescales it. So what it would do is it would transform this data by a simple standardization procedure into a well behaved data set. Of course, remembering the transformation for a back prop, and then feeding that data to the next layer. That's batch norm. And it has several disadvantages. So the disadvantages of batch norm, this paper identifies three batch normalization has three significant practical disadvantages. First, it is a surprisingly expensive computational primitive, which incurs memory overhead, okay, which is, you know, you need to compute these means, and these scalings, and you need to remember them for the back prop. All right, second of all, sorry, significantly increases the time required to evaluate the gradient in some networks. I mean, there is Yeah, there is some back prop you have to do through all of this standardization. Second, it introduces a discrepancy between the behavior of the model during training and at inference time, which is also true because at inference time, you don't want this kind of batch dependence, you want to be able to feed a batch of data to the next layer, you want to be able to feed a single data point, and the result should always be the same irrespective of the other data. And people usually do this by so at training time, you simply calculate this mean shift right here and the scaling that you have to do. And what you would do is you'd have kind of a database, a special buffer where you save these things for every batch. And then at test time, you simply look at your buffer, you kind of build a mean a moving average over your training data, and you'll simply use those shifts and variants. So you have a discrepancy between training data, which just looks at the current batch and inference, which looks at your mean your average over the last few batches. And third of all, and this is the so this introduces hidden hyper parameters that have to be tuned, which is kind of how fast the mean decays in your database. And third, most importantly, so most importantly, batch normalization breaks the independence between training examples in the mini batch. So not you, it now matters which other examples are in the batch. And that has two consequences. So the first consequence is that batch size matters. So batch size matters in batch normalization. If you have a large batch, you can compute these means of the data, they are a much better approximation to the true mean of the current data set at this particular representation, then a small batch. So if you just have three examples, the mean is going to be a very noisy approximation. Whereas if you have a large batch, it's a good approximation. So batch size matters for batch norm. And second of all, so distributed training, distributed training, yeah, distributed training becomes extremely cumbersome. Because if you do, for example, data parallelism, which means that here you have your batch of data. And we know for some applications that large batches are pretty favorable for training, they stabilize training, you can do larger step sizes, and so on. So what people do is they split the batch, they shard one batch into, let's say, three different parts. And they have the network on three different machines. So the same network is on three different machines. And what you would like to do is you would like to forward propagate all of these batches through the network, sorry, this whole batch in three different shards through the network, and then back propagate and sort of communicate the gradients around. But now imagine if you have a batch norm layer. So if you have a batch norm layer right here, it's going to be the same here. And it's going to be the same here. What you would have to do technically is you have to forward propagate the signal right here to the batch norm layer. And then you'd have to communicate these batch statistics between the batch norm layers, because otherwise, you don't have the mean and the variance over your whole batch that you feed in, right, you can opt to not do this computation. But then again, you run into the problem that usually these, the number of samples in the shard is fairly small, and you have a bad approximation. So batch norm just kind of makes certain things complicated, right. And this interdependence of training data points is one of those things, and they call it the most important things. So they say this third property has negative range of negative consequences. Practitioners have found that batch normalized networks often difficult to replicate precisely on different hardware. Batch norm is a very common term in training. So let's say you have a batch normalization, the cause of subtle implementation errors. Okay, well, yeah, especially during distributed training. And then it cannot be used for some tasks since the interaction between training examples in a batch enables the network to cheat certain loss functions. So this is, let's say you have a like a time series prediction, right. And in a time series prediction, so you have your your time series. So what you usually do is you say, well, this is my input. And this is my goal. And then, and this is my input. And this is my goal. So it's kind of it's like language modeling if you do that. So you want to slice one sequence into many training samples. So you do like overlapping training samples are like, this is the input. And this is the goal. Now, imagine you have those two in the same batch, then technically, the this training sample here could just kind of by means of the batch statistic aggregation, information can actually flow because this here technically is part of the input of one training data point, but it's the label for the other training data point. So there can be information leakage in that. So you shouldn't use batch norm or anything that connects the training samples to each other in these particular cases, it's kind of an edge case. And you can, you can probably get around it by just having a big data set and shuffling a lot, but still, so they say they solve all of these things. Specifically, they say, we propose adaptive gradient clipping, which clips gradients based on their unit wise ratio of gradient norms to parameter norms. And we demonstrate that AGC allows us to train normalizer free networks with larger batch sizes and stronger data augmentations. So their method of of circumventing batch norm of building networks that don't have batch norm anymore, is going to be this adaptive gradient clipping, it's going to be in combination with earlier work from an earlier paper that they've done. And but this paper introduces specifically that active gradient clipping, you're going to see it's a pretty simple idea, it should be implementable in pretty much any network out there. And it has the potential to become kind of a staple component in deep learning, if it turns out to actually work as well, as they say in the paper. They say we design a family of normalizer free resnets called NF nets, which set the new state of the art validation accuracies on ImageNet for a range of training latencies. Okay, so they repeat these things from what I said in the intro. And they also say achieve substantially higher validation accuracy than batch normalized networks when fine tuning on ImageNet after pre training, so they also have a good transfer accuracy. Now, my first problem with this is that the two things here are kind of not very related. So the gradient clipping is an actual, let's say, a contribution, it's a new method, they suggest that they measure it absolutely cool. But then they go around, and they do like giant architecture searches for how could we replace the ConvNet block and so on, to come up with these NF nets, which is also cool. But it is not clear to me that these two things are necessarily as connected as they make it to be, of course, they would say, well, since it's normalizer free, we can build up but I don't see why you couldn't just do like better architecture search for classic batch norms networks. So it seems like and then you don't you don't know where the gains actually come from, like whether or not you need the gradient clipping or whether the contribution here is actually to figure out a kind of a better ResNet architecture. You know, who, who knows? In any case, they the structure of the paper is the follows, they first go, what does batch norm do? What does it do well? And then how can we replace all of the things that it does well, by our own stuff and then not need batch norm anymore. So they identify four things, batch normalization downscales the residual branch. So in a ResNet, you usually have an input. And then you put that through a series of layers to the output. But first, you add the input again, so you add the two. And this and this is, so this part is called the residual branch, it's kind of, so this is the identity function, I've done a video on ResNets, if you want to learn more about that on residual networks. And batch norm will downscale the residual branch implicitly. And that just means that the signal strength is more in favor of this identity function, which is the entire point of ResNets, which is the whole point of the entire point of ResNets, which makes training more stable. Second, batch normalization eliminates mean shift. And that's the thing we said before, that for example, if you have relu's or something like this, they only retain the positive part of the signal, which leads down the network to quite a shift in the mean of the data. And batch norm eliminates that. Third, batch normalization has a regularizing effect by means of the, the batch statistics are noisy, which, you know, we said is a problem for inference. Yes, but it is also has a regularizing effect during training. And lastly, batch normalization allows efficient large batch training. So it smoothens the loss landscape. And this increases the largest stable learning rate. Okay, so we want to get we want to get to a point where we get all these benefits, but don't need batch norm anymore. So first, they introduce their old paper and their old paper, it's not that old, I think it's so it is this one here, you can see it's also this year, it's an it's it's an iClear paper. And there, they built these normalizer free ResNets, these NF ResNets, not to be confused with NF Nets, which this paper introduces, okay. So the normalizer free ResNets already tried to build normalizer free ResNets, they manage, they manage to build, you know, networks that train, but they don't beat the efficient net efficiency yet. What they do, specifically, is they just pay attention a lot to scaling. So they introduce, for example, these parameters, alpha and beta. And what they do is, essentially, in every single block in the neural network, they try to very carefully predict how this block will change the variance of the data. And then they build constants here. So this is, is this alpha or is this beta, I think this is alpha goes after and beta goes before, they build constants, alpha and beta, these are constants that are made particularly for the architecture. So if this is like a conv layer, they pay attention, and they make these constants such that the variance kind of stays constant as you go down the network. So it's very much like people build deep learning frameworks, where you know, for every operation, you have to define a gradient, and then you can chain them together. Here, for every block, they, you know, carefully think about how it affects the variance of a signal. And then they design appropriate scalings to bring that variance back. And if you do that consistently, and it's it is quite hard, right? And they have to do a lot of things, for example, also kind of a, a variant of weight standardization and so on. But if you do this, then you can train quite large batch sizes. So normalizer free resnets match the test set accuracies achieved by batch normalized pre activation resnets on ImageNet, a batch size 124. They also significantly outperform their batch normalized counterparts when the batch size is very small, but they perform worse than batch normalized networks for large batch sizes. Crucially, they do not match the performance of state of the art networks like efficient nets. And this paper is going to fix this. All right. The main way, or one way, the thing the paper introduces is this adaptive gradient clipping. Now what is gradient clipping? So usually, usually, right, you have a parameter, it sits here in the parameter space, and then you get a gradient and you follow that gradient, like over here, down here, over here, down here during training. Now sometimes, sometimes you have a batch of data that just tells it to make a huge jump. And this, these huge jumps are often the cause for training instability. Because, for example, if you use SGD with momentum, that thing will get into your momentum term and just skew the training over here, it will screw with your atom buffers and even plain SGD, it's not really good if you take giant jumps. So gradient clipping simply says whenever a gradient of any parameter is larger than a size, let's say, this size here, we'll simply clip it, that's, we'll scale it. So that's the maximum length. So if it is, if it is, you know, if it's a good gradient, we're surely going to see it again. But if it's a bad gradient, we want to limit its impact. The problem is that it's very sensitive to this parameter right here. And the reason is, it's not adaptive. So what do they mean by adaptive? What they do is the following, it's almost the same. So as you can see, G is the gradient. So this part right here is the same, you want to scale the gradient, but you want to not only clip the gradient to its own norm, but you want to clip the gradient to the ratio to this ratio right here. So the ratio is going to be how large the gradient is, versus how large the weight that the gradient acts upon is. So if you have a small weight, if you have like a small weight, and you suggest a small change to it, fine. But if you suggest a big change to the weight, then it's like, I'd rather sorry, I probably should draw this like this. So small change, fine, large change, not so fine. However, if you already start with a large weight, then you know, large changes might be appropriate, because that's the general scale of that weight. It is though, it is an approximation, right? It is not, it is not a it is not the end all, it's simply a good heuristic, because you can make cases where just comparing these norms don't mean everything. So if your weight is this, and you have kind of a gradient that's really large, that goes into this direction, you know, that might be bad, because you kind of scale the gradient by a factor of three right here. But if I take the same length gradient, and just put it into the other direction, you've not scaled the weight at all, basically, but it's the same length of gradient. So just looking at norms isn't everything, but it seems to be a good heuristic. And with that heuristic, a lot of the problems of batch norm fall away. So they do ablations right here, where you can see that, for example, if you compare batch norm networks, the normalizer free resonance from the last paper and the normalizer free ResNet, plus this adaptive gradient clipping, you can see that after a certain batch size, the non AGC network simply collapses while the ones while the batch norm one and the gradient clipping one prevail. So this seems to be the recipe to go to higher batch sizes, pretty, pretty cool. But over here, you can see, here is a different thing here, it's top one accuracy versus clipping threshold. So where where do you set? Of course, there is still this parameter here. And they complain that it's very finicky with the if you don't do adaptive gradient clipping, so I expect this to not be as crucial if you do non adaptive gradient clipping. However, here, you can see that it has a crucial dependence on the batch size of all things. So you can see at small batch sizes, you can get away with clipping at a pretty large threshold. But then at large batch sizes, you can see you have to, you have to keep the threshold pretty low because if you clip it higher, then it's you know, it collapses. Now, I was told that one of the problems with batch norm is this dependence of training data points amount like to each other. And I kind of expected this paper to fix it, but it doesn't in a very subtle way. So here is how here is how the gradient clipping works. I told you right here, if the gradients too large, we're going to clip it, right? Pretty simple. If it's too large, you know, just clip it down. But what is a gradient? A gradient is actually composed of the batch of data that you feed through, right? So you feed a batch of data through a network, da da da da da. And then you have a weight somewhere here. And the gradient that you get for the weight, so maybe the weight is here in weight space, the gradient you get for the weight is an S sum. So your gradient for your weight of f of x is going to be so this is a large x, this is all the data is going to be a sum over your data points of the gradient now with respect to that, because your loss, sorry, this is a loss function that your loss is a sum. So your gradient is the gradient of a sum of loss functions. And these are interchangeable. Don't come at me math people, not always, but in this case, I guess. So I hope you can you can sort of see that your gradient is going to be a sum over data points or a mean over data points. And that means that it's not actually one gradient, this one gradient is made up by many, many data points pulling that weight in different directions. And the gradient you end up with is simply the average over or the sum over all these gradients that the individual weights put it. So if you now think in this in terms of gradient clipping, you can see that gradient clipping, and you think that during the data, data feeding process during the training process, every data point is an sort of an estimate of the whole data set. That means that your gradient is going to be noisy. That's the point of SGD. What happens to noise if you average it over a bunch of iid samples, it gets smaller in relation to the signal, right? If you have if you input the whole data set, you have no noise, you have a perfect gradient, at least over your training data, as you make the batch smaller and smaller, you have more noise. So if you clip on the final gradient, as opposed to the individual data points, and I've checked in the code, they first do the sum or the average, then they do the clipping. If you do that, that means now the effect of the clipping is going to be dependent on the batch size. And it means that you implicitly interconnect your training data, because if you have a noisy process, right, so if this is your, this is your base noisy process, and you average, you'd always sample two things from that from the noisy process, it has this much noise, you're going to get something that has less noise, because it's the average of two things. Now, if you average over 1000 samples, you're going to get something that has very little noise, right, every now and then it has a bit of noise. What you want to do with the gradient clipping is you want to limit the impact of bad training data points, training data points that just tell you to go a lot into a bad direction. What does that mean? If I have one bad training data point in my batch of four, that is going to spike the gradient a lot, like right here. So my gradient clipping can be pretty high. If I want to clip if I want to limit the impact of that bad data point, if I have a bad data point, my gradient is going to spike pretty heavily, and therefore my clipping threshold should be high. However, if I have one bad training data point in 1024, it's only going to spike the the total gradient a little bit. And therefore, in order to filter out my bad training data points, I need that threshold at a much lower level, right. And therefore, I'm going to, you know, filter out that one here. Now. So that's what I mean, it makes the training data points implicitly dependent on the others in the batch as batch norm does, it just doesn't do it explicitly. But still, there is a dependence on the batch, which I guess you could solve by doing the clipping before you do the averaging. But it's not as easily implemented in the frameworks that we have. By the way, if you do, and if that gets you a better network, cite the channel. Yep, on the way to become the first cited YouTube channel in a machine learning research paper. I could be wrong, though. I mean, I've looked at the code, I could it could be that they do it before. I don't know. Okay, so that's the deal with clipping and my issues with the fact that this does still depend on the batch. So we haven't, we haven't saw actually solve the dependence on the batch yet. We have probably solved the computational issue, they say, you know, for calculating batch norm, it takes a while, and it takes lots of compute this here, it doesn't, it still needs compute. However, probably not that much since you can still you can just do it during the backward phase, right? You don't need anything during the forward phase for doing this clipping, you simply during the backward phase, you need to normalize clip, and you're good. So we can take that one. And then my third criticism right here is that they say the third or the second criticism on batch norm is that it has different train timed behavior as test time behavior, which we discussed, which is true. But then what does their network contain? Dropout dropout? What's the property of dropout? It has a different behavior at train and at test time. Like, so, you know, don't. It's, it's okay, we get that batch norm has these limitations, but your paper doesn't necessarily make them better. It just kind of shifts them to different to different things. Okay, enough rant. So the second part of the paper goes into architecture building. So I actually don't want to touch this as much. But what they do is they say, well, now we go about building a beast architecture that just outperforms everything else. And I'm not sure what it has to do with normalizer free networks. Like, this is something you can do with or without batch norm, but they come up with this new architecture right here, this new block. Let me scroll to the end these new two blocks for resnets. So the right one is where you do not have a kind of a down or up sampling. And this one is where you do. But, you know, they have done a lot of search. And you can see here are the beta and alpha parameters to make this normalizer free. But, you know, doing architecture search, you can do that by yourself, like you don't need the normal, maybe you need the normalizer free, but they don't make it clear that these two things are so intimately connected. And then they get the model they get up here. And, you know, there is quite a bit of evidence in the paper that sorry, this one, there's quite a bit of evidence in the paper that this adaptive gradient clipping actually has some nice properties. Yeah, it allows you to go larger, larger batch size and so on. But again, it's it's a bit unclear what gains come from the normalizer free what gains come from the adaptive gradient clipping and what gains simply come from the fact that they have better architectures. So their whole point in architecture search is that efficiency net what it tries to do is it tries to achieve an accuracy with as little as little flops as possible. However, modern accelerators cannot necessarily make use of those, you know, savings in flops, because you know, they have certain constraints. And therefore, this network right here, it focuses explicitly on training latency, which means that if you use current hardware, which means GPUs or TPUs, how fast is training. So for a given time of training, how much accuracy do you get in there, since it's particularly built for that, as you can see, it beats efficient net by a lot. However, if you look at this in terms of flops, they have a graph down here. So if you look at this in terms of flops versus accuracy, as you can see, it aligns with efficient net. So the kind of line here is pretty, as you can see, like it's pretty straight. It's as if you were to scale up the efficient net architecture for a bit more in terms of flops. So this is better in terms of so this is more optimized for current hardware, this kind of of networks. Yeah, so that is pretty much it they do do a lot of ablations comparisons. And it's not like I don't believe that the adaptive gradient clipping is, you know, does nothing or that, you know, clearly they also they always do experiments, they compare the normalizer free res nets with the batch on res net. So they try to isolate the individual parts. Still, I'm not sure how I feel about papers that have, you know, a lot of different things in one paper. And then they get state of the art, you never exactly know why that is. And the last thing I want to mention that's cool about this paper is appendix E, appendix E show you that appendix E is negative results. And this is really cool. So here is a list of all the stuff they tried that didn't work. And you know, it's one page, but still, it is very, very good, even if it's only to see that other researchers try a whole lot of stuff and fail as well. So I invite you to check out the paper, I've linked the code, you can take the code, it's in jacks, which \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 699,\n        \"samples\": [\n          \"2020-12-01T15:31:42Z\",\n          \"2017-06-22 09:44:05\",\n          \"2020-09-25T17:49:03Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"https://youtu.be/B9PL__gVxLI\",\n          \"https://youtu.be/YaPXHoOo9Pw\",\n          \"https://youtu.be/rNkHjZtH0RQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"B9PL__gVxLI\",\n          \"YaPXHoOo9Pw\",\n          \"rNkHjZtH0RQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"UCr8O8l5cCX85Oem1d18EezQ\",\n          \"UCobqgqE4i5Kf7wrxRxhToQA\",\n          \"UCZHmQk67mSJgfCCTn7xBfew\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 700,\n        \"samples\": [\n          \"576e795a-af09-4202-ae1a-891e8c709071\",\n          \"ada1aecf-feb3-44f0-9122-9c0c3f95bd41\",\n          \"982bf11b-d644-45d8-9758-8d0759a4ba00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "grouped_youtube"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-46a37695-a3e5-4c8c-8878-7aa154f17b71\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$5 MILLION AI for FREE</td>\n",
              "      <td>Imagine an AI where all in the same model you ...</td>\n",
              "      <td>2022-08-12T15:18:07Z</td>\n",
              "      <td>https://youtu.be/3EjtHs_lXnk</td>\n",
              "      <td>3EjtHs_lXnk</td>\n",
              "      <td>UCfzlCWGWYyIQ0aLC5w48gBQ</td>\n",
              "      <td>47a99c9d-16d1-4259-9c79-13e71d08c6cf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 week out from SPARTAN Race + How to bake a s...</td>\n",
              "      <td>What's going on guys welcome back to another S...</td>\n",
              "      <td>2018-04-26 07:36:40</td>\n",
              "      <td>https://youtu.be/04HX2zgQNXE</td>\n",
              "      <td>04HX2zgQNXE</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>d2dd76ae-5060-4dfd-98bd-d0bde70a2369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10-minute Bodyweight Back &amp;amp; Shoulders Bedr...</td>\n",
              "      <td>You know in the coming months, leadership at e...</td>\n",
              "      <td>2020-03-25 08:59:18</td>\n",
              "      <td>https://youtu.be/SBLp0Z4pJko</td>\n",
              "      <td>SBLp0Z4pJko</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>2bbdbfd3-bc45-47e5-997a-759e366a8c65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10-minute Lower Body Bodyweight Workout for Be...</td>\n",
              "      <td>Oh, I'm building this hat. Look how good it is...</td>\n",
              "      <td>2020-04-03 07:09:41</td>\n",
              "      <td>https://youtu.be/HtSeYhm1e7A</td>\n",
              "      <td>HtSeYhm1e7A</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>37071f8c-ec06-4035-8c47-aa07d57cd052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10-minute Morning Wake Up Stretching Routine f...</td>\n",
              "      <td>Good morning! We are up to day 14 of Reps For ...</td>\n",
              "      <td>2020-03-30 08:47:10</td>\n",
              "      <td>https://youtu.be/wm5QkbQ5LFI</td>\n",
              "      <td>wm5QkbQ5LFI</td>\n",
              "      <td>UCr8O8l5cCX85Oem1d18EezQ</td>\n",
              "      <td>beb445ba-b3dc-471b-a724-541851873ace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>git for research basics: fundamentals, commits...</td>\n",
              "      <td>Hi there. Today we're taking a look at Git and...</td>\n",
              "      <td>2017-12-13T07:40:17Z</td>\n",
              "      <td>https://youtu.be/BBp0tHcirtQ</td>\n",
              "      <td>BBp0tHcirtQ</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>ab5ba9b4-91a7-41d3-a920-b2cea39bc3ff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>iMAML: Meta-Learning with Implicit Gradients (...</td>\n",
              "      <td>Hi there, today we're looking at meta-learning...</td>\n",
              "      <td>2020-05-19T13:37:21Z</td>\n",
              "      <td>https://youtu.be/u5BkO8XMS2I</td>\n",
              "      <td>u5BkO8XMS2I</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>eb1a709d-6e96-486a-ba33-c51bba2bd915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>mixup: Beyond Empirical Risk Minimization (Pap...</td>\n",
              "      <td>Hi there, today we'll look at Mix-up beyond em...</td>\n",
              "      <td>2020-05-27T14:13:12Z</td>\n",
              "      <td>https://youtu.be/a-VQfQqIMrE</td>\n",
              "      <td>a-VQfQqIMrE</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>a66ee38a-ec6a-42f2-be95-e56d9b8acd6c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>âˆž-former: Infinite Memory Transformer (aka Inf...</td>\n",
              "      <td>Hello there, today we'll look at Infinityforme...</td>\n",
              "      <td>2021-09-06T12:07:08Z</td>\n",
              "      <td>https://youtu.be/0JlB9gufTw8</td>\n",
              "      <td>0JlB9gufTw8</td>\n",
              "      <td>UCZHmQk67mSJgfCCTn7xBfew</td>\n",
              "      <td>154ede3d-e19b-45a2-bf07-ae330778aceb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>ðŸ¤— Hugging Face just released *Diffusers* - for...</td>\n",
              "      <td>Today we're going to have a look at a new libr...</td>\n",
              "      <td>2022-07-26 15:27:46 UTC</td>\n",
              "      <td>https://youtu.be/UzkdOg7wWmI</td>\n",
              "      <td>UzkdOg7wWmI</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>bcb4bddd-b3f6-4ef1-abd5-d9fa80af6829</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46a37695-a3e5-4c8c-8878-7aa154f17b71')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-46a37695-a3e5-4c8c-8878-7aa154f17b71 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-46a37695-a3e5-4c8c-8878-7aa154f17b71');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb6b3d79-8967-4e5f-8b08-fbbae0d9111b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb6b3d79-8967-4e5f-8b08-fbbae0d9111b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb6b3d79-8967-4e5f-8b08-fbbae0d9111b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dfba7e02-0335-4c28-96f7-ac4fa596ad90\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('grouped_youtube')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dfba7e02-0335-4c28-96f7-ac4fa596ad90 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('grouped_youtube');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 title  \\\n",
              "0                               $5 MILLION AI for FREE   \n",
              "1    1 week out from SPARTAN Race + How to bake a s...   \n",
              "2    10-minute Bodyweight Back &amp; Shoulders Bedr...   \n",
              "3    10-minute Lower Body Bodyweight Workout for Be...   \n",
              "4    10-minute Morning Wake Up Stretching Routine f...   \n",
              "..                                                 ...   \n",
              "695  git for research basics: fundamentals, commits...   \n",
              "696  iMAML: Meta-Learning with Implicit Gradients (...   \n",
              "697  mixup: Beyond Empirical Risk Minimization (Pap...   \n",
              "698  âˆž-former: Infinite Memory Transformer (aka Inf...   \n",
              "699  ðŸ¤— Hugging Face just released *Diffusers* - for...   \n",
              "\n",
              "                                                  text  \\\n",
              "0    Imagine an AI where all in the same model you ...   \n",
              "1    What's going on guys welcome back to another S...   \n",
              "2    You know in the coming months, leadership at e...   \n",
              "3    Oh, I'm building this hat. Look how good it is...   \n",
              "4    Good morning! We are up to day 14 of Reps For ...   \n",
              "..                                                 ...   \n",
              "695  Hi there. Today we're taking a look at Git and...   \n",
              "696  Hi there, today we're looking at meta-learning...   \n",
              "697  Hi there, today we'll look at Mix-up beyond em...   \n",
              "698  Hello there, today we'll look at Infinityforme...   \n",
              "699  Today we're going to have a look at a new libr...   \n",
              "\n",
              "                   published                           url     video_id  \\\n",
              "0       2022-08-12T15:18:07Z  https://youtu.be/3EjtHs_lXnk  3EjtHs_lXnk   \n",
              "1        2018-04-26 07:36:40  https://youtu.be/04HX2zgQNXE  04HX2zgQNXE   \n",
              "2        2020-03-25 08:59:18  https://youtu.be/SBLp0Z4pJko  SBLp0Z4pJko   \n",
              "3        2020-04-03 07:09:41  https://youtu.be/HtSeYhm1e7A  HtSeYhm1e7A   \n",
              "4        2020-03-30 08:47:10  https://youtu.be/wm5QkbQ5LFI  wm5QkbQ5LFI   \n",
              "..                       ...                           ...          ...   \n",
              "695     2017-12-13T07:40:17Z  https://youtu.be/BBp0tHcirtQ  BBp0tHcirtQ   \n",
              "696     2020-05-19T13:37:21Z  https://youtu.be/u5BkO8XMS2I  u5BkO8XMS2I   \n",
              "697     2020-05-27T14:13:12Z  https://youtu.be/a-VQfQqIMrE  a-VQfQqIMrE   \n",
              "698     2021-09-06T12:07:08Z  https://youtu.be/0JlB9gufTw8  0JlB9gufTw8   \n",
              "699  2022-07-26 15:27:46 UTC  https://youtu.be/UzkdOg7wWmI  UzkdOg7wWmI   \n",
              "\n",
              "                   channel_id                                    id  \n",
              "0    UCfzlCWGWYyIQ0aLC5w48gBQ  47a99c9d-16d1-4259-9c79-13e71d08c6cf  \n",
              "1    UCr8O8l5cCX85Oem1d18EezQ  d2dd76ae-5060-4dfd-98bd-d0bde70a2369  \n",
              "2    UCr8O8l5cCX85Oem1d18EezQ  2bbdbfd3-bc45-47e5-997a-759e366a8c65  \n",
              "3    UCr8O8l5cCX85Oem1d18EezQ  37071f8c-ec06-4035-8c47-aa07d57cd052  \n",
              "4    UCr8O8l5cCX85Oem1d18EezQ  beb445ba-b3dc-471b-a724-541851873ace  \n",
              "..                        ...                                   ...  \n",
              "695  UCZHmQk67mSJgfCCTn7xBfew  ab5ba9b4-91a7-41d3-a920-b2cea39bc3ff  \n",
              "696  UCZHmQk67mSJgfCCTn7xBfew  eb1a709d-6e96-486a-ba33-c51bba2bd915  \n",
              "697  UCZHmQk67mSJgfCCTn7xBfew  a66ee38a-ec6a-42f2-be95-e56d9b8acd6c  \n",
              "698  UCZHmQk67mSJgfCCTn7xBfew  154ede3d-e19b-45a2-bf07-ae330778aceb  \n",
              "699  UCv83tO5cePwHMt1952IVVHw  bcb4bddd-b3f6-4ef1-abd5-d9fa80af6829  \n",
              "\n",
              "[700 rows x 7 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from uuid import uuid4\n",
        "\n",
        "def add_additional_info(row):\n",
        "  row['published'] = youtube[youtube['title'] == row['title']].iloc[0]['published']\n",
        "  row['url'] = youtube[youtube['title'] == row['title']].iloc[0]['url']\n",
        "  row['video_id'] = youtube[youtube['title'] == row['title']].iloc[0]['video_id']\n",
        "  row['channel_id'] = youtube[youtube['title'] == row['title']].iloc[0]['channel_id']\n",
        "  row['id'] = str(uuid4())\n",
        "  return row\n",
        "\n",
        "grouped_youtube = grouped_youtube.apply(add_additional_info, axis=1)\n",
        "grouped_youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cSye4sb9R2Hz",
        "outputId": "cdd3a1fd-a84b-4977-a188-9492b60abba2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Training and Testing an Italian BERT - Transformers From Scratch #4'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "youtube.iloc[0]['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "o0qXrVu6SgHn",
        "outputId": "38a370ba-27c6-4c88-c4d9-9a011bc90e73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e752e7ff-7d7b-4331-bb3d-52cef54af591\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>Hi, welcome to the video. So this is the fourt...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>85fdfb39-07f4-4574-9e27-02a9f264782c</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e752e7ff-7d7b-4331-bb3d-52cef54af591')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e752e7ff-7d7b-4331-bb3d-52cef54af591 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e752e7ff-7d7b-4331-bb3d-52cef54af591');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 title  \\\n",
              "576  Training and Testing an Italian BERT - Transfo...   \n",
              "\n",
              "                                                  text  \\\n",
              "576  Hi, welcome to the video. So this is the fourt...   \n",
              "\n",
              "                   published                           url     video_id  \\\n",
              "576  2021-07-06 13:00:03 UTC  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ   \n",
              "\n",
              "                   channel_id                                    id  \n",
              "576  UCv83tO5cePwHMt1952IVVHw  85fdfb39-07f4-4574-9e27-02a9f264782c  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_youtube[grouped_youtube['title'] == 'Training and Testing an Italian BERT - Transformers From Scratch #4']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "G1SNYT0LSsG2",
        "outputId": "bea08856-96b1-42c8-f6e1-38ced6be8c4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"youtube[youtube['title'] == 'Training and Testing an Italian BERT - Transformers From Scratch #4']\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Training and Testing an Italian BERT - Transformers From Scratch #4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2021-07-06 13:00:03 UTC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://youtu.be/35Pdoyi6ZoQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"35Pdoyi6ZoQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"UCv83tO5cePwHMt1952IVVHw\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"35Pdoyi6ZoQ-t3.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"So this is the fourth video in a Transformers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.4221740867092665,\n        \"min\": 0.0,\n        \"max\": 15.84,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.681828702547756,\n        \"min\": 9.36,\n        \"max\": 20.6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          11.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-19c2f323-0886-486a-b1f8-29cb9f1d2af9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
              "      <td>Hi, welcome to the video.</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t3.0</td>\n",
              "      <td>So this is the fourth video in a Transformers</td>\n",
              "      <td>3.00</td>\n",
              "      <td>11.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t9.36</td>\n",
              "      <td>from Scratch mini series.</td>\n",
              "      <td>9.36</td>\n",
              "      <td>15.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t11.56</td>\n",
              "      <td>So if you haven't been following along,</td>\n",
              "      <td>11.56</td>\n",
              "      <td>18.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "      <td>35Pdoyi6ZoQ</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>35Pdoyi6ZoQ-t15.84</td>\n",
              "      <td>we've essentially covered what you can see on ...</td>\n",
              "      <td>15.84</td>\n",
              "      <td>20.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19c2f323-0886-486a-b1f8-29cb9f1d2af9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19c2f323-0886-486a-b1f8-29cb9f1d2af9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19c2f323-0886-486a-b1f8-29cb9f1d2af9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-54daa544-3123-4e94-81f3-a86f9d1668af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-54daa544-3123-4e94-81f3-a86f9d1668af')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-54daa544-3123-4e94-81f3-a86f9d1668af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               title                published  \\\n",
              "0  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "1  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "2  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "3  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "4  Training and Testing an Italian BERT - Transfo...  2021-07-06 13:00:03 UTC   \n",
              "\n",
              "                            url     video_id                channel_id  \\\n",
              "0  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "1  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "2  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "3  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "4  https://youtu.be/35Pdoyi6ZoQ  35Pdoyi6ZoQ  UCv83tO5cePwHMt1952IVVHw   \n",
              "\n",
              "                   id                                               text  \\\n",
              "0    35Pdoyi6ZoQ-t0.0                          Hi, welcome to the video.   \n",
              "1    35Pdoyi6ZoQ-t3.0      So this is the fourth video in a Transformers   \n",
              "2   35Pdoyi6ZoQ-t9.36                          from Scratch mini series.   \n",
              "3  35Pdoyi6ZoQ-t11.56            So if you haven't been following along,   \n",
              "4  35Pdoyi6ZoQ-t15.84  we've essentially covered what you can see on ...   \n",
              "\n",
              "   start    end  \n",
              "0   0.00   9.36  \n",
              "1   3.00  11.56  \n",
              "2   9.36  15.84  \n",
              "3  11.56  18.48  \n",
              "4  15.84  20.60  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "youtube[youtube['title'] == 'Training and Testing an Italian BERT - Transformers From Scratch #4'].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "gLLht3TuSyWc",
        "outputId": "0977e2c3-8fc5-4608-c57f-f8ae92e0f054"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>published</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>url</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>video_id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>channel_id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "title         0\n",
              "text          0\n",
              "published     0\n",
              "url           0\n",
              "video_id      0\n",
              "channel_id    0\n",
              "id            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_youtube.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz7lsel3gi_K"
      },
      "source": [
        "# The text column with None or Empty string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "6BaHirxCfin7",
        "outputId": "f0228e45-1503-4316-8ee7-8e6c486da349"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-56e2c070-5349-4d94-bcc7-cee8b7677216\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>A Deep Dream of a Neural Network</td>\n",
              "      <td></td>\n",
              "      <td>2018-03-31T14:02:40Z</td>\n",
              "      <td>https://youtu.be/sh-MQboWJug</td>\n",
              "      <td>sh-MQboWJug</td>\n",
              "      <td>UCfzlCWGWYyIQ0aLC5w48gBQ</td>\n",
              "      <td>a8b6fe40-e746-40cd-97fd-b0c7b54cd54b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56e2c070-5349-4d94-bcc7-cee8b7677216')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56e2c070-5349-4d94-bcc7-cee8b7677216 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56e2c070-5349-4d94-bcc7-cee8b7677216');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                               title text             published  \\\n",
              "19  A Deep Dream of a Neural Network       2018-03-31T14:02:40Z   \n",
              "\n",
              "                             url     video_id                channel_id  \\\n",
              "19  https://youtu.be/sh-MQboWJug  sh-MQboWJug  UCfzlCWGWYyIQ0aLC5w48gBQ   \n",
              "\n",
              "                                      id  \n",
              "19  a8b6fe40-e746-40cd-97fd-b0c7b54cd54b  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_youtube[grouped_youtube['text'] == '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "l25RVNspgcMW",
        "outputId": "49bf0386-ecbc-457c-9ae5-840a0d7bd0b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-62557e5a-adff-4fec-b29f-15b15ec135fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62557e5a-adff-4fec-b29f-15b15ec135fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62557e5a-adff-4fec-b29f-15b15ec135fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62557e5a-adff-4fec-b29f-15b15ec135fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, text, published, url, video_id, channel_id, id]\n",
              "Index: []"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_youtube[grouped_youtube['text'] == None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TGOunWo-hNOs"
      },
      "outputs": [],
      "source": [
        "grouped_youtube.replace('', 'Pause', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cPNcODyghjmG",
        "outputId": "888c069d-3bce-46ab-f587-56cf476921cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-be9f28aa-3ca4-49ef-8b86-31ee758bab18\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>published</th>\n",
              "      <th>url</th>\n",
              "      <th>video_id</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be9f28aa-3ca4-49ef-8b86-31ee758bab18')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be9f28aa-3ca4-49ef-8b86-31ee758bab18 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be9f28aa-3ca4-49ef-8b86-31ee758bab18');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, text, published, url, video_id, channel_id, id]\n",
              "Index: []"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_youtube[grouped_youtube['text'] == '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g8rw7VPRWJN",
        "outputId": "94bd6189-5e27-4518-ff8d-79216725765a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(700,)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_youtube['text'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCtlI0CMNxci",
        "outputId": "c30c0af5-b616-4dd6-8fa1-0d3fd88640d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5, 702, 565, 355, 128, 680, 568, 891, 1261, 1838, 75]\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "tokens = [len(text) for text in grouped_youtube['text']]\n",
        "\n",
        "leq_list = []\n",
        "for token in tokens:\n",
        "  if token <= 2000:\n",
        "    leq_list.append(token)\n",
        "\n",
        "print(leq_list)\n",
        "print(len(leq_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPrnsy4MW3jH"
      },
      "source": [
        "# Initialize Pinecone and NVIDIAEmbeddings and ChatNVIDIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j4PFnbwCWYUy"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# 'pcsk_7KR79t_4aNy34dtuqxqcKVymczPhSafRkT7oZ3YSAgQ7s1DAseejzcXP3q1BwGMHH2RLu7'\n",
        "pc = Pinecone(api_key='pcsk_6NEJ7Z_Q735aCYzuXzxszWoUDiX2LCvr3MvS3z4Uraajr5yEEvBZpJpupPSfMkCcbrRX57', pool_threads=30)\n",
        "\n",
        "if not pc.has_index('rag-chatbot'):\n",
        "  pc.create_index(\n",
        "      name='rag-chatbot',\n",
        "      dimension=1536,\n",
        "      metric='cosine',\n",
        "      spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "  )\n",
        "\n",
        "llm = ChatNVIDIA(\n",
        "    model=\"meta/llama-3.3-70b-instruct\",\n",
        "    api_key=\"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\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "embedding_func = NVIDIAEmbeddings(\n",
        "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
        "    api_key=\"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\",\n",
        "    dimensions=1536\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARcWaCl7YMK3"
      },
      "source": [
        "# Ingesting Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HsTBBu67YEOu"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def create_chunks(iterable, chunk_size=100):\n",
        "  it = iter(iterable)\n",
        "  chunk = tuple(itertools.islice(it, chunk_size))\n",
        "  while chunk:\n",
        "    yield chunk\n",
        "    chunk = tuple(itertools.islice(it, chunk_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbCDEgkS_exW"
      },
      "source": [
        "to handle this error\n",
        "\n",
        "```\n",
        "message\":\"Error, message length too large: found 7556572 bytes, the limit is: 4194304 bytes\n",
        "```\n",
        "we should either shrink the size of metadata or reduce the size of chunk_size in the create_chunks function. This makes the size of vectors in each ingestion don't exceed the limit of the starter Pinecone account, which is 4MB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAPtvoThxSUo",
        "outputId": "dc95e913-95e9-484a-bc8f-73b67b652b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 11s, sys: 2.15 s, total: 1min 13s\n",
            "Wall time: 3min 58s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def ingester(ids:list, texts:list, metadata:list):\n",
        "  \"\"\"\n",
        "  Ingesting documents into Pinecone using parallel batching\n",
        "  \"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      # Set a suitable chunk size considering your text length and token limit\n",
        "      separators=['\\n\\n', '\\n', ' ', ''],\n",
        "      chunk_size = 2000,\n",
        "      chunk_overlap  = 200,\n",
        "      length_function = len,\n",
        "  )\n",
        "\n",
        "  all_texts = []\n",
        "  all_metadatas = []\n",
        "  all_ids = []\n",
        "\n",
        "  for i, text in enumerate(texts):\n",
        "    if len(text) > 4000:\n",
        "      # Split the text into smaller chunks\n",
        "      splits = text_splitter.create_documents([text])\n",
        "      for s_i, split in enumerate(splits):\n",
        "          all_texts.append(split.page_content)\n",
        "          # Instead of storing the entire text, store a reference to the ID and chunk id\n",
        "          all_metadatas.append({**metadata[i], 'original_id': f\"{ids[i]}-s{s_i}\"})\n",
        "          all_ids.append(f\"{ids[i]}-s{s_i}\")\n",
        "    else:\n",
        "      all_texts.append(text)\n",
        "      all_metadatas.append({**metadata[i], 'original_id': ids[i]})\n",
        "      all_ids.append(ids[i])\n",
        "\n",
        "  embedded_texts = embedding_func.embed_documents(all_texts)\n",
        "  vectors = zip(all_ids, embedded_texts, all_metadatas)\n",
        "\n",
        "  with pc.Index(name='rag-chatbot', pool_threads=30) as index:\n",
        "    async_results = [index.upsert(vectors=chunk, namespace='youtube-transcriptions', async_req=True) for chunk in create_chunks(vectors, chunk_size=50)]\n",
        "    return index, async_results, all_ids\n",
        "\n",
        "\n",
        "# 'start': row['start'], 'end': row['end']\n",
        "ids = grouped_youtube['id'].tolist()\n",
        "texts = grouped_youtube['text'].tolist()\n",
        "metadata = [{'title': row['title'], 'published': row['published'],\n",
        "             'url': row['url'], 'video_id': row['video_id'],\n",
        "             'channel_id': row['channel_id']} for _, row in grouped_youtube.iterrows()]\n",
        "\n",
        "index, async_results, all_ids = ingester(ids, texts, metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGefKec9DOyL"
      },
      "source": [
        "# Verify Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEdnNGD-k3gN",
        "outputId": "3142697f-a9bb-4f74-f961-33a345d37c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully upserted 179 chunks\n"
          ]
        }
      ],
      "source": [
        "# Wait for all async operations to complete and check results\n",
        "results = [async_result.get() for async_result in async_results]\n",
        "success_count = sum(1 for r in results if r.get('upserted_count', 0) > 0)\n",
        "print(f\"Successfully upserted {success_count} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmdf09_nVsSd",
        "outputId": "a0d99290-cf5c-4f1d-a02c-d6c59942d1a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8950"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# There is 179 chunks. each chunk contains 50 records.\n",
        "179 * 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S16JL-yTNGrW",
        "outputId": "21d8f72f-d380-4696-d2ba-a6bbe1fc2e56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8918"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVgldvLuBx5t",
        "outputId": "c7d77492-bccc-41a6-bb21-15c255c25d94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'youtube-transcriptions': {'vector_count': 8918}},\n",
              " 'total_vector_count': 8918,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnvqEQ-4yrDa",
        "outputId": "7ead9c7e-ca96-4794-b4b3-9e1416726600"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FetchResponse(namespace='youtube-transcriptions', vectors={'47a99c9d-16d1-4259-9c79-13e71d08c6cf-s0': Vector(id='47a99c9d-16d1-4259-9c79-13e71d08c6cf-s0', values=[0.0344238281, 0.00436401367, 0.00965118408, 0.00307273865, 0.0366821289, 0.0332336426, -0.039642334, -0.0131454468, 0.0001475811, -0.00656509399, -0.0323791504, 0.0197753906, -0.0100631714, 0.0297088623, -0.0113372803, 0.00269317627, 0.0293579102, -0.0160827637, -0.0025100708, 0.0422668457, -0.0274505615, 0.00232887268, 0.00447464, 0.00434494, 0.0142059326, -0.0274810791, -0.0244293213, -0.0208282471, -0.00561904907, 0.0150756836, -0.0428771973, -0.00618743896, 0.0189361572, 0.0595397949, -0.0570373535, -0.0273895264, 0.0248565674, 0.0726928711, 0.0157470703, -0.0199432373, 0.0428466797, 0.0607910156, -0.00047492981, -0.00103092194, -0.0292816162, -0.0218048096, -0.0239868164, -0.0440673828, -0.00470733643, -0.0152359009, -0.0112228394, -0.0321350098, 0.017868042, -0.0413208, -0.0449523926, 0.0266723633, -0.0129318237, 0.00413513184, 0.0305023193, -0.0173187256, -0.00030040741, 0.0197296143, 0.0157928467, 0.0451965332, 0.0227508545, 0.00920105, -0.00123500824, -0.0128250122, 0.0198974609, -0.021697998, 0.0499572754, -0.0204162598, -0.0391235352, -0.00307846069, -0.00926971436, 0.0223236084, -0.0126571655, -0.0202331543, -0.0292053223, -0.00197219849, -0.0214538574, 0.0230407715, -0.00254249573, 0.0793457, 0.0109558105, -0.0183868408, -0.00885772705, -0.00760269165, 0.0065574646, -0.0479125977, -0.0377502441, -0.00154495239, -0.0115737915, 0.00311470032, 0.0348510742, -0.0505371094, -0.00310707092, 0.025894165, 0.046875, 0.00591659546, -0.0113296509, -0.0111083984, 0.0317993164, 0.00263786316, -0.0451049805, 0.013168335, 0.00657272339, 0.0288391113, -0.00686264038, 0.0175933838, -0.0420837402, -0.0100631714, 0.0312805176, -0.01197052, 0.0515441895, 0.0138626099, 0.0132446289, -0.0166625977, -0.0249786377, 0.0783691406, -0.0083694458, -0.00508880615, 0.00681686401, -0.00211524963, 0.0512084961, -0.00855255127, 0.00263786316, 0.0423584, -0.0361328125, -0.00836181641, 0.00456619263, -0.0283813477, -0.0085067749, -0.00517654419, -0.0228881836, 0.0225067139, -0.0186767578, -0.010055542, -0.00069475174, 0.00428390503, 0.0516357422, -0.0117950439, 0.00621032715, 0.0094909668, -0.00224494934, -0.00498199463, -0.00632095337, -0.00339508057, 0.0386962891, -0.0180206299, 0.0310058594, -0.0214385986, 0.00574493408, 0.00462341309, 0.00710678101, 0.0293579102, 0.0020904541, 0.0290527344, -0.0110549927, 0.0257873535, -0.0289306641, 0.00360870361, 0.00574874878, -0.00636291504, 0.00798797607, -0.033416748, -0.0238189697, 0.0400695801, 0.0213623047, 0.0623474121, -0.0432739258, -0.00749969482, -0.0391235352, 0.0277404785, 0.00573349, -0.00459289551, 0.0218048096, 0.00849151611, -0.0102539062, 0.0174865723, -0.0437927246, 0.0321960449, 0.0377807617, 0.0182495117, 0.02293396, -0.000346183777, 0.019317627, 0.0053062439, 0.00499725342, 0.0630493164, 0.0182952881, -0.0452575684, -0.0103530884, -0.00131607056, 0.0444641113, -0.0261230469, 0.0207519531, -0.00157642365, -5.89489937e-05, 0.012588501, -0.0422973633, -0.0162353516, -0.00296211243, 0.000154495239, -0.0170593262, -0.0229949951, -0.0149383545, 0.0043258667, 0.00762176514, -0.0223693848, 0.032623291, -0.00968170166, 0.0265655518, 0.0154876709, -0.00056886673, -0.000880241394, 0.0252227783, -0.0044708252, -0.036895752, 0.00638961792, 0.0078125, 0.0174713135, -0.00689697266, -0.000841140747, 0.0333862305, -0.0075340271, 0.0183563232, -0.0523071289, 0.0293121338, -0.0465087891, 0.0044670105, -0.0161590576, -0.00803375244, -0.0341186523, -0.0125198364, 0.0150604248, 0.0375976562, -0.00370597839, 0.0113601685, -0.0307617188, -0.0186309814, 0.0100402832, -0.0221557617, -0.0114135742, 0.0361938477, -0.0242156982, 0.00291442871, -0.0252990723, -0.00949859619, -0.025680542, 0.0218505859, -0.0157928467, -0.0650024414, -0.0039100647, 0.00110816956, 0.0156097412, -0.00752258301, 0.0418701172, 0.01222229, -0.00109958649, -0.0100631714, 0.00730514526, 0.00800323486, 0.0146331787, -0.0594787598, 0.0282440186, 0.0165710449, -0.000463962555, 0.0202484131, 0.0486450195, -7.18236e-05, 0.0168762207, -0.0164031982, 0.0212097168, 0.0231933594, 0.0218963623, -0.0303649902, -0.0179443359, 0.0246887207, 0.0479125977, -0.0764160156, 0.0735473633, 0.00323486328, -0.0103530884, -0.00413513184, 0.0329284668, 0.00176143646, 0.000519752502, -0.0405578613, 0.0129394531, 0.0352783203, 0.00399398804, 0.0109024048, -0.0217437744, -0.0158233643, 0.00542068481, -0.00524139404, 0.0263671875, 0.0213012695, -0.0286712646, 0.00329971313, 0.0161895752, -0.0399169922, -0.00135707855, -0.00475311279, 0.0403137207, 0.0158233643, -0.0285491943, -0.0424499512, 0.00363349915, 0.025894165, 0.0151138306, 0.0351867676, 0.0342102051, -0.0372619629, -0.0341796875, -0.00814056396, -0.0204315186, -0.0106887817, -0.00186347961, -0.0258331299, 0.0238647461, 0.00381851196, -0.0102767944, -0.0331420898, -0.000728130341, -0.0393371582, -0.00286102295, 0.0306854248, -0.0228271484, 0.00139904022, -0.0148544312, 0.000255584717, -0.0241546631, -0.00838470459, 0.0120544434, -0.0273132324, -0.0179290771, -0.0351867676, 0.0193023682, 0.0222930908, 0.0349121094, -0.0456848145, 0.0120849609, -0.0222167969, -0.0126571655, 0.0120162964, -0.00340461731, -0.0223236084, 0.0225830078, -0.0413208, 0.00525283813, -0.0158691406, -0.0265960693, -0.019241333, -0.0311431885, 0.00840759277, 0.0420227051, -0.00574111938, -0.0213623047, -0.0502624512, -0.058380127, -0.0446777344, -0.00754928589, -0.0137939453, -0.0546569824, 0.0285186768, -0.0020160675, -0.00792694092, 0.0276794434, -0.0187530518, 0.00642776489, 0.00836181641, 0.00357627869, 0.0492248535, -0.0233001709, 0.00929260254, -0.0309906, 0.0272216797, -0.0218658447, 0.0320739746, 0.0275726318, -0.0186309814, -0.0135726929, 0.0257720947, -0.00949859619, -0.01222229, 0.0225524902, 0.0148620605, -0.0170593262, 0.00358772278, 0.0257110596, -0.0160064697, 0.000268936157, -0.0304718018, 0.062286377, 0.00393295288, -0.0505371094, 0.0379943848, 0.0299987793, 0.000293731689, -0.0162811279, 0.0177764893, 0.033203125, 0.0139312744, -0.0288848877, -0.03515625, 0.0212097168, -0.00345230103, 0.005859375, 0.0286102295, 0.0346069336, -4.8995018e-05, 0.0110549927, 0.000870704651, -0.0258789062, -0.0260314941, -0.013420105, -0.0128250122, -0.0361328125, -0.0146789551, 0.00317955017, 0.0143127441, -0.0236053467, -0.0269622803, 0.0337524414, -0.033416748, -0.0100402832, 0.00104236603, 0.0174713135, -0.0114212036, 0.0251922607, 0.00136947632, -0.00491333, -0.0213165283, 0.0132141113, 0.0142364502, -0.052734375, 0.00717926025, 0.025100708, -0.0143051147, 0.0219726562, 0.0057220459, 0.00882720947, 0.0062828064, 0.0331726074, 0.0318603516, -0.0492248535, 0.0356140137, 0.0351867676, -0.0183410645, 0.0169677734, -0.0226287842, 0.0114440918, 0.00680923462, -0.0148696899, -0.0265960693, 0.0311126709, 0.00952911377, 0.0156097412, -0.0161590576, 0.0105743408, 0.00231933594, 0.0119552612, -0.0131454468, 0.0121688843, 0.00681304932, 0.00878143311, -0.0203704834, 0.0284118652, 2.57492065e-05, 0.00288200378, -0.0120010376, 0.0131225586, -0.0135879517, 0.00648498535, -0.033416748, -0.0140533447, -0.00782775879, -0.0147781372, 0.00391769409, -0.0201568604, 0.0180664062, 0.0303039551, 0.0476379395, 0.0408325195, -0.00489425659, -0.0235900879, 0.0201263428, 0.0198364258, -0.00400543213, 0.0102539062, 0.0437927246, -0.018951416, 0.0247497559, -0.00647735596, 0.00110435486, 0.0629272461, -0.00298309326, 0.0179595947, -0.0257263184, -0.00369262695, -0.00978088379, -0.0147247314, 0.00330352783, -0.00525283813, 0.0146331787, 0.0171051025, 0.0157012939, -0.00598907471, -0.00233840942, 0.0567626953, 0.0333862305, -0.0213775635, -0.000782012939, 0.0235748291, 0.00355148315, -0.0557556152, -0.0117874146, 0.00852203369, 0.024810791, -0.0317993164, 0.011932373, 0.00300788879, 0.00463867188, 0.0207519531, -0.01612854, 0.0160675049, 0.0241699219, -0.0242004395, -0.0274353027, -0.00315094, -0.0461730957, 0.0120391846, 0.0201263428, -0.017868042, -0.0101013184, 0.0211181641, 0.0447692871, -0.0293121338, 0.0320739746, 0.00635910034, 0.0445861816, 0.0137252808, 0.0115356445, 0.0342102051, 0.0170898438, -0.00994873047, 0.0328979492, -0.0132751465, -0.0266571045, -0.00559997559, -0.00726699829, 0.0163269043, 0.00548553467, -0.0156707764, 0.0128479, 0.0224761963, 0.049621582, 0.0539550781, -0.0225524902, 0.0279083252, -0.0197296143, 0.0128479, 0.00714492798, 0.0413818359, 0.0403747559, 0.008934021, 0.0485229492, -0.00551605225, -0.0171966553, 0.0136413574, 0.0512390137, -0.0458374023, 0.0341491699, -0.0240936279, -0.00096654892, -0.00326538086, 0.0210113525, 0.0168457031, 0.0135116577, -0.0273284912, -0.0523986816, -0.042175293, -0.00210571289, 0.0199584961, 0.0116653442, 0.052947998, 0.00741577148, 0.00589752197, 0.0351867676, 0.0380249023, -0.0222015381, 0.02293396, -0.0454406738, 0.00297927856, -0.0203704834, -0.0175170898, 0.00398254395, -0.000974178314, 0.0162963867, 0.00738143921, 0.0279388428, -0.0164031982, 0.0598144531, 0.0155410767, 0.0408630371, -0.0036315918, -0.032409668, -0.00651550293, -0.00779342651, 0.0484924316, -0.0433959961, 0.0177154541, 0.000400543213, 0.0281829834, -0.015914917, -0.0323791504, 0.00755691528, -0.000125646591, 0.0442504883, -0.00621032715, 0.0187072754, 0.0135726929, -0.0138320923, 0.0135650635, -0.0339050293, 0.0276031494, -0.0135650635, 0.0678710938, -0.0348815918, -0.00588989258, 0.00925445557, 0.0161895752, 0.0150146484, -0.00401687622, 0.0096206665, -0.0134887695, -0.0167236328, 0.0163574219, 0.0324401855, -0.00978851318, -0.0614929199, 0.0133361816, 0.0305633545, -0.00247001648, 0.00922393799, 0.0616760254, 0.0173034668, -0.0362854, 0.0158081055, -0.0527648926, -0.0109710693, 0.0142288208, -0.00439834595, -0.0244140625, 0.00542068481, 0.0559082031, -0.0180511475, -0.0368347168, 0.0131225586, 0.0117874146, -0.000267744064, -0.00306892395, -0.0186157227, 0.0278320312, -0.0208740234, -0.00500106812, 0.00478744507, -0.026763916, -0.0141830444, 0.0137939453, -0.0401916504, 0.0227050781, -0.0137405396, -0.000415086746, -0.0224151611, 0.0105896, -0.0122680664, 0.0106735229, 0.0191802979, 0.0076789856, 0.0212860107, -0.0145568848, 0.000659942627, 0.0275878906, 0.018371582, -0.00612640381, 0.0560302734, -0.00235366821, 0.00743484497, 0.00382041931, -0.0014629364, -0.00772857666, -0.0376281738, 0.00332832336, 0.0242614746, 0.0687866211, 0.0379943848, 0.00746917725, -0.00126743317, -0.0269927979, -0.00833892822, 0.0113830566, -0.00550079346, 0.011100769, -0.0422363281, 0.0343017578, 0.0198669434, -0.0392150879, -0.0122146606, -0.00961303711, -0.0267181396, -0.0366821289, -0.00900268555, -0.0055809021, 0.0581970215, -0.0164489746, 0.0168914795, 0.0362548828, -0.0234832764, 0.0284576416, -0.011680603, 0.0259094238, -0.0082244873, -0.0233154297, 0.0550231934, -0.0318603516, 0.0613098145, -0.0123138428, 0.0212554932, -0.0128707886, 0.0165100098, 0.025604248, 0.00801086426, -0.0181274414, -0.020324707, -0.0174255371, -0.0480041504, -0.0372924805, 0.0368042, -0.0214691162, 0.0151901245, 0.00772094727, 0.002576828, 0.063659668, -0.00885009766, 0.0522155762, 0.00229263306, -0.00138568878, -0.0092086792, -0.0424194336, -0.0333251953, 0.00796508789, 0.0234222412, -0.00312995911, -0.0325317383, -0.0396728516, 0.0323486328, -0.0151901245, -0.0446167, 0.00424575806, 0.0296020508, -0.0126342773, -0.0301208496, -0.0260314941, 1.15633011e-05, -0.0509033203, -0.0102233887, 0.0454101562, 0.0131225586, 0.0300598145, -0.0169830322, -0.00410461426, 0.00386047363, 0.00690078735, 0.00506591797, 0.0187988281, 0.0729370117, -0.0216064453, 0.00916290283, -0.0151824951, 0.0171966553, -0.0305480957, -0.0199890137, -0.0128936768, -2.46763229e-05, 0.012512207, -0.0148696899, 0.00154685974, -0.034942627, 0.0195007324, -0.0323791504, 0.00728225708, -0.0238342285, 0.0263671875, 0.00813293457, -0.0184173584, 0.0135955811, 0.0390014648, -0.0179748535, 0.0157318115, 0.00173568726, 0.0225830078, 0.0173797607, -0.00553894043, -0.0260009766, 0.0210876465, 0.0243225098, 0.00978088379, 0.065246582, -0.0134887695, -0.0252380371, 0.0140838623, 0.000726699829, -0.00999450684, 0.0105896, -0.00692367554, -0.00220680237, -0.0362854, 0.0148849487, -0.0338439941, 0.0358581543, 0.0166931152, 0.00522995, 0.0372314453, 0.0118942261, 0.0406494141, 0.0353088379, -0.00988006592, 0.0195007324, -0.00253105164, -0.01537323, -0.0249328613, 0.0350646973, 0.0214691162, 0.037902832, -0.0155410767, 0.0143508911, 0.0325317383, 0.012008667, 0.0307617188, -0.0415039062, 0.00206184387, 0.0130157471, -0.00536346436, -0.0255279541, 0.00764465332, -0.0198516846, -0.0383300781, 0.0261077881, 0.0745239258, -0.00731658936, 0.0102615356, 0.0326843262, -0.0257263184, 0.0145492554, -0.0153427124, -0.00620269775, -0.0186004639, -0.0160827637, 0.0531921387, 0.0279998779, 0.00410461426, 0.0197296143, 0.00606536865, -0.0145492554, 0.00941467285, 0.0240020752, 0.028213501, -0.00919342, -0.00325965881, 0.0188140869, 0.0124893188, 0.0235290527, 0.0498962402, -0.0152511597, -0.0262298584, -0.0563964844, -0.0253143311, 0.0100402832, 0.00931549072, -0.033782959, 0.000450134277, 0.0078125, -0.00499725342, -0.0495605469, -0.00704193115, -0.0382080078, 0.00735855103, -0.000961303711, 0.0256958, 0.0166168213, 0.0272827148, -0.0080871582, 0.0152816772, 0.0510864258, -0.0205078125, 0.00676727295, -0.0149536133, -0.0274047852, -0.0601501465, 0.00246047974, -0.000400304794, -0.00295257568, 0.0163116455, -0.00885009766, 0.0393676758, 0.068359375, 0.0151290894, 0.0267028809, -0.0376586914, 0.0127334595, 0.00184726715, -0.0206604, -0.0126571655, 0.0170135498, -0.0254821777, 0.00358581543, 0.0204315186, 0.0169677734, -0.0265350342, -0.0086517334, 0.0228424072, 0.0266265869, 0.016494751, -0.0189208984, -0.0244903564, -0.0269165039, 0.0034942627, 0.0386962891, -0.0186767578, 0.00375556946, -0.0277557373, -0.00158023834, -0.0146865845, -0.00582504272, 0.0329589844, 0.0267181396, -0.00130558014, 0.0336303711, -0.0215606689, 0.00262832642, 0.0109024048, 0.0299835205, 0.00471878052, 0.0116729736, 0.0202026367, -0.0215759277, -0.0149917603, 0.0417785645, 0.0127334595, -0.00288581848, 0.0270843506, -0.00854492188, -0.0604553223, -0.039276123, -0.0114593506, 0.00235176086, 0.0663452148, -0.0318908691, -0.00300216675, 0.0295715332, -0.0292358398, 0.0346374512, 0.0145111084, -0.00476837158, -0.00388145447, 0.00790405273, -0.00652694702, -0.0105514526, 0.0189361572, -0.0109100342, -0.0149307251, -0.00359916687, 0.0567321777, -0.0323791504, -0.00645828247, 0.0593261719, -0.0110702515, 0.0198364258, -0.00762558, 0.032989502, -0.0104904175, 0.0159912109, -0.000294208527, 0.00485610962, 0.0165405273, 0.000870227814, 0.01146698, -0.017868042, -0.0246734619, 0.00847625732, -0.0346374512, 0.00374603271, 0.00361061096, 0.0010766983, 0.0614318848, 0.0191802979, 0.0227355957, -0.0362548828, 0.0277252197, -0.0179138184, 0.0329284668, 0.00779342651, 0.0559082031, 0.00140571594, 0.00335121155, 0.0114059448, -0.0175170898, 0.0105667114, 0.0172119141, -0.0307922363, -0.00940704346, 0.0617675781, -0.010017395, -0.0391235352, -0.00677108765, -0.0357666, 0.00487518311, 0.00173091888, -0.00994873047, 0.00355148315, -0.0489807129, -0.063293457, 0.0114822388, 0.0196838379, -0.0205078125, 0.0202941895, -0.0495910645, 0.028213501, -0.0270690918, 0.0430297852, 0.000211000443, -0.0168609619, 0.033782959, -0.000395059586, 0.0032119751, -0.0204467773, -0.0227508545, -0.00424575806, -0.021270752, 0.0254516602, -0.0458068848, -0.0726318359, -0.0556640625, -0.018371582, -0.0193023682, 0.0473327637, -0.0277404785, -0.0231018066, -0.00727462769, -0.011505127, 0.0277709961, -0.0228424072, -0.0115814209, -0.0158233643, -0.000636577606, -0.00722503662, 0.0294342041, 0.000375270844, 0.0122909546, -0.00384521484, 0.00523757935, -6.19292259e-05, 0.0272827148, 0.021270752, 0.0205078125, -0.0193328857, 0.0377197266, 0.0265655518, -0.0157623291, -0.00649261475, 0.0221557617, -0.00649642944, 0.00891876221, 0.0104370117, -0.0067024231, -0.00339508057, -0.0547180176, 0.0458679199, -0.0408325195, 0.0305633545, 0.0580749512, -0.0132751465, 0.0586853027, -0.0363464355, -0.0227661133, 0.00214004517, 0.0017118454, 0.0166625977, 0.00918579102, 0.0303039551, 0.0462646484, -0.0325927734, 0.0353088379, -0.00292778015, 0.00816345215, -0.00717926025, 0.0156173706, -0.0272674561, -0.00459289551, 0.00914764404, 0.00485610962, 0.0342712402, -0.0121459961, 0.0211181641, -0.0325012207, 0.0294342041, 0.0443115234, 0.0147247314, -0.0144271851, 0.00217437744, 0.0162963867, 0.00179004669, -0.0218811035, -0.012260437, 0.0150299072, -0.00689697266, 0.0296173096, 0.0339050293, -0.0218963623, 0.0168457031, 0.000156879425, -0.0101852417, 0.0354614258, -0.036895752, 0.00939941406, -0.00190162659, 0.0376281738, -0.0218658447, 0.0150680542, 0.0185699463, 0.00437927246, 0.0621032715, -0.0142593384, -0.033782959, 0.0035572052, -0.0300292969, -0.0209350586, -0.0089263916, 0.0365905762, -0.00458526611, 0.0319213867, -0.012588501, 0.00925445557, -0.0657959, -0.00307273865, 0.0395202637, -0.0317993164, -0.0372924805, 0.0171661377, -0.0410766602, -0.00420379639, 0.023223877, 0.0189361572, -0.00323677063, 0.00244522095, 0.0366821289, -0.0018529892, 0.0274200439, 0.0210571289, -0.0122451782, -0.0027141571, 0.00715637207, -0.0101852417, -0.00459289551, 0.00653457642, 0.022064209, -0.0133361816, 0.000478982925, 0.0159454346, 0.0234680176, 0.00091362, 0.017868042, 0.0153808594, 0.0141220093, -0.00159740448, 0.0295410156, 0.0164794922, 0.0311126709, 0.0319824219, 0.017791748, -0.0362854, 0.000950813293, -0.033996582, -0.0183410645, 0.00194740295, 0.00312614441, -0.00855255127, -0.0265045166, 0.0320739746, 0.0271911621, 0.0195922852, 0.00448989868, -0.0270996094, 0.00389289856, 0.00247764587, 0.014289856, 0.0157623291, -0.0266571045, 0.0175628662, -0.0253753662, -0.0439758301, -0.0292663574, 0.0247650146, -0.00297164917, -0.00839996338, 0.00146484375, -0.017791748, -0.00347709656, 0.000721931458, -0.0157623291, 0.0230407715, -0.00918579102, 0.0123519897, -0.0392150879, 0.00486755371, 0.00721359253, 0.000448703766, 0.0181884766, -0.0263671875, -0.0280914307, 0.0210266113, -0.0213470459, -0.0140533447, -0.00318336487, -0.0405273438, 0.0131149292, -0.00510787964, 0.0261535645, 0.025894165, 0.0106582642, -0.0207519531, 0.0185089111, 0.0218048096, 0.0101318359, -0.00201034546, -0.0267028809, -0.0197601318, 0.0107574463, 0.0100097656, 0.0224609375, -0.0159759521, -0.0198059082, 0.0112228394, -0.017578125, 0.0152282715, 0.013671875, 0.010269165, 0.0269012451, -0.0172576904, -0.0221405029, 0.000283718109, 0.0126113892, 0.00904846191, -0.00881958, -0.042755127, 0.0336608887, 0.0118789673, 0.0102081299, 0.00564575195, -0.0180358887, -0.0282897949, -0.0289306641, -0.0541992188, -0.0280456543, -0.0165252686, -0.0292816162, -0.0139160156, 0.0685424805, -0.0437927246, -0.000859737396, 0.00254631042, 0.048034668, 0.00564575195, 0.0200195312, 0.00924682617, 0.00788879395, -0.0184021, -0.0389709473, -0.0540466309, 0.015838623, 0.000544548035, -0.00298118591, -0.0181121826, -0.00833129883, 0.0190124512, 0.0206756592, -0.016708374, 0.0327148438, -0.00459289551, 0.013130188, -0.0283203125, 0.0152511597, 0.000689983368, 0.028213501, 0.0214385986, -0.00499725342, 0.0286407471, -0.0377502441, -0.00596618652, -0.00946807861, -0.046081543, 0.00545120239, 0.0774536133, 0.0376281738, -0.000385046, -0.0681762695, 0.0319213867, 0.0206604, 0.0346679688, -0.0251464844, -0.0100479126, -0.0115356445, 0.0187225342, 0.00996398926, -0.0147247314, 0.00696563721, -0.0126724243, -0.0239257812, 0.0235595703, -0.0262451172, -0.0447387695, 0.0228118896, 0.0121688843, 0.0050163269, -0.00593185425, 0.0473938, 0.00713348389, -0.00978088379, 0.00320053101, -0.0176239014, 0.024887085, 0.013458252, -0.00839996338, 0.0314025879, -0.03125, -0.0233154297, -0.0233154297, -0.00393295288, 2.96235085e-05, 0.0328369141, -0.00570297241, 0.00829315186, -0.011756897, -0.0183410645, -0.0219116211, 0.0228881836, -0.0155563354, -0.0138626099, 0.0184021, 0.0274047852, 0.0156860352, -0.0117340088, 0.0197753906, 0.0329284668, 0.0177001953, 0.0138092041, -0.0254821777, -0.00901794434, -0.00325012207, -0.0397338867, 0.0213470459, -0.0229797363, 0.051574707, -0.00907897949, -0.0271759033, 0.023727417, -0.0165710449, -0.00120830536, 0.0205535889, 0.00754928589, -0.0096282959, -0.0106964111, 0.0199432373, 0.0076789856, -0.0330505371, -0.0450744629, -0.00762939453, -0.0228729248, 0.00685501099, 0.0172729492, 0.00202178955, -0.0343322754, -0.00405883789, -0.00373268127, 0.0321655273, 0.0253143311, -0.0503540039, -0.0171356201, 0.0476379395, 0.0146026611, -0.0100402832, 0.01953125, 0.0159912109, -0.0217132568, 0.0207214355, -0.00415420532, -0.0238494873, -0.0242004395, 0.00897216797, 0.0248413086, 0.00437927246, 0.0208892822, -0.0432739258, 0.0532531738, -0.0481262207, -0.00147438049, -0.0246734619, 0.0667724609, 0.00437927246, 0.0020236969, 0.0520629883, -0.0240936279, 0.0211334229, -0.0148468018, -0.00570297241, -0.0101165771, -0.0121917725, -0.0207977295, 0.0182800293, 0.0368042, 0.00503158569, 0.0229949951, -0.00748062134, 0.0396118164, -0.0203399658, 0.00871276855, -0.0160217285, 0.0274047852, -0.0298919678, -0.00331878662, -0.0173950195, 0.022064209, -0.0531311035, 0.0192108154, -0.0480651855, -0.0039711, 0.027557373, 0.01902771, 0.00540542603, 0.0142211914, -0.00199699402, -0.00700378418, 0.0102081299, -0.00329971313, -0.0452880859, -0.0118331909, -0.0518188477, 0.0178070068, 0.0422668457, -0.0109481812, -0.0386352539, -0.0296630859, 0.0160980225, 0.00817871094, 0.0490112305, -0.0125656128, 0.0550231934, -0.0243225098, -0.0162963867, -0.0233001709, 0.0395507812, -0.0312805176, -0.000916957855, 0.021697998, 0.0160827637, 0.00158977509, -0.0250244141, -0.0326538086, 0.0202331543, 0.00732040405, 0.0171051025, 0.0127639771, 0.0668945312, 0.0061340332, 0.00717544556, 0.0127563477, -0.00289344788, -0.0186309814, -0.0218658447, -0.0414428711, 0.00604248047, -0.0248413086, -0.00468063354, 0.00286483765, 0.026473999, 0.00839233398, 0.0207519531, -0.0273132324, -0.0200805664, 0.046875, 0.0196075439, 0.00563812256, -0.0298614502, -0.0323486328, -0.0395202637, 0.0284729, -0.041015625, 0.00212478638, -0.0582885742, -0.0380249023, -0.061920166, 0.0267028809, 0.0129776, -0.0401916504, 0.000949859619, 0.0257873535, 0.00846099854, 0.0218505859, 0.0089263916, 0.0308227539, -0.0123519897, -0.0143585205, -0.00510787964, -0.00470352173, 0.00290679932, -0.0314025879, 0.0468139648, -0.0415344238, 0.0325012207, -0.00391387939, -0.00248146057, 0.0272674561, 0.0124206543, -0.0207977295, -0.0234069824, 0.0252227783, -0.0431213379, -0.0623779297, -0.00232887268, 0.00993347168, 0.00563430786, -0.00542068481, 0.0218658447], metadata={'channel_id': 'UCfzlCWGWYyIQ0aLC5w48gBQ', 'original_id': '47a99c9d-16d1-4259-9c79-13e71d08c6cf-s0', 'published': '2022-08-12T15:18:07Z', 'title': '$5 MILLION AI for FREE', 'url': 'https://youtu.be/3EjtHs_lXnk', 'video_id': '3EjtHs_lXnk'}, sparse_values=None)}, usage={'read_units': 1})"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.fetch(ids=['47a99c9d-16d1-4259-9c79-13e71d08c6cf-s0'], namespace='youtube-transcriptions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UYTLp03EpQd"
      },
      "source": [
        "# Retrieving Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-tRwpIVdKl9"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "def retriever(user_query, top_k=3, namespace='youtube-transcriptions', embed_model=embedding_func):\n",
        "  \"\"\"\n",
        "  Retrieving the most similar documents to user query with sources from Pinecone\n",
        "  \"\"\"\n",
        "  user_vector = embed_model.embed_query(user_query)\n",
        "\n",
        "  retrieved_docs = index.query(\n",
        "      vector=user_vector,\n",
        "      top_k=top_k,\n",
        "      namespace=namespace,\n",
        "      include_metadata=True,\n",
        "      include_values=False\n",
        "  )\n",
        "\n",
        "  retrieved_texts, retrieved_sources, scores = [], [], []\n",
        "  pattern = re.compile('-s\\d+')\n",
        "\n",
        "  for retrieved_doc in retrieved_docs['matches']:\n",
        "    retrieved_id = retrieved_doc['metadata']['original_id']\n",
        "    retrieved_text = grouped_youtube[grouped_youtube['id'] == re.split(pattern, retrieved_id)[0]]['text'].values[0]\n",
        "    retrieved_source = (retrieved_doc['metadata']['title'], retrieved_doc['metadata']['url'])\n",
        "    scores.append(retrieved_doc['score'])\n",
        "    retrieved_texts.append(retrieved_text)\n",
        "    retrieved_sources.append(retrieved_source)\n",
        "\n",
        "  return retrieved_texts, retrieved_sources, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW42tILTbV_O",
        "outputId": "322fad58-9388-44e1-885c-00f615f5b752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source1:\n",
            "Machine Learning Holiday Live Stream: https://youtu.be/WvTU-y8HyDg\n",
            "Document1:\n",
            "you you you you you it How's that? Anyone hear me now? Yeah, it wasn't an error. It was just me being stupid. Because I made a new scene in OBS and I just didn't add an audio source. So that's that. There's not much more to it, honestly. Cool. So I have actually no idea what we're going to do today. I just thought maybe people are sitting somewhere at home and yeah. So cool. Yeah, I might not get all of chat. So I'm sorry for that. As we're coding at the same time, this might not be super possible. But okay, my sort of goal today was to get a little bit of a hugging face, tuning something going. So the idea is to do question answering training, maybe make that into a little video later. But I have no clue how this is going to go. Also got my trusty Weights and Biases cup here with warm tea. The stream is not sponsored. It is an awesome beaker. I genuinely don't like tea in these tiny cups that only hold like two or three deciliters. It just sucks. Okay. What was the first machine learning algorithm I implemented? I have no idea, honestly. Probably we had to implement like support vector machines by hand at some point. Yeah, that was it. Okay so wait, if I click on this, the other thing is going away. Okay, maybe I can pop out chat here. I'm trying to follow the chat as closely as I can. Forgive me for getting... Not tea, you need coffee. Oh yes, there's one other thing we need to do. There's one other thing and that is, one second, that is obviously we need to do a chroma key on the video right here. So yeah. Is this even... Not too bad. I can't see it in the previewer. I don't know if you see it or not, but OBS is screwing up, so I can't really see it. But this seems fine, right? You can see sort of through me. All right, so prediction, someday at the Alex Friedman podcast. What's the story behind the sunglasses? I just put them on randomly and then sort of that became my brand. No, I guess I'm not sure. I'm not sure that that was a wise choice, but it is what it is. So okay, the goal, let's say the goal, the current goal is train question answering. Let's do that. And I have not prepared this, so the previous was nicer. You mean this screen, but now you can see the screen behind me. So if something is here, you'll be able to see it. Yeah, but the settings might not be super good. One second. Well these are polarized, so I don't see exactly too much. Which route do you suggest? Passionate product management or ordinary ML engineer? Chat product manager and beta ML engineer. Well, do whatever you have fun doing. I feel people make too much fuzz about career choices and I don't know. It seems like you should just do whatever you want to do. And as long as you have fun doing that, there's really nothing that can go wrong. So just do that. That seems like the most logical thing. Okay, so what we're going to do is a little bit, let's say we're in a hogging phase and I have not trained a hogging phase model in a while and this API keeps changing and progressing. So my first question is, I know there's this pipeline thing. What about training an answer question instead? That's actually what we're going to do. So one of the techniques for getting better question answering is to produce questions from the answer, which I hope is what we might be able to do this now. So best way to get started with machine learning, I don't know, do some tutorials. I think just whatever you like. There's enough information out there, especially for beginners. Oh no, wait. Proper product placement. I need to hold it like this into the camera. So refreshing. So I know there's this pipeline thing, right? The question is, can I fine tune a pipeline? Can I fine tune a pipeline? Or essentially, can I train a pipeline? What or is this only for inference? So I don't know. Here under fine tuning a pre trained model. Can you train hogging phase model with CPU? Sure. But do you have Omicron? I have no idea. You can fine tune the model, right? With this trainer, however, the model would be sort of, you know, like a model like this. So where do they get the model from? The model is like a auto model for sequence classification. It is not a pipeline. And the good thing about the pipeline, obviously, is that it makes, you know, things like loading data sets and and stuff like this pretty, pretty easy. My question is, where does the tokenizer come in right here? So you see pre process your data, input function, tokenized data sets. So you need to set up, you need to set up the tokenizer at the data sets, and then input the model as this model right here. But it seems like fine tuning a pipeline might not be a thing. What hardware do you plan on using? I'll just do a Colab for now. I guess that's my that's gonna be my thing or even fully, fully locally. Yeah. All right. Test trainer evaluation strategy. Fine tuning in native PyTorch. I don't want to do that. I just want to do it with the trainer with the trainer API. That seems fine. All right. Let me quickly pull out a Colab or open a Colab. And I'm sorry, sometimes I'm going to pull stuff to the side right here because I have no clue what's going to pop up. All right, we have one with the absolute awesome name of Untitled2. And we're going to change that to hogging face question answering. So how much GPU size do you have? Only none. What do you mean by fine tuning a pipeline? You can fine tune a model only. Yeah, exactly. You can fine tune a model, but there's this pipeline abstraction that sort of so if I maybe I can I can show you so let's place this in here again, right there is this pipeline. And it takes care of a lot of stuff. So if you did question answering, right, you can just load a model and the tokenizer. And what it would do, what it would do is essentially it would it would handle all of the sort of plumbing around question answering. But we don't we're just going to do it by ourselves. I think that's pipeline batching. Yeah. Pipeline custom code. Yeah. Implementing a new pipeline. So then we have just to use models for inference. OK, now the question has been answered. All right. Let's see whether we can answering fine tune example. Because we're not gonna because we're not gonna do anything ourselves. We're just gonna use stuff. OK. OK. OK. I mean, there must be there you go learn how to fine tune a model for question answering squad. So this is going to be our first thing. We're going to fine tune model for question answering. Then we're going to try to use when I try to use the model that we've we've trained as a baseline and we're going to try to train another model that will generate questions given some answers. And then we're going to augment our data set using those pairs. And then hopefully that will lead to a better data set. I've seen papers that do this and in question answering, especially in natural questions. And this has been really has been sort of improved, but or has been improving state of the art. But who knows? I'm a student in need of compute for a project. Any advice for compute and in general? Not really. I mean, there's there's Colab. There's there's quite cheap cloud compute. And if you are a student, your university might have your university might have some resources or sometimes the big companies, they give sort of credits to university students. So I don't know, it might it might take a bit of politics, but you can usually get stuff. All right. Let's see. I've as I said, I haven't done this in a long time. So yeah, so we go out. We have a tokenizer. We need the distilbert. Okay. But at the time, but at the time, but at the time. And to now we don't want TensorFlow. Okay, this seems okay enough. Sequence classification, token classification question answering. Pytorch notebook, but there is already one. Why do we open? Why do we open a new one? To be able to share your model with the community. Now we don't want that. All right. So let's start. Yeah. Okay, no need to log in. But what we want to do is we also want to install one DB. Because how do we do that exactly? Let's check. Yep. Got it. And code. And this is just logging. Okay. I thought we had it. Yeah. All right. Thank you. What's wrong? Ah, very smart, very smart. Should have made it quiet. Anyway, okay. So we need to install git LFS. So what do we go here? We're going to for squad v1 or v2. Going to use distal-berth batch size of 16. And use the datasets library, which is pretty good. Then, okay, we're going to show some random elements. Just fine tune evaluation. Seems easy enough. We'll do that and we'll figure out in the meantime what we need to do in order to go further. Yep, yep, yep. Uncomment the first slide. Yes. I was absolutely able. Okay, appending. Okay, good. So we're logged in. Nice. Let me open. Let me open. Let me open the 1DB. We'll figure it out once we log something, right? I guess we can just run all now. All right, this looks good so far. We got probably the same random examples. And so here you see how this dataset is built. This is made from Wikipedia articles. So the dataset creators, they always sort of grabbed a paragraph from Wikipedia. I think it's always kind of the first one. Not sure. So I don't know, carrot. I think it's always this one, but I'm not entirely sure. It might be any one, but I think I'm also not sure of the exact differences between squad V1 and V2. However, that's kind of... Do you have your own GPU workstation or university? So our university had a cluster. They built it up during my PhD, which is fairly big. But our group also had sort of our own GPUs. Yep. Where were we? So yeah, this is sort of a paragraph. This is, I guess, the ID or the title of the article in Wikipedia. And then there's a question like what was proclaimed the state religion under Theodosius I? And then there is an answer. And the answer is somewhere in the text. So it's extractive question answering. And you can see here Christianity or sorry, Nessene Christianity is the correct answer. Now the answer might actually be multiple times, mentioned multiple times in the paragraph. And it is important that you not only get the answer correct, but you get the correct index. So you also need to decide which, if it's mentioned multiple times, which of the mentions is the correct one. And that would be sort of the one that directly answers the question. So it's pretty strict. So this is closed domain extractive question answering. It has sort of a reputation of being a lot about kind of keyword finding and sort of comparing the model doesn't need to understand that much about. The question is out of frame. What? What? Oh, why does this keep happening? Thank you. No, wait. Huh? Huh? OK. I'm out of frame, too. Look at that. All right. How do I fix this? I'm out of frame. OK, so. Hmm. OK. So. OK. So. OK. So. OK. So. OK. So. OK. So. OK. So. Isn't YouTube 16 by nine because I'm streaming 16 by nine. If you see like here and OBS. Right. You can see. Oh, no. OK. It's the side of it's the side of the screen here. OK. okay we might be able to fix this there's probably a crop somewhere going on you see there's nothing here OBS is screwing this up. Crop right there's no crop right none so I don't know what's what's going on alright we'll just have to live with it I think this is OBS not not handling the screen resolutions well in any case oh yeah you can see me totally fine I was confused about that alright sorry so the answers if you're able to see them I'll try to make it a little bit more inside the answer right here is a string and also an index into the text 169 and I'm gonna guess that points to sort of this piece right here alright so this is the data set and pre-processing the training data consists of what consists of instantiating the tokenizer then you can see that we tokenize on two sentences one for the answer one for the context okay and depending on your model we'll see different keys yeah okay so what's this what is your no this is beginning of sentence right 102 what's 102 what is your name is maybe this run right here then it's sort of the end of sentence and then it's another my name is Sylvan and then it's another end of sentence that's this tokenization the attention mask is there okay so we can feed things to this tokenizer with a max length then what we do is we go through the data set let's find a long example this is just exploring if we just truncate we'll lose information so we need to go sort of in a stride over the data set but the tokenizer is doing this for us now we have a list of input IDs okay so we split up the context if it's too long into these different things into these different chunks and we iterate over the data set you can see that the question is always the same but for the context for the context will truncate the second or will stride over the second thing so here is the men's basketball team has over yada yada yada yada yada and if you're at the end here well that is a long context you see that 32 wins were that is the end and that is not the beginning here the beginning is championship so if I am going to guess if you go on here you're gonna see the 32 wins somewhere in yeah you see here the 32 wins were the most so this kind of striding over this document any new machine learning street talk episode coming yes yes there's one coming we're working on it when do you release a Udemy course? I don't have the stamina to create a course and I think other people are are better at that but who knows I don't know not yet not yet okay this will give us we need to find which of those features the answer is where exactly the models will you require to start and end position to map parts of the original context to some tokens thankfully the tokenizer can help us by returning an offset mapping this gives for each index of our input IDs the corresponding start and end character in the original text that gave our token that's pretty useful is this how new is this because I remember having to do this myself which was awful or I just didn't know about this the very first token has 0,0 because it doesn't correspond to any part of the question answer the second token is the same as the characters 0 to 3 of the question the how here that is one token and so that goes from 0 to 3 and then the next token goes from 4 to 8 4 would be many to 8 you can see that the white space is sort of falling away so not every character is covered by an ID this is pretty useful yep oh that's what they do right here so first token ID, input IDs, okay we get this, offsets, we get this then we get the token and we also get it from the question you can see that it's almost the same but it has been lower case yeah to anyone who's new will train question answering but will also just I've not prepared also chatting let's do that up mmm okay yeah I'll kick the bot or hit it cool alright so so we can use this mapping to find a position of start and end tokens have to distinguish which part of the offset correspond to the question which correspond to the context sequence IDs okay the sequence ID says 0 and 1 for the question and the context respectively it returns none for the special tokens okay so if we now have an answer say we have an answer and the answer start which is the index right into into the which is the index into the text the end character we can figure out by simply taking the length of this answer and adding it to the start and what we're gonna do now start token index of the current span in the text so we're simply going to loop over the entire thing until we reach the second thing so here is the question and here is the context so we're going to loop until we hit one which is the first token and then we're going to also to find the end index of that we're going to the end here and we're going from the end we're subtracting negative one until we reach okay that's that's cool then we're going to um detect if the answer is out of the span in which case this feature is labeled with the CLS index there's a lot of like plumbing to do right if the offsets of the start token in of the token start index so now we're going to look at these offsets so where in the original text is a given token if that's outside of the um starting character which we have right here so if that is smaller okay so if the answer is somewhere after the start index so the answer is somewhere after this one and somewhere before the last one right over here then we have the answer in this context otherwise we don't and this code here is to figure out where exactly the answer is in that we can double check that it's indeed the theoretical answer so what we're going to do is we're going to look at the answer that's given from the data set and then we're also going to tell the tokenizer to decode from the start position to the end position in the input IDs and we got it cool okay so we also tell it to pad right let's put everything together in one function we will apply to our training set in the case of impossible answer the answer another feature being another context we set the CLS index for both the start and the end position we could also simply discard those examples from the training set if the flag allow impossible answers is false okay alright so this is prepare train features we're going to we'll explain how to handle long texts for train and inference yeah that's what they do right here right they simply set this stride right here and so they stride over the context that's what they do in the tokenizer so the tokenizer is going to emit always the question and then a part of the document and this part is strided over the document and then for each one you're trying to figure out where the answer is in the context and if the answer is inside you try to figure out where if the answer is outside you you'd simply say this the answer is nowhere in here which you dummy encode as saying the CLS token is the correct answer so I'm which gives you also an indication of whether or not the question is unanswerable which is going to be important in later data sets for example natural questions has questions that are sort of unanswerable and having sort of a this this gives you like an immediate classifier so that's how I guess how you do it what's the best way to de-tokenize text I don't know give it to the tokenizer and say say decode right here okay so here we put everything together I'm okay this pad on right yeah max length stride return overflowing tokens true offset mapping is true are this is what you need to put to get these mappings padding with pad to the max length okay and now for each now we look offset mapping here we get a dictionary and we're going to get the offset mappings out of it I'm going to feel other in entries here calling start positions and end positions okay I guess we can skip over that but at the end we're going to end up with sort of a a dictionary that I guess we can look at it right here no okay this is a we can't we can't this is not a global variable alright but in any case we have up this one here features features is a dictionary no batch encoding but they should be the same ish as a dictionary does this work yeah exactly so we have input IDs which is or the samples I'm what T do you have weights and biases T obviously it's a some kind of berry T you can activate vim on colab mmm let me figure that out and I've knew I've known this but I somehow don't like I don't like I'm not too much too much of a fan of in bindings in other things I'll try where you tools settings there we go yes that done arm okay good we have we have been bindings alright not too big of a fan because I I don't have all my key bindings like my been mercy thought the same alright and finally finally will have a tokenized data set by applying the map function to data sets alright and then we instantiate the model from the checkpoint where's this where does this come from see now search for it and I don't know I don't know where it is right so I need to search like this and where is it it's on top here it's just not not too pleasant of an experience in bindings but continue so will load up the model like this from the checkpoint right then arm will have some training arguments right here yada yada yada push to hub no arm yeah exactly so okay we've instantiated our trainer and now we can just say train and if this is working out all we should be able to see this in our 1DB um dashboard there we go wait break out the tab here do we have an accelerator even um runtime type we do not alright let's try again runtime restart and run all so in the meantime we can think of what do we do in order to actually uh do what we came here to do namely um wait is this on a custom data set no it's on squad squad is a question answering data set so what we want to do is we want to actually take the squad data set and generate more questions from it so what we need to do is need to train a model to take in a um a context and an an answer and give us the question right this is is different so it's a bit like um we actually need to do generation of text and not just question answering and we can take that data set and feed it back into the question answering so how do we do that um we need some sort of a text generation model so we go onto the hub let's say here we have examples fine-tuning with custom data sets um sequence what's this sequence classification token classification question answering it's all along the along the same lines but what we need to do is a text generation right so um train and what do we want maybe a t5 or so or or maybe a gpt2 some some model like this we could we could generate so maybe we'll start real small with like a distil gpt or something and we'll fine tune it okay so let's take that can we fine tune the distil gpt probably or do we have to fine tune gpt and then um that's we already have this how to fine tune a model on yeah okay translation summarization speech recognition audio classification language modeling which one I guess the translation might be most appropriate to us no since since um what we want to do is we have like an input like it's a sequence to sequence task and can we already see anything no we're training we don't see anything yet let's see there we go where are my charts we only have that yet we didn't actually log anything yet okay but okay we have a GPU now that's good yeah see there's no no epoch yet I mean this is gonna take a while I have a feeling it's gonna take a while right it's just five hours look at that okay in the meantime we need to we need to figure out what to do so where do we need to hook in um where do we need to hook in why not question answering we're doing question answering we're just doing it a little bit weird where do we hook in in order to find so I feel the I feel the data set here is just fine right we get input ids which contains this contains the this contains the but do we really need to point if we give it if we give it start and end positions those are the labels input ids the point with these transformers is you can always you can give them right you can you can give them a part of the input as labels like what we had as the sequence ids before these are zeros and ones and usually you do zeros for like the for the first sentence and ones for the second sentence or here you do zeros for the question and ones for the context can we somehow abuse this to mark the answer probably not maybe what we need to do is we need to give it the context the answer and then produce the question but in an order aggressive way I'm late can someone shortly explain what we're trying to achieve so we're just we've just analyzed this notebook here that fine tunes question answering and we're seeing now that's just going to take way too long but the goal is to take this and actually produce questions new questions so we're going to train a date this a model that takes in the context then the answer and then gives us a question so that is that is the goal right here and for that we are looking at fine tuning fine tuning a translation model fine tuning a pre trained model common downstream tasks here we're going to fine tune a translation model but the tokenizer this is very very much into this is another sentence can we fine tune a T5 is that too big of a model T5 see people have done this before huh are they used they used this is an old version classification sentiment span extraction mmm summarization OK so we could potentially use this right here is full stack advantage OK attractive summer yep so this and that we got a GPU what are they doing here custom data set so we're outputting said he's target IDs does this still run I wonder let's just sort of let's just sort of stop this right here interrupt and let's give this a try too many sessions yeah just OK good stuff well this is this is training by hand so essentially all we need to do is we need to get the squad data set and we need to replace the summary we need to explain it replace the summary source with the context and answer and then the summary target with the question this should be OK ish I have done this already no sorry I need to log in briefly into one DB just putting someone something else in my control C in my clipboard OK so we got an error this might be an old an old thing sentence piece library we can do that yep done so now we just splice the two together first loader train OK this goes from a data frame probably easier if we do if we did the so we need these source mask and here we need source mask as well target IDs source IDs and source mask you also need a tokenizer so I feel we can copy over the data set from the last one OK good so now we we don't have the data set which is fine T5 for conditional where does this come from have you tried comment I have not tried comment I'm sorry T5 for conditional generation that's what we need with the language modeling head on top yes so input IDs position embeddings from a T5 tokenizer what about target returns a loss labels are there we go no this is sequence classification how can we compute the loss if we have batch size sequence length how do we compute a loss exactly if we don't have targets like your label so you just give OK I guess we can do that all right so here here's what we do we'll do a new collab as we had it here we'll just make our own and we'll click click click together whatever whatever we need so this is probably going to yell at us no OK do we have the GPU change runtime type GPU yes it's going to yell at us we're going to terminate the others and close I've just started a job and your channel is a common source of inspiration Thank you. I hope I'm not destroying that today because live streams are quite a bit more more boring but we'll try our best yes OK well so we get the data set and then we get this will just try that we'll try the T5 thing that they have here so this is for inference this is for yeah we can do this this is fine the Vim binding still here no they're not tools settings editor Vim so OK we still need sentence piece excellent favorite beer I don't I don't have a I don't drink beer so no idea sorry OK tokenizer we wait we can't get pretrained token this is impossible what's our tokenizer none our tokenizer is none how let's grab this put it down here and see my bindings don't work this is annoying how can this how can it be all right help me here why is this why is it none wise tokenizer none have you experienced imposter syndrome like long periods without published papers how did you go through yeah I think yeah this is quite unless you're like super stellar at academia this is very very common yeah it's it's like it it seems like everyone else knows more than you and you can't like you just can't let it get you down and also a lot of research fails but I mean I think for most people the answer is just to prevail there it might be a little bit tricky if you have like if you really have a wrong topic that where it's really hard to publish stuff yeah but oh OK oh I'm just seeing that no not again sorry need need to get quick power yeah see this previously happened on live stream where I was streaming and my battery just kept decreasing in power and now I even have the big charger here but when I did the Minecraft stream that was the case so even though it was plugged in it would just slowly decrease in in in battery over time so I might have to stop it might just break at some point and then I'm really sorry but yeah we'll see how long this lasts OK in any case did we figure out did we figure out why the tokenizer here is none maybe we have a wrong one so up to five small all right I mean this should be should be OK no maybe we can get an auto tokenizer all right instead of a an auto tokenizer instead of using the T5 class seems reasonable enough so like this let's take this put it here up that's better OK all right see none of this Vim stuff is working if I cut cut this and OK I can paste it but like I have I don't have my my zero buffer aren't you afraid that your search history will pop up when you type the search bar yes yes I'm afraid well I'm afraid I mean it will pop up but I'm not I'm not that interesting of a I don't think my searches are interesting enough to warrant big worry yeah I I'm rather yeah what are we working on we're trying to make a model that that gives us questions to answers which so let me update the text right here current goal just chatting also training model that OK all right so now let's look at our loss our loss should be a number prediction scores are something 1732 that's probably the vocabulary that's probably the length I guess because the input IDs what's the input IDs should be a tensor of length seven all right so and then we can use the second line right here in order to conditional generation from pre trained that's the same line that's the same line that's the same line and there we can generate stuff from it and the outputs here are so let's give it to the code done done done done done done done done list canopy interpreted as an integer how about that yep so there you go that is hello my dog is cute hello my dog is cute no shouldn't we sort of auto aggressively and shouldn't we be able to generate text from this I have no idea how this how this works generate inputs max length min length do sample really stopping let's let's put 10 here so cute and cute use batch decode yes true absolutely very cool all right so we can force it to to create something now we need a data set and will copy this from here will get the squad data set uh huh data sets we have data sets over here we have data sets cool we'll get the squad data set load data set squad let's just go with squad data sets is not defined right good let's just remind ourselves and mostly me how this works okay yeah so this is all this is all good I used to we used to all do the all of this by hand right like load the data and process pre process stuff and yada yada yada so, and this has been, it's gotten so much easier recently, it is, it is crazy. So we'll get the training data set, and this is an index of all right this is not just an iterable yeah. So, it's technically what we can do is, how does this map work right here. map. Right, we can we can do this. This map function. No. Yep. Do this map function. And what it does is you can give it a like takes a callable accepting a dict as an argument and iterates over the data set by calling the function with each example. So that's what we do. We'll map it. Or how do we, how do we then train. How do we train. Trainer dot train so we got the model, we have a data set, and how does the data set need to be structured for training, that's my, my only other question. So, like how do we how do we know what the, the loss is. IMDB. Tokenize function. So we tokenize it. But then what's the. How do we tell the model. What the label is. We just give it the data set, but is that just like inherent in this data set. Because here you see map. So. We return, we map the tokenize, tokenize the data sets. Small train data set. Right, this is what I don't get. So we simply extract the text. We tokenize it, which is cool, which is fine. Right. But then we only have the text. How does it know what the labels are. Labels. Now this is for evaluation. Report metrics. Report metrics. This must be inherent to the data set somehow. Auto return by tokenizer. OK. I see. Cool. So if, if we transform this right now. So let's make a function. Dev process data because why not because. I feel like implementing a Turing machine. Every function could be called process data. So this is going to be an example right here. We. We pass and at the end we'll just. Pass in the zero if data point right here. All right. So. If we do this, we'll just get our normal. Our normal thing. Now what we need to do is we need to get the context. The answer and the question. And for the context and the answer. Let's just concatenate them. Yeah. We'll do it. We'll do the cheapest. We'll do like the GPT three cheap trick right here. So. Process data of data. Yeah. That's what we're going to do. OK. So we'll do the input. Text is. Let's put the. Context right here. And. We'll do. We'll do GPT three style. We'll do. Context colon. Of this. And. We'll do. Answer colon. I know T5 does things differently a little bit. But we don't care. Example. Answers. We have multiple and we could have multiple answers, but we'll just take whatever the first one is. So. The answer. Text. Zero. Like that. So this is going to be our input. And then. This one right here is the prompt. And output text is going to be simply the question. All right. Now we need the tokenizer. The tokenizer. We have it somewhere up. Right. Good. Oops. Oops. Oops. See what did I do? Encode. The input text. So these are going to be the input IDs. Do we need a mask? We can give it two things. Can we also give the T5 thing two things? Maybe not. Tokenizer. Probably not. It's going to be fine. Right. This right here. Encode. Return tensors. PT will do that. Let's just do this once. OK. So the input IDs is going to be. Good. Could we put a second one here? Like what happens if we do this? Then. Are these two lists now? Let's see. This is the tensor bracket. Anyone see whether these are two lists? Probably not. Right. So if we do this, the last one is a 10 1. And then if we do this. The last one is not. Here is 10 1. And then there is 304 something something. OK. I mean, we could just do language modeling as such. Right. And we could then even let it figure out the answers. But that's kind of crappy because we would also train it to do the context. T5 is encoder. Yeah. T5 is encoder decoder. You must specify the decoder input plus pad plus plus label. Yeah. So the query, the queue here. Would probably be the part of the decoder. Like here, we're going from a T5. This is Pandas. This is validation down here. We have target IDs and source IDs for this one. Right. So input IDs. But we don't have... If input IDs, decoder input IDs is this. And LM labels is what language model labels. So here we have input IDs. The attention mask. Encoder outputs. See our decoder input IDs. That's, I guess, also what we want for the decoder. Provide for sequence to sequence training. Uses the pad token ID as the starting token for decoder input IDs generation. If... Take a look at T5 training. There we go. We could use those extra IDs, right? Unsupervised denoising training. Supervised training. Translate English to German. The house is wonderful. Labels. Return tensors. Easy enough. Easy enough. Thank you for letting me know. So we put our end of sentence here. Then we put... Let's do this. Yep. And our end of sentence here. Input IDs, labels. Output text. PyTorch tensors. Let's remove this one. And we should be good to go. So input IDs and labels. Excellent. Excellent. Where was it? Where was T5 training? Too many tabs open. Supervised training. OK, if we put this into the model now, do we get a loss? That's the question. If we get a loss, everything's fine. Everything's good. So where is our model? Model. Let's call this X because why not? My coding voice is very different. Yeah, because I can't keep up hours of shouting. So what if we put X into the model? That is not cool. OK. What if we put... ...if we unpack X into the model? That is better. Better. Better. Better. What is this? OK, let's call this Y. And let's look at type of Y. Investigate. Sequence to sequence language model output. Thank you. That is very helpful. What can I... Is it a dict? Yes, it is. OK, so we have a loss. We have a loss. I guess that is... ...why do you always wear shades? So people ask the question. Is branding. Is just branding. OK, so we have this. Very cool. Now... So if we do this, we can even see it. Now we can use our data set map. Process data. What do they do here? Map. I guess we can use... ...fatched and we can remove all the columns. No? We can probably remove all the columns. Module object is not subscript. Remove columns. Data sets. Ah, this is it. Mm-hmm. Mm-hmm. It's not happy. It's not very happy. List indices must be integers or slices. OK, we should do batched false. All right. I guess we should also drop... We should probably also drop some of the too long stuff. No? I mean, that's what they do here. They put a max length. Return overflowing tokens. Ah, we don't want that. We simply want the max length, right? So, no. Stop it. Stop it. OK, so tokenizer. See, now is the trouble, right? The max length is what? The max length is what for t5 small? Does anyone know? Let's see, t5. Or can I figure this out somehow? There should be this auto... There's this auto config, no? By hugging face? Can I load this somehow? Somehow. Here, like... There is... auto config from pre-trained t5 small. No? This should exist. Yeah, see? So here we have task-specific parameters. Model type, number of positions, number of layers, task-specific parameters, vocab size, translation, summarization, max length. Is this, though... Is this the max length of the encoder or the decoder? I mean, here it's 512. We can do the following. We can do... We can do max length here to something... I mean, what is... What is 512? Let's say minus 128. OK, and then... And then here we can do 128. Padding equals... How about that? So we pad everything max length, min length. 230. Yeah, thanks. Thank you. So... We'll see. We'll make everything such that it is at max 512 long. We can still change these things. We should probably make some variables somewhere, but... Yeah. So, good. Now that the dataset is mapping, what can we do? I guess all we have to do is give it to the trainer. No? Hugging face trainer. Mm-hmm. Compute loss, yeah, and then... Do we have examples right here? Sec2SecTrainer. Can we just get a trainer or do we need the specific one? Evaluate? No. Predict? No. Training arguments. Ooh, that is a... Model. We put in model. Yep. Train dataset, evaluate dataset. Yeah, this should be doable, no? Here, trainer, we just give it... This, and we're good. 384. Really? Hmm. We could make it even smaller, given that it's going to take a while. Also... We could just do this. Let's just cancel for now. How long does it take? What? But they did it. They did it up here. They did it here, no? They made this smaller dataset. Um... What do I need to do? No? Here. Here is a dataset. Okay, I guess they just took the training one. Like this. Okay. Excellent. So now we have a dataset and we should just be able to give this to a trainer. Yes, yes. Okay, so let's get some of them. Training arguments. Good. And let's also get some of them. Trainer itself. Good. Yeah, okay. That's where the 384 comes from. Thank you. Okay, model is model. This is that. And I guess... Okay, so... And... This and that. Good. So here we have... Train dataset and eval dataset. This seems to work so far. This is more like magic. How... Where is all of this... Like... Okay. There seems to be so much magic going on and this is... Okay, I see. Finally, finally we get something. Something that crashes. So... What did we do? Input shape. Too many values to unpack in the input shape. So where are we going wrong? Too big context? Nah. We have too many... We have too many things. So here we might be... We might want to inspect this X a little more. So these are our input IDs, right? So... Input IDs are right here. And if we do the shape of them, we get two numbers. Now this is complaining that if it looks at the input shape, right? So if we go to T5 modeling... Wow! This opens in a Colab directly. This is crazy. Okay, if we go to the input shape... This is the input embeddings. No, that's not true. See the input IDs... Oopsie. Dot size. Well, what is it? Aren't you going to tell me? Too many values to unpack? Well, what is it? So... Maybe we need to batch this ourselves somehow, right? Because... Here, as you can see, the shape... Where was it? Is 1 and 384. So this already assumes that it's kind of batched. And... I might be completely wrong right here. Unpack is about the input shape being only one object. Try printing the shape. Yeah. I can print. Can I print in here? But it seems the problem is the batching, no? So... Does this work? This seems weird. Like if this works, this is... This is magic. I've not developed in a Colab in so long. No, right? This does not work. So if... All changes saved, yeah? Ah, okay. Now it says too many values to... Yeah, it did not load, reload this thing. Can I... Sorry. Click the six frames. Yep. Well... It just tells me where it's happening. Which I know, right? I just... Compute loss. Models. Unpack the inputs. That's fine. I think it's the... The collate function is wrong. It assumes that what comes out of the data set is a single example. So it probably batches it together. Need to use auto reload too. Thank you. Let's try. Auto reload. Magic function not found. Too bad. It probably assumes that. So what we could do is we could... Simply tell it that this is not a batched thing. Right. I found at least two devices. Great. Now we're... Okay. Process data. Ah... How about that? No attribute detach. Yeah. Okay. Sounds good. We'll try again. Too many values to unpack. Still no. Yeah, it still thinks it's there. But it doesn't matter. We have bigger problems. We have bigger problems. Look at this thing. Now where was it? Here. Right. So we need to... Do we really need to get back... Which ones are on? You need to run auto reload to afterload. Okay. Okay. Okay. Okay. Okay. So see, I'm so unfamiliar with Colabs because I've just been using my Vim in my terminal. That I have no clue about the magic functions. Okay, so... There. Yeah, there you go. Exactly. So this is what I meant. It batches it to eight. It assumes that what comes out of the data set is already a batch. And therefore, it doesn't get it. So either we somehow tell the tokenizer or something that... Or we tell the collate function. Collater default. We'll batch our processed examples together. Let's see. There's the default one. Label IDs handles a list of values per object. Maybe this one, data collater for SICK to tokenizer. Dynamically pad the input as well as the labels. Maybe this one's good for us. So let's try this one. Yes, I'll make a video about my Vim config. Okay, all tensors need to be on the same device. Found at least two devices. How can I... Where is my data set? So how about... I look at... What's X? What is X.inputIDs? What's the device of that? CPU. Okay. So how do I put that on? And X is what? X is just... X is a data point. Now... Can I just tell HoggingFace to... Or do I have to actually say... Did the model have a device? Okay. So now what we'll have to do is we'll have to input IDs equals X input IDs to model device and labels equals X labels to model device. Yeah, okay. So now we got sort of the same... We got the same problem here. So we don't want to necessarily do this. We want to output this because after a while we could also overflow and then output several... Could we put several different ones? No, I guess we can continue doing this. And then here we'll just have to... Do something like that or expand. We'll just go none. Do a little unsqueeze. Nice. Okay. So now we still... Oh no. Now we have... We need to remove the print statement again, otherwise we'll just be bombarded by print statements, right? Okay, let's do this. Let's do that. Please save. Thank you. Okay. Good. Problem solved. Happy holiday. Thank you. Happy holiday to you too. Good. We're at the next error. Expected a sequence length of 384 at dimension 1. Got 517. What? What? We even limited it to... We limited this. Got 512 data collater. We're here. Label. Do we have label or label IDs? We should have label IDs, no? Because... Yeah. We should have label IDs. That seems reasonable. Still the same. Okay. So the problem is that here... What's f in features? What's features? What is it? List of dict f of k. So what's k? If k is not in label and v is not none... And here we say expected a sequence of length. Got... Okay. We can do this and we can print the length of that and let's see what happens. 384, 384, 384. And then we get like a long one. Why is it not respecting the max length? So this one... Let's see. Why is it not respecting the max length? Hmm... Yes, max length. Pad to max length. Max length. I'm just searching for this now. In all the things I have open. Yeah, I should be able to truncation true. Is that a thing? Okay. Let's see. Auto models. Tokenizer. Pre-trained tokenizer. Can I click on it? Please. Yes. No. This is version 2. Sure. There we go. Max length. It's called model max length, really. Truncation. Defaults to false. Well, that explains something. Let's in this case do true. How about now? 384, 384, 384, 384. This looks better. Okay. Now we just need to remove the print statements. Otherwise the file will be gone. Cool. Okay, so this isn't too long. This isn't five hours like before. So hopefully, hopefully something cool will happen. Truncation. You throw away everything after the first 384 tokens. Is that on purpose? Yes. We can still refine like it. Keep these tokens that it's handled in the... Yeah, the tokenizer can sort of split and, and... Tokenizer can stride over the document. But in first instance, I'm just happy with getting any sort of result. Can still be refined later, but we just want to train anything at all. Well, this is the fun part. So I think people sometimes ask, you know, about streaming and machine learning. And, you know, streaming video games is sort of the most like super entertaining, right? Because it's like stuff is happening. And then streaming coding is still okay if you can talk and code and read chat and so on at the same time. But then streaming machine learning is just sort of... Even if we make the data sets deliberately small like we did here, it is just... It is, it is... We just have to wait. And then what do we do during waiting? Could you show line data set equals? Data set equals... Well, we have data set is simply loading it here from... We simply load the squat data set. So that's kind of trivial. But then we have this mapping function here that tokenizes the text and puts it into sort of this framework context and answer. And we want to output the question. What do you do during wait normally? I watch... I love watching loading bars and screens. But usually I... I don't know. I code something else. I guess there is something else to code, which is... I'm making like a super... Sure, doing an AMA. Am I not doing an AMA during all of the live stream Swordfight? Yes, the XKCD meme. So I see if I have enough GPU left. That's a tight rope, no? Try to stream some reinforcement learning. Watch an agent not really working. Yeah, if you get the visualization set up correctly, that could be something. Also like smaller problems, right? Like way smaller problems is also something. This here I sort of wanted to try. And oh, we can look at our... We can look at the... This here, it should be... Yeah, yeah. Look at this. At least the CPU utilization is going on. That is something that's happening. We don't have metrics yet. We might want to log more intermittently. Training completed. Training completed. Excellent. We have zero logging. We did log, right? It said it logs to... It logs to report to... Didn't it say it would log to weights and biases? Did we even log in? WondB. We didn't log in. We didn't log in. Oh, we did. Somewhere. At some point, maybe. Am I planning to do the ML year recap video? Did I announce an ML year recap? I don't know. Alpha fold happened or something. Was it this year or last year? I have no idea. The paper of the year. No idea. It's really hard to predict what paper is going to have a big impact. For example, the foundation models paper. It can either have a really big impact or it can just be forgotten by everyone. There's no way to tell right now. What do you think about the recent hype in graph neural nets? I've not looked super much into graph neural nets. It seems they're pretty cool, but they also seem to be quite limited right now because they're essentially RNNs on a dynamic graph. Check the report. Yeah, so WondB has a trainer. So there should be it should be possible. Here, report to WondB. That's fine. With the training arguments. Good stuff. We might want to restart and run all. OK, now once the model has trained, we can then let it generate some stuff. I really don't think that it will produce something interesting right now. OK, good stuff. Reusing the data set. That's what we want. OK, here. Yes. This one. OK, as this is going to come in, we can already think about what we want to do. So what we want to do is we want to simply take the model that we have and let it generate something again. So let's just try. Let's just try this again. I think. No. No, like. Try this. And where was the generate? Here. And then we do batch decode. There we go. Aren't graph neural networks just a year behind NLP or attention? I'm not sure what a year behind. You mean like in a year they're going to be as popular? It could be. I've also seen like there's also a big history of such things just flopping like there being a lot of hype and then not much. It depends. It depends if sort of large breakthroughs, I guess, are being made in relevant domains. That's usually what fuels these things like transformers. I think BERT has transformers have existed. And I remember I remember people sort of the sentiment was, yeah, transformers for language, they kind of work, but they use so much memory and they're harder to train and so on. And people would still sort of be on the be on the fence and say we'd rather use LSTMs with attention. And I think only the advent of BERT really say it wasn't even the attention is all you need paper. It was the BERT paper that really fueled the transformer hype. So if there is something similar for graph neural networks in a relevant domain, so either in a domain that exists like but I don't I don't really see graph neural networks being that useful in vision or or NLP or something. So I'm not entirely sure. They are probably super useful in biology. But it's not like biology is that it's just not that accessible to most people. And they don't want to work in biology because if I'm good at like if I do something in NLP, like I can build a little app and so on. It's very tangible and very useful for most people. And same with vision. But with something like biological data, I mean, yeah, sure, I can download some bio data from from online. But then, you know what? What am I going to do with it? Are you familiar with PINNS? No, I'm not actually. Socks, 1 dB socks giveaway. Well, if you if I'm I don't know, I should wear them probably. Yeah. Sorry, I'm trying to keep up with chat. Most interesting meta learning paper. I'm not I'm not super I don't have a maybe not anyone, but I don't have a good overview over meta learning. According to a Luther graph, neural networks are largely NLP research from 2019 and 2020. If you want to achieve Soda, you'd have to use 2020. So you're saying that current NLP methods beat the graph neural network methods. So when people in the past said, hey, these graph neural networks are actually pretty good. Then all you had to do essentially is wait a year and then the field would have sort of caught up. I guess I hope that's what you're saying. OK, so now we're sort of kind of a bit ready. No. Yes. So here, let's say. Yes, we can generate a context. Hi. The dog was loud and. The answer is dog. Right. And then end of sentence. OK, and then we have. Yes. Does that does that work? OK, so input IDs and then we'll put them to. Device. Like this, like this. How about this? Ha ha. You have to. Either decoder input IDs. OK, so. And code, let's encode just the. I see. Ha. What am I going to put here? How do I how do I have T5 answer a question then? No. How? Will there be Yannick merge sometime soon? Yes. Yes. There is. There's merge. It's I mean, I'm still I'm still fleshing out the merge, right? I'm I'm still this is a changing thing. If you I guess you can have it today. But I'm just telling you this is it's going to change. So if you're into super rare stuff, maybe it's going to be deleted soon. It's also going to be augmented. But let me. Let me show you. Yeah, OK. It's up. So but as I said, this is subject to change. And and so on. But if you so you can either go here or you can go the URL is. Store.YKilcher.com. That's it. That should be a redirect to the it's a Teespring store. So and it's going to be integrated with super rare, super rare in the sense that I might decide something is just too dumb of an item and just remove it again because I need to thin it out a little bit. But, you know, no one stops you. I'm just taking like if you if you really let's say I remove this T-shirt, right? No one stops you from just taking the channel logo and slapping it on a T-shirt yourself. It's going to be literally exactly the same thing. So, you know, the rarity is the rarity is is only because people are usually too lazy to do that. So, yeah, I'm going to probably thin this out a little bit. But I've ordered some samples and I think I'll announce it once I have the samples ready. There's also one. So it's usually it's the same like it's the channel logo on various items. It's sort of the the channel model, which I don't usually highlight, but skill greater than destiny is kind of the I don't know. It's been sort of a battle cry since I was young. It yeah. It sort of it sort of says that you can if you if you dedicate yourself, you can overcome sort of the maybe the hurdles of life, whatever. It's a bit cheesy, but actually doesn't mean like if you take it literally, it doesn't even make sense. And because both are predetermined, like your skill is maybe predetermined. Sure, you can learn something, but and then your destiny is like ultimately predetermined. So if there is destiny, you can't change it. But maybe the paradoxical nature of it is what makes it good and appealing. And obviously there is the attention for you if you're sewing, if you enjoy that. And yeah, I think that's I'm thinking of a technology good technology, bad technology biased motif as well. But that's that if you want to want to do this. But as I said, it's not announced yet. It's going to change. And there's also there's there's more stuff coming out. In any case, let's like how do we how do we actually generate stuff from this? Like, okay, we have a language modeling head here. But what do we have here? We have an auto model also with with a language modeling head. Huh? So we should we should provide some inputs. So let's say, okay, how about we make our data set such that it always starts with a Q, right? Like this. And then we could put here we could put like Q. I'm not making NFTs. No, no, no. Unless they actually become more useful. I'm not I'm not going to make NFTs. Don't worry about that. So we got decoder input IDs. Yes, outputs. Wait, that's what that's what we that's what we did. Oh, OK. Oh, no, wait, is it really? Could we do that? Aha. OK. Well, good. Yes. Most definitely. Totally. It's a good model so far. OK. How do I the hugging face trainer? You know, how do I control its its logging? How do I control it? Log level, log level replica. Like how often how often it logs. Yeah, yeah. The logging frequency. Yes, that's what I want. Frequency. Hey, where is it? Can't find it. Where is it? OK, no. Logging steps. Does this exist? There we go. Logging steps. Number of update steps between two logs if logging strategy equals steps. OK. So this is steps. And we'll just set it to. Is this a trainer or is this the training arguments? Training arguments. OK. So in our training arguments. Wait, we might want to reprocess the data set, right? And here now let's do this because. Oh no, it's being tokenized like that. Sorry, sorry. Uh huh. OK. Training arguments, logging steps 10. Train. Yes, please. And now we should finally see something. Here. Hopefully, please. Runs. This is green, so this means it's going. Yes. Yay. And we are decreasing in loss. Excellent. Oh wait, I need to catch up in chat. For how long will you stream? Not for too much longer. So maybe last year during I remember during Christmas. You know, everything's kinda. And still during Christmas, right? And I remember PewDiePie streaming and I thought, oh, that's neat because, you know, nothing else is going on. So and. Yeah, so I thought I'd do the same thing and maybe someone. Well, appreciate it. Or just be sort of mildly entertained while they search for other content. Will it be delivered all over the planet? The merch? I think so. So I've deliberately I first wanted to go with a different like with what was it? Don't remember the other one, but there was another one that could be integrated with YouTube, but they are not shipping to India right now, which is kind of I thought kind of sucked. So I think the Teespring should be able to ship anywhere. Don't feed in the decoder input IDs manually. It makes the outputs go weird. Well, it won't let me do anything else. Yeah, so I have not done Hugging Face training in a long time from scratch. But this is just the first the first instance. So we should be. Yay. Yeah, we're we're nicely decreasing. No, not towards the end here. Is this one epoch? Three epochs. OK. Batch size of eight. Yeah, that works out. 125 per epoch. And then we'll see what happens. In 35 seconds. Cool. Dun dun dun dun dun. Do not forget to share your model. I don't think this model is necessarily the best to be shared. OK, so we still. We're still kind of. Yep. So the model isn't the best right now. What data set are using its squad. So we're using squad. But I think this would be sort of the process. A sort of the process to get this started. Right. I mean, what if I just don't do anything here? What does it do? It just gives me like an end of end of sentence immediately. Is this to be expected to it? Does the. Does this even need an end of sentence? Not sure. So the question does does the model is the model initializing. Like if I reset the model like like this. Then. OK, this is a lot of output. If I do that and then train again, what happens? Yeah, OK. No, I don't want to learn about the newest weights and biases updates because. I make ads for them. I should know about the newest features and updates. So this is better right now because before we just kept retraining the same model, right? And not reinitialize it. And now at least we're at least we're reinitializing it. See, this was like our global step was. We just kept resetting the same model. But now now. You see. Do we have is this the is the one that we currently have? No. We'll see in a bit once it sinks. But even if not, you can see here loss is going down quite nicely. Anyone know if one to be self-hosting is fairly seamless training on a local GPU server to watch progress. So the I think that like I might be wrong, but the self-hosting for one to be is is mostly a thing you do as a as a company. It's not it's not the same as like. It's not like, you know, here is our open source tool. And, you know, we also host it for you if you want. They sort of take the different approach. It's it's cloud first. And if you are if you're a business and you say, well, I really need the data to stay in my in my in my data centers in my own environment, then they'll give they'll they'll give you sort of a self-hosted environment. I don't know how easy it is, though, to get to one. If you just say, well, I'm an individual and I'm just. Maybe. Yeah. I don't know, but. I guess you can ask them. So soon we should have this done. And then we can see then we'll have a model that's kind of trained from scratch. That shouldn't be too bad. No. This is for research you're working on just trying out something. This is just trying out something I haven't done. I haven't actually done NLP coding in a while. So and. I just randomly wanted to. Or I just randomly thought that I might might as well live stream it. You know, maybe we're not we're not doing a whole lot here. We'll see. We'll see. So we got three hundred and seventy five steps. We're almost done. Do we get some metrics here in the meantime? Not yet. Maybe I need to give it a different name, though. Right. I think. Like because I'm always logging to the same name. That can't be too good. What am I working on right now? I've co-founded a startup, an NLP startup, ironically. Right. Which means that I work with a lot of smart people that all also know NLP. Therefore, I don't get to code NLP anymore. I get to code. I get to code mostly plumbing. OK, so. So as you can see, let me. Let me train. Yeah, let me get a data point from here and we'll take this right here. And we'll we'll make our. Prepare data. Of this one. How do we call it? Process data, of course. Is there any model for transformers that don't need N squared paper? No, but they do have pseudo code, I believe, in the paper itself. How does your day look like? It's a wild mess of whatever burns the most. It's I do not advise. I do not advise following my daily routine. OK, so let's call this X1. And let's input that into the model. So X1 input IDs. Not enough values to unpack. Where? How? Why? No. What? Come on. Come on, man. We'll do this. List. It needs to be a tensor. So. That. Beautiful. Beautiful. Uh. Uh. Uh. Our model is terrible. How and why? Right. It just. Baffles me. What if we don't train it at all? Like, what if what if we just load the model? And we'll just input this. How many steps do you usually train for an image classification task or an image reconstruction? Do you smaller or large batch sizes? I don't I don't have a general recipe. You just go with whatever is is appropriate. And it also depends really on the task. So for unsupervised learning, people do hundreds of epochs for ImageNet. I guess you can get away with a bit less. I think like 50 epochs or something like this. But also for. Yeah, it's it's I know C410 or so people use 50 to 250 epochs, which is like a lot. But then heavy augmentation. It's really depends fully on the task. Tell us about your startup. Sure. It's called Deep Judge. It's very cliche name. And we do NLP for the legal domain. And if you're allowed to work in the European Union, no. Well, if you if you are allowed to work in Switzerland and you want to work in Switzerland and your skills in engineering with an affinity for machine learning, you may you can happily apply. That is so. Yes. Let's see if Google finds us. Yes, of course, Google finds us. OK. That is us. Me wondering what I missed. You didn't miss anything. It's just me. It's just me dicking around in this. OK, so it seems like we haven't trained anything. If or I'm just I'm just doing this completely wrong. I might want to look up some more tutorials. I've just jumped into this completely. But what you can what we can do, what we can do is you can see that the this X1 that we have right here. So the first data point and this is usually a good thing to do if you wonder if your model works at all. So we take the training data set right. We look at this data point and the context is whatever architecturally the school has a Catholic character. The question is to whom did the Virgin Mary allegedly appear in da da da da da. And the answer is St. Bernadette. Whatever. Now, what we can do is we can completely overfit on that one data point. That that is that is what we're going to do. And for that, let's restart the runtime first and foremost. Then we'll run this these things. Run this. And when it comes to the data set, we are going to. So process data right here. And then when it comes to the data set, we're just going to select one data point. Right. We'll call that overfit data set. I'm going to take the training data point. I want to just get one single data point out of it. We won't even we won't even touch those right here. Data set is not defined. Of course, why would it be? Because I need to run this. OK, so da da. OK. So we'll get our overfit data set and then we'll just we'll just train on that overfit data set. And how where is the number of epochs? Oh, num train epochs is three. We'll just put this to. Um, num train epochs is, I don't know, like a hundred or so. We'll just overfit on this one data point heavily. And instead of the training data set being the training data set, we'll make train data set is overfit data set and eval data set. Is also the overfit data set. Overfit data set is not defined. That set. I see. So if this works out. Image net is 100 to 600 epochs. 50 isn't really possible. OK. Yeah. If your judge model doesn't work, claim it is a company secret. Training, wait, already completed? This is not, this is, that is shady. OK, let's try. See, OK, we're doing, I'm doing something terribly wrong. Or we'll just put the number of epochs to 10,000. Do we have output? Please. Do I need to rename? I should probably rename, right? Ah, see, OK, this is the output. Now we're continuing. Yeah, we're overfitting nicely. We tested out the extreme overfitting approach leading to super convergence. You talked a while back. You mean the grokking? Yeah, I think this will be a different, different domain right here, but I haven't, I haven't actually done or investigated grokking any time myself. It seems super interesting, though. Yeah. Change the name of 1db thing. Yeah, I should. I see no problem with training equals zero. Are you going to do a more classic paper review? Yes. Yeah, this is, this is also planned. The classics. Why do you look like Bezos? It's a, whenever the hair gets long, longer, it just gets on my nerves. Are you loading all the data? I am, but right now I'm just loading a single data point. So. Remove the end of sentence token. True, that could be. But this isn't this part of the. I've just seen this on like one example. I might be wrong. A great masculine jaw. What's the secret of the glasses? There's no, there's no secret. It's just that it was a, it was a maybe bad branding decision. And now it's I'm stuck with them. So. So see, but now you can you can see that we are, we are converging in. Yeah, look at that. I mean. What is what is this? Do we? We don't have validation stuff. OK, but training total. Training runtime. This one right here. I mean, look at that. That is just very low. Very, very small loss. So we got a thousand, a thousand steps. We might as well sort of interrupt. What is this clear output? No, we might as well. Interrupt a little bit. Hey, would you? Would you mind stopping? Just interrupting a bit. Stop. OK. And we'll just check the output. Maybe it's something better now. Ahh, still no, still no. Hmm. Are we doing something wrong? Unk, unk, unk, unk, unk, unk, unk. Am I doing something wrong? I'm probably doing something wrong. Are you hiring? Yes, we're hiring. But you need to be able to work in Switzerland, which is, it sucks and it's not available to most of the world because of legal reasons. But we have no choice really. So you need to be able to get a working permission. And yes, this still sucks, even though we've completely overfit, which means that we're, I'm doing something quite wrong. The question is what? What's so wrong? I may not use this in the exact correct way right here. So I could sample, I guess. So do sample is. But the argmax, it should be good if we overfit. The problem is that we might just remove these end of sentence tokens, right? Because like why? Let's restart. And this is the last try. If this doesn't work, we'll just take the loss. So we'll remove the end of sentence tokens like this. And let's hope the tokenizer puts them in itself. Like that. Good. And we'll do the 1000 epochs again. And there we go. OK, last run for today. Wrong input, yeah. Yeah, we can inspect train data set entries after process data play. That's a good idea. We should do that as well. I don't understand the meaning of the output. Can anyone explain? No, no one can explain. That's it's it's it's not the desired output. We're trying to predict text. And right now we're just it's garbage. But what we're trying to do is overfit. Let's see. I could change this. Also. Overfitting on one data point. We're just trying. We're just a bunch of researchers giving our best. Your input is from the full data set. But I mean, but I take I just select the first entry like I do select. I selected this entry. You're predicting the text from your full data set, but not from overfitting data set. Yeah, but the overfitting data set is taking. It's taking the. It's taking the first data point, right? The overfit data set selects exactly the first data set. The first data point. So it selects the zero of the train data set. So this should be the same. I can I can try to just input. I could try to input. Yeah, I see. Is there a reason you're overfitting? Yes, it's because it's a good way to see if you don't have like it's a good way to catch bugs. So what you want to do is you want to take a single data point and then overfit to it. And if you can do that, it means that your model is OK or well, it means your model can at least process data. That's essentially what it means. But for us right now. So let's let's call that X2. Overfit data set zero. Right. So X2 is now. It's a whole lot lot of zeros. Zero zero zero zero. No, it's actually it's actually OK. It's more than zeros. We can decode it. Tokenizer decode X2 input IDs. Let's see if that. Yeah, architecturally, the school has a Catholic character. Right. This is fine. This is and the answer, St. Bernadette. We might. Oh, this is here. Good. So this is done automatically. We don't have to do this. And then the pad. Maybe we'll leave away the pad because right now we're padding. Right. And we technically don't need to pad or we also grab the mask. Yep. Maybe the masking is is the problem and label IDs. Yeah, this is fine. And if we input now this into. Let's input this. So we'll take X2 right here. And the decoder IDs will input nothing. This has no attribute to. What was X2 again? This is. Just a list. We need to. OK. Can we do this? I'm just I'm just hoping here. Nope. No attribute shape. Yeah, we should. We should put it into a tensor. How to make a tensor from this, do I need to import Pytorch? Yeah, probably. Yep. So. OK. How? OK, does anyone know how to make an int tensor? Like this? Long. Yep. OK. Yeah, so the output seems to be correct, but we can't overfit. So I have. OK, let's say we have a last trick up our sleeves and that is we remove the padding. So the model will just have to. Take whatever we have it. So the padding. We don't pad. We just truncate. Dun dun dun. Alright. OK. How's it coming in? Excellent. Loss is decreasing. That is a good sign. And it's going to take another. 90 seconds. Does anyone know how Yannick deletes a chunk of text with one button? It's probably you mean like the entire line like this? It's a. Oh, no. It's a Vim command. It's capital S. Tim is calling me. I should. I should leave soon. We're making we're making the the final argument, the final things for the next machine learning street talk. Alright. Done soon. You mentioned me as an inspiration for your master's application to ETH. Nice. But I don't I don't think human resources is too happy with me. Given that they've received calls in the past about my behavior. So maybe maybe don't don't mention that too much. OK. OK. I mean, you know, no to sample through. Some things, some things wrong. Like something's clearly wrong. I don't know what, but I'm doing something wrong with the with the inputs right here. Because it's. Yeah. So I'm not sure what exactly what exactly they are. What exactly they need. But here fine tuning T5. A mixture of supervised. So here in supervised training. Right. That's what they do. They have input IDs. They have labels. I have labels and not label IDs. The function automatically creates the correct decoder input IDs. But you see, you see this. It seems weird because I'm not putting it and they. Yes, something is something is definitely wrong. So labels. Let's call it labels. Right. Oh, no. I was calling it labels all along. Label IDs, labels, label IDs, labels. Labels. Labels. Well. Decoder IDs. It's worth a try. I mean. We might just try it until here. Might be the parameter where you can auto add the end token. Yes. Yeah. True. Yeah. Here you generate. So. Let's try. And instead of. Yeah. Here I have labels too. Oh, that's what I just changed a second ago. I see. Sorry for being dumb. No, but here labels. Right. It's already running. OK. OK. What's your opinion on Jax? Well, given that it has a built in. If I understand, I have not looked into Jax, but if I understand correctly as a built in XLA compiler, which is really good if you want to do super custom stuff in PyTorch. Sorry on TPUs, which I don't. But I as far as I've seen so far, I do like sort of the. It's much more clean. So, you know, maybe if they put a bit more attention also on common usability, not just with TPUs. I guess it might be a cool language. Any good suggestion for a master's in NLP? I have no idea. Sorry. I've chosen my university because it was the only one in the country that was like it was that was that was a had the things that I wanted to or that I was interested in vaguely. See, got an unexpected labels. Why? OK. How? No. Decoder IDs. OK. OK. All right. Maybe now. Labels. Yeah, no. What was it before? Decoder IDs, right? Decoder input IDs. That's better. That is better. No, you don't think so. Catholic Catholic. And what did we want to whom did the Virgin Mary allegedly appear? It's better than before. No. A two. Yeah. We say do sample. True. Now it just evolves again. I might be imagining that this is better than before, but it has the end of in the main main of the main building. I might be imagining that this is better. Maybe also because we trained it for not as long. Oh, look at that. Look at that. Look at that. Yeah. OK. Decoder IDs, whatever that means. But if I don't do decoder input IDs, but decoder IDs, I get the correct question. So we can, in fact, overfit on a single data point. See, this is the output. It is the output. Yes. It's padded to the left for whatever reason. But it is the output. And what if I put like a cue here? Yeah. Excellent. So it turns out that the most challenging part about doing machine learning is using the correct keyword arguments in the hugging face library. Coincidentally, this would be something where like Copilot would be a killer at. I've been I've been using this for a while now. And it's this kind of stuff. It's really good. So, yes. So that was essentially this was where I wanted to get to today. We have a model that can take in context answer pairs and give us questions. The next step is to take this and make this train it on SQuAD, then generate a data set of more questions and answers. And then we can use that to train another question answering model. And at the end of it, that one should be better than our original one that we just trained on the data set alone, which is a little bit magic because we don't introduce new data. Right. We don't. But what we do, what we can do is, you know, we always need we need new question answer pairs. So the input to this model is going to be a context and an answer. And we let it produce a question. Now, the context we just take from the same data set. What is the answer? That is that is the question. Well, ironically, that is the question. So what we can do is we can use something like a maybe like a sequence tagger, maybe from from space here. So that recognizes nouns or noun phrases in the context. And then we'll just take these noun phrases as possible answers. So what these papers this is not my idea, right? These are papers that that are in question answering. What they do is they would find noun phrases. They would let this model that they've trained to produce questions, produce a question given the context and this possible answer. Then they use the original question answering model to check whether it actually gets it right. And that's how they generate new data. That's essentially what we're going to do. But yeah, this was it for for today's live stream. It took it took a bit longer, but it's good to refresh myself as well, because as I said, I've been doing mostly plumbing coding and not machine learning coding. So this is fun. And I hope I hope at least someone someone learned about the trick of overfitting on a single data point. Because I remember when I first when first someone told this to me, I was like, oh, wow, this is this is really helpful. And I'm sure most people know it by now, but it's still super helpful. All right. So in this case, thank you very much to everyone who's who's been here. I need to hop on a call with with Tim and Keith for the street talk. And yeah, I'll see you around. \n",
            "Score: 0.445334822\n",
            "------------------------------------------------------------------------------------------ \n",
            "\n",
            "Source2:\n",
            "Streamlit for ML #2 - ML Models and APIs: https://youtu.be/U0EoaFFGyTg\n",
            "Document2:\n",
            "Okay, in the last video, we had a look at how to build what you can see on the screen right now. A very simple interface using Streamlit. Now, what we want to do in this video is go through how we actually build the smart part behind the Open Domain Q&A system that we're going to put together here. So, I said before, there are a few components to Open Domain Q&A. We're going to stick to the first two for now. So the Vector Database, which we're going to use Pinecone for, and the Retriever Model, which we are going to download from Hug & Face Model Hub. And we're going to use the Sentence Transformers library to actually implement that. Now, the first thing we are going to want to do is create our Vector Database. So our index. Now, to do that, there are three parts or three steps we need to take. First, we need to download our data. We're going to be using the Squad Dataset from Hug & Face Datasets. Then we want to encode those vectors, encode those paragraphs, or what we call context, into context vectors. And we use Sentence Transformers and a Retriever Model for that. And then the next part is uploading or pushing all of those vectors into our Pinecone Vector Database. So, to do all of that, we're just going to very quickly go through that code, because it is a lot and I don't want to focus on it too much. So here we have a script. I'm going to maybe zoom out a little bit so you can see. So the first thing we do is import everything. You don't need TQDM here, but you can pip install TQDM if you do want to use that. So we are from Datasets. So this is Hug & Face Datasets. You will need to install this. So that is just a pip install Datasets. We're going to first initialize our retriever model. We're using the Pinecone MPNet Retriever Squad 2 Retriever Model. So this is a retriever model that is based on the MPNet model from Microsoft. And it's been trained on the Squad 2 Dataset. And first thing we need to do is initialize our connection to Pinecone. So this is where we're going to store all of our vectors. Now to do that, you do need an API key. I wouldn't write it in your code, but I'm going to just do that for the sake of simplicity here. So I'm going to go to this app.pinecone.io. And this is free by the way. You don't have to pay anything. So we just go to app.pinecone.io. And then you will have to sign up. So you create an account. I already have one, so I don't need to worry about that. And I have this default API key over here. I could use that. And yeah, I'm just going to use that so we can see the key if we want. I want to zoom in a little bit. Of course, it's a little bit bigger. So we can see that. And we can see the value there. We can just copy it or we just press over here and copy that across. And then I'm just going to copy that across. And then I'm just going to paste it in here. Okay. Now this script, by the way, I will leave a link to this in the description. So you can just download it instead of writing it all out. Because this isn't essential to our app. It's just how we build our... We encode all of our contexts and actually saw them in our vector database. So we have that. We have the cloud environment that we're using there. Switch this back to the app. We want to check if the index already exists. So I'm going to create this QA index. Now, actually, you can see in mind I already have it because I've run this code already. So QA index already exists. So it's not going to create a new index. And instead, it's just going to connect that index here. Right. So we just connected or we created our index, our vector database index. And now what I want to do is I'm going to switch back to our data. And I'm going to run through that. So I'm going to load the data set and the squad data set from Hugging Face. Now I'm going to use a validation split because the model has been trained on the training data. But squad, I want to make it a little bit hard. So we're going to use a validation split that hasn't seen before. I'm removing any unique or duplicate context in there. So zoom out a little bit here. Squad depth, we're using this filter. So this is all Hugging Face data sets syntax here. And then we're encoding it. So this model.encode, so this is our sentence transformer. We're encoding it to create a load of sentence vectors for our context. And we're converting these to lists because we are going to be pushing these through an API request to Pinecone. Again, we need a list, not a NumPy array or otherwise you're going to get an error. OK. Then back to the Pinecone side of things. We want to create a list of, it's basically a list of tuples. And those tuples include the ID of each context. So there's a unique ID for each context. We want the vector or the encoding, the context vector. And then we also have this dictionary here. Now this is metadata. So metadata in Pinecone is like any other information about your vectors that you want to include. And this is really good if you want to use metadata filtering, which is super powerful in Pinecone. And I definitely want to leave the option open later on. I'm not sure if we'll use it or not. We'll probably put something in there just so we can play around with it. Now that creates the format that we need to upset everything, which means just like push or upload everything to Pinecone. So then I do that in chunks of 50 at a time. Just makes things a little bit easier on the API aggressor rather than sending everything at once. OK. So that's like how we create the index. So now what we're going to do is actually integrate that a little bit in our app. So let's switch back to our app here. Let's move this over here. Let's view it. So first, let's just remove this. We don't need that. OK, save will automatically reload. So first thing we want to do here is let's initialize the Pinecone connection. So I'm going to just take. Let's just take this part of the code. Just copy it and then we'll remove what we don't need in a minute. And so we don't need we do need sentence transformers in a minute. We don't need data sets. We do need Pinecone. So actually here we're initializing our retriever model. It's the same as what we did before. So we do want to keep that in there. Make that bigger. API key again, just store this somewhere else. Or if you are using Streamlit Cloud, they have like a secrets management system. And it's something we'll look at in the future for sure. But for now, I'm just putting it in here. So we have our API key environment and we're just doing the same thing we did before. But actually, we don't want to create an index. We're assuming we've already created an index if we're in our app. So we're just going to connect to it. OK, so with that, we've got our API key. We've kind of set up the like the back end part of our app. Like the smart part that's going to handle the open the main Q&A. But it's going to be a little bit slow. And we will have a look at how to solve that pretty soon. But for now, what we're going to do is actually just implement this. And we're going to actually query and see what we return. So I'm going to save this. We won't see anything change in our app now other than the fact that it takes longer to load because it's downloading the driver model. That's the main part of the slowness here and then obviously connecting to Pinecone. Also takes a second as well. So for now, we're going to deal with how slow it is. But we will fix that pretty soon. And now I actually want to do is I would say, OK, if the query is not empty, because by default it is empty, that's why we add that in there. So I'm going to actually remove this. Enter. If it is not empty, so if query is not equal to nothing, we're going to query Pinecone for whatever is in that query. So the first thing we need to do is create our context vector. So I'm going to write XQ, just shorthand for context vector. It's pretty standard, especially if you use FICE before they tend to use this. And I say I said context vector, I meant query vector. So we're going to do model and code. And we need to put this in square brackets and we have query. OK, and I'm going to convert that to a list. OK, so this is going to create our query vector. Let's write it down. Create query vector. And then the next thing we want to do is query Pinecone with this vector. So to do that, we want to write first list. Get relevant context. And we're going to store these in XC. So like query like context vector, similar thing to the query vector that we used before with XQ. But this time I'm going to write index.query. And we're going to pass XQ, so our query vector. And we're going to say how many results we want to return. Now, later on, we're going to use Streamlit, a little like a slider bar to decide how many we would like to return. But for now, we will hard code it. And another thing that we want to include here is we want to tell Pinecone to return the metadata because by default, it will not return metadata. So return metadata equals true. So these are like the extra little bits I mentioned before. So included the title. So like the topic, Wikipedia topic that the context is coming from and also the text itself. OK, so we're going to return the relevant context and then we're going to look through each of those. Now, when we do this, there's a particular format that we need to follow. So our context are actually going to be stored for context in XC results. And results is going to return a list and we just want the first item in that list. The reason it returns a list is because if you are querying Pinecone with multiple queries, it will return lists of your answers for each query. But in this case, we are only ever going to query with one query vector. So we always enter at position zero here and then in there we will have all of our returned matches inside this matches key value value. So for context in there, all we're going to do is write st.write context. And then we want to go into the metadata that we were returning. And we have title and text here. We don't want title, we want text. OK, so let's save that and check that actually works. So again, this is going to take a little while to load because we're initializing like the full pipeline of our vector database and the tree model. Every time we run this, it's downloading the full tree model, which takes quite a bit of time. OK, so this is just rerun our app. Now we can say who are the Normans? OK. Again, it's going to reload everything, so it's going to take a while. We're going to fix this in the next video. So we should be returning five contexts and if we scroll down, we can see we have these five paragraphs. Each one of these paragraphs is a single context. So we could maybe we can respect the element. OK, so we can see down here. Pretty horrific to look at. So if I zoom up, we can see each one of these is a single one of our contexts. Right. These here. Cool. So I think that is that's it for this video. So now we have these backend working and the next one, what we'll do is we'll do a little bit of a fix and the next one, what we'll do is fix this issue with it taking forever to load, reload everything every time, which is actually super easy. But we'll make a big difference to our app. So thank you very much for watching and I will see you in the next one. \n",
            "Score: 0.430762559\n",
            "------------------------------------------------------------------------------------------ \n",
            "\n",
            "Source3:\n",
            "How-to Structure a Q&A ML App: https://youtu.be/4Jmq28RQ3hU\n",
            "Document3:\n",
            "Okay so I had an idea. I thought maybe what we could do is actually put together a few videos and what we'll do in the next few videos is build out a full Q&A project and this is something that I've actually been wanting to do for quite a while now and I think it will actually be really cool. I was thinking through this morning and I thought okay today we're gonna start it. So what you can see on the screen right now is the GitHub repo for this project which is completely empty almost. There's a read me a gitignore and this one architecture drawing that I sell literally you can see four minutes ago. So I'm gonna show you that and I'm going to explain what we're actually going to go through and I think we're gonna cover a lot of different things. So I think this is I think that is one of the reasons why I think this is such a cool project. So at the moment this is like the basic architecture of what we would need to build a Q&A model and the end goal is to have is to have a front end which looks kind of like this. So we'll have like a search bar here and we'll have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this is like this is Marcus Aurelius and I think something like this maybe this or something like it would be pretty cool to just have in the middle of the web page and underneath we have a search bar and keep it pretty simple so I think that's everything really for the plan and I mean the first thing we're going to do in the in the next video is actually sell requests and download that data and maybe pre-process it as well or they they might be two videos so that's everything for this video I hope that you're as excited about this as I am because I'm really looking forward to actually building all of this I think it'll be super cool and I mean ideally at the end it's one it's gonna look cool and two we're gonna learn like a huge amount of stuff if you even put all this together there's so many different things that you need to know in order to make everything work so it should be really cool and I'm looking forward to getting started with it so I will see you in the next video where we'll \n",
            "Score: 0.415397942\n",
            "------------------------------------------------------------------------------------------ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_text = \"How to build next-level Q&A with OpenAI\"\n",
        "retrieved_texts, retrieved_sources, scores = retriever(query_text)\n",
        "for i, (retrieved_text, retrieved_source) in enumerate(zip(retrieved_texts, retrieved_sources)):\n",
        "  print(f'Source{i + 1}:\\n{retrieved_source[0]}: {retrieved_source[1]}')\n",
        "  print(f'Document{i + 1}:\\n{retrieved_text}')\n",
        "  print('Score:', scores[i])\n",
        "  print('-' * 90, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcVHZNQn7PGb"
      },
      "source": [
        "# Instruct a template for model's prompt using user query and retrieved documents to give model additional context to genrate answer based on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZguHFgcth-pV",
        "outputId": "f9c9d173-26ca-489f-9ca5-fa7095398e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SystemMessage(content='You are a helpful assistant that always answers questions.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"\\nAnswer the question based on the context delimited by triple backticks and show answer sources with their titles and urls.\\n\\nContext:\\n```you you you you you it How's that? Anyone hear me now? Yeah, it wasn't an error. It was just me being stupid. Because I made a new scene in OBS and I just didn't add an audio source. So that's that. There's not much more to it, honestly. Cool. So I have actually no idea what we're going to do today. I just thought maybe people are sitting somewhere at home and yeah. So cool. Yeah, I might not get all of chat. So I'm sorry for that. As we're coding at the same time, this might not be super possible. But okay, my sort of goal today was to get a little bit of a hugging face, tuning something going. So the idea is to do question answering training, maybe make that into a little video later. But I have no clue how this is going to go. Also got my trusty Weights and Biases cup here with warm tea. The stream is not sponsored. It is an awesome beaker. I genuinely don't like tea in these tiny cups that only hold like two or three deciliters. It just sucks. Okay. What was the first machine learning algorithm I implemented? I have no idea, honestly. Probably we had to implement like support vector machines by hand at some point. Yeah, that was it. Okay so wait, if I click on this, the other thing is going away. Okay, maybe I can pop out chat here. I'm trying to follow the chat as closely as I can. Forgive me for getting... Not tea, you need coffee. Oh yes, there's one other thing we need to do. There's one other thing and that is, one second, that is obviously we need to do a chroma key on the video right here. So yeah. Is this even... Not too bad. I can't see it in the previewer. I don't know if you see it or not, but OBS is screwing up, so I can't really see it. But this seems fine, right? You can see sort of through me. All right, so prediction, someday at the Alex Friedman podcast. What's the story behind the sunglasses? I just put them on randomly and then sort of that became my brand. No, I guess I'm not sure. I'm not sure that that was a wise choice, but it is what it is. So okay, the goal, let's say the goal, the current goal is train question answering. Let's do that. And I have not prepared this, so the previous was nicer. You mean this screen, but now you can see the screen behind me. So if something is here, you'll be able to see it. Yeah, but the settings might not be super good. One second. Well these are polarized, so I don't see exactly too much. Which route do you suggest? Passionate product management or ordinary ML engineer? Chat product manager and beta ML engineer. Well, do whatever you have fun doing. I feel people make too much fuzz about career choices and I don't know. It seems like you should just do whatever you want to do. And as long as you have fun doing that, there's really nothing that can go wrong. So just do that. That seems like the most logical thing. Okay, so what we're going to do is a little bit, let's say we're in a hogging phase and I have not trained a hogging phase model in a while and this API keeps changing and progressing. So my first question is, I know there's this pipeline thing. What about training an answer question instead? That's actually what we're going to do. So one of the techniques for getting better question answering is to produce questions from the answer, which I hope is what we might be able to do this now. So best way to get started with machine learning, I don't know, do some tutorials. I think just whatever you like. There's enough information out there, especially for beginners. Oh no, wait. Proper product placement. I need to hold it like this into the camera. So refreshing. So I know there's this pipeline thing, right? The question is, can I fine tune a pipeline? Can I fine tune a pipeline? Or essentially, can I train a pipeline? What or is this only for inference? So I don't know. Here under fine tuning a pre trained model. Can you train hogging phase model with CPU? Sure. But do you have Omicron? I have no idea. You can fine tune the model, right? With this trainer, however, the model would be sort of, you know, like a model like this. So where do they get the model from? The model is like a auto model for sequence classification. It is not a pipeline. And the good thing about the pipeline, obviously, is that it makes, you know, things like loading data sets and and stuff like this pretty, pretty easy. My question is, where does the tokenizer come in right here? So you see pre process your data, input function, tokenized data sets. So you need to set up, you need to set up the tokenizer at the data sets, and then input the model as this model right here. But it seems like fine tuning a pipeline might not be a thing. What hardware do you plan on using? I'll just do a Colab for now. I guess that's my that's gonna be my thing or even fully, fully locally. Yeah. All right. Test trainer evaluation strategy. Fine tuning in native PyTorch. I don't want to do that. I just want to do it with the trainer with the trainer API. That seems fine. All right. Let me quickly pull out a Colab or open a Colab. And I'm sorry, sometimes I'm going to pull stuff to the side right here because I have no clue what's going to pop up. All right, we have one with the absolute awesome name of Untitled2. And we're going to change that to hogging face question answering. So how much GPU size do you have? Only none. What do you mean by fine tuning a pipeline? You can fine tune a model only. Yeah, exactly. You can fine tune a model, but there's this pipeline abstraction that sort of so if I maybe I can I can show you so let's place this in here again, right there is this pipeline. And it takes care of a lot of stuff. So if you did question answering, right, you can just load a model and the tokenizer. And what it would do, what it would do is essentially it would it would handle all of the sort of plumbing around question answering. But we don't we're just going to do it by ourselves. I think that's pipeline batching. Yeah. Pipeline custom code. Yeah. Implementing a new pipeline. So then we have just to use models for inference. OK, now the question has been answered. All right. Let's see whether we can answering fine tune example. Because we're not gonna because we're not gonna do anything ourselves. We're just gonna use stuff. OK. OK. OK. I mean, there must be there you go learn how to fine tune a model for question answering squad. So this is going to be our first thing. We're going to fine tune model for question answering. Then we're going to try to use when I try to use the model that we've we've trained as a baseline and we're going to try to train another model that will generate questions given some answers. And then we're going to augment our data set using those pairs. And then hopefully that will lead to a better data set. I've seen papers that do this and in question answering, especially in natural questions. And this has been really has been sort of improved, but or has been improving state of the art. But who knows? I'm a student in need of compute for a project. Any advice for compute and in general? Not really. I mean, there's there's Colab. There's there's quite cheap cloud compute. And if you are a student, your university might have your university might have some resources or sometimes the big companies, they give sort of credits to university students. So I don't know, it might it might take a bit of politics, but you can usually get stuff. All right. Let's see. I've as I said, I haven't done this in a long time. So yeah, so we go out. We have a tokenizer. We need the distilbert. Okay. But at the time, but at the time, but at the time. And to now we don't want TensorFlow. Okay, this seems okay enough. Sequence classification, token classification question answering. Pytorch notebook, but there is already one. Why do we open? Why do we open a new one? To be able to share your model with the community. Now we don't want that. All right. So let's start. Yeah. Okay, no need to log in. But what we want to do is we also want to install one DB. Because how do we do that exactly? Let's check. Yep. Got it. And code. And this is just logging. Okay. I thought we had it. Yeah. All right. Thank you. What's wrong? Ah, very smart, very smart. Should have made it quiet. Anyway, okay. So we need to install git LFS. So what do we go here? We're going to for squad v1 or v2. Going to use distal-berth batch size of 16. And use the datasets library, which is pretty good. Then, okay, we're going to show some random elements. Just fine tune evaluation. Seems easy enough. We'll do that and we'll figure out in the meantime what we need to do in order to go further. Yep, yep, yep. Uncomment the first slide. Yes. I was absolutely able. Okay, appending. Okay, good. So we're logged in. Nice. Let me open. Let me open. Let me open the 1DB. We'll figure it out once we log something, right? I guess we can just run all now. All right, this looks good so far. We got probably the same random examples. And so here you see how this dataset is built. This is made from Wikipedia articles. So the dataset creators, they always sort of grabbed a paragraph from Wikipedia. I think it's always kind of the first one. Not sure. So I don't know, carrot. I think it's always this one, but I'm not entirely sure. It might be any one, but I think I'm also not sure of the exact differences between squad V1 and V2. However, that's kind of... Do you have your own GPU workstation or university? So our university had a cluster. They built it up during my PhD, which is fairly big. But our group also had sort of our own GPUs. Yep. Where were we? So yeah, this is sort of a paragraph. This is, I guess, the ID or the title of the article in Wikipedia. And then there's a question like what was proclaimed the state religion under Theodosius I? And then there is an answer. And the answer is somewhere in the text. So it's extractive question answering. And you can see here Christianity or sorry, Nessene Christianity is the correct answer. Now the answer might actually be multiple times, mentioned multiple times in the paragraph. And it is important that you not only get the answer correct, but you get the correct index. So you also need to decide which, if it's mentioned multiple times, which of the mentions is the correct one. And that would be sort of the one that directly answers the question. So it's pretty strict. So this is closed domain extractive question answering. It has sort of a reputation of being a lot about kind of keyword finding and sort of comparing the model doesn't need to understand that much about. The question is out of frame. What? What? Oh, why does this keep happening? Thank you. No, wait. Huh? Huh? OK. I'm out of frame, too. Look at that. All right. How do I fix this? I'm out of frame. OK, so. Hmm. OK. So. OK. So. OK. So. OK. So. OK. So. OK. So. OK. So. Isn't YouTube 16 by nine because I'm streaming 16 by nine. If you see like here and OBS. Right. You can see. Oh, no. OK. It's the side of it's the side of the screen here. OK. okay we might be able to fix this there's probably a crop somewhere going on you see there's nothing here OBS is screwing this up. Crop right there's no crop right none so I don't know what's what's going on alright we'll just have to live with it I think this is OBS not not handling the screen resolutions well in any case oh yeah you can see me totally fine I was confused about that alright sorry so the answers if you're able to see them I'll try to make it a little bit more inside the answer right here is a string and also an index into the text 169 and I'm gonna guess that points to sort of this piece right here alright so this is the data set and pre-processing the training data consists of what consists of instantiating the tokenizer then you can see that we tokenize on two sentences one for the answer one for the context okay and depending on your model we'll see different keys yeah okay so what's this what is your no this is beginning of sentence right 102 what's 102 what is your name is maybe this run right here then it's sort of the end of sentence and then it's another my name is Sylvan and then it's another end of sentence that's this tokenization the attention mask is there okay so we can feed things to this tokenizer with a max length then what we do is we go through the data set let's find a long example this is just exploring if we just truncate we'll lose information so we need to go sort of in a stride over the data set but the tokenizer is doing this for us now we have a list of input IDs okay so we split up the context if it's too long into these different things into these different chunks and we iterate over the data set you can see that the question is always the same but for the context for the context will truncate the second or will stride over the second thing so here is the men's basketball team has over yada yada yada yada yada and if you're at the end here well that is a long context you see that 32 wins were that is the end and that is not the beginning here the beginning is championship so if I am going to guess if you go on here you're gonna see the 32 wins somewhere in yeah you see here the 32 wins were the most so this kind of striding over this document any new machine learning street talk episode coming yes yes there's one coming we're working on it when do you release a Udemy course? I don't have the stamina to create a course and I think other people are are better at that but who knows I don't know not yet not yet okay this will give us we need to find which of those features the answer is where exactly the models will you require to start and end position to map parts of the original context to some tokens thankfully the tokenizer can help us by returning an offset mapping this gives for each index of our input IDs the corresponding start and end character in the original text that gave our token that's pretty useful is this how new is this because I remember having to do this myself which was awful or I just didn't know about this the very first token has 0,0 because it doesn't correspond to any part of the question answer the second token is the same as the characters 0 to 3 of the question the how here that is one token and so that goes from 0 to 3 and then the next token goes from 4 to 8 4 would be many to 8 you can see that the white space is sort of falling away so not every character is covered by an ID this is pretty useful yep oh that's what they do right here so first token ID, input IDs, okay we get this, offsets, we get this then we get the token and we also get it from the question you can see that it's almost the same but it has been lower case yeah to anyone who's new will train question answering but will also just I've not prepared also chatting let's do that up mmm okay yeah I'll kick the bot or hit it cool alright so so we can use this mapping to find a position of start and end tokens have to distinguish which part of the offset correspond to the question which correspond to the context sequence IDs okay the sequence ID says 0 and 1 for the question and the context respectively it returns none for the special tokens okay so if we now have an answer say we have an answer and the answer start which is the index right into into the which is the index into the text the end character we can figure out by simply taking the length of this answer and adding it to the start and what we're gonna do now start token index of the current span in the text so we're simply going to loop over the entire thing until we reach the second thing so here is the question and here is the context so we're going to loop until we hit one which is the first token and then we're going to also to find the end index of that we're going to the end here and we're going from the end we're subtracting negative one until we reach okay that's that's cool then we're going to um detect if the answer is out of the span in which case this feature is labeled with the CLS index there's a lot of like plumbing to do right if the offsets of the start token in of the token start index so now we're going to look at these offsets so where in the original text is a given token if that's outside of the um starting character which we have right here so if that is smaller okay so if the answer is somewhere after the start index so the answer is somewhere after this one and somewhere before the last one right over here then we have the answer in this context otherwise we don't and this code here is to figure out where exactly the answer is in that we can double check that it's indeed the theoretical answer so what we're going to do is we're going to look at the answer that's given from the data set and then we're also going to tell the tokenizer to decode from the start position to the end position in the input IDs and we got it cool okay so we also tell it to pad right let's put everything together in one function we will apply to our training set in the case of impossible answer the answer another feature being another context we set the CLS index for both the start and the end position we could also simply discard those examples from the training set if the flag allow impossible answers is false okay alright so this is prepare train features we're going to we'll explain how to handle long texts for train and inference yeah that's what they do right here right they simply set this stride right here and so they stride over the context that's what they do in the tokenizer so the tokenizer is going to emit always the question and then a part of the document and this part is strided over the document and then for each one you're trying to figure out where the answer is in the context and if the answer is inside you try to figure out where if the answer is outside you you'd simply say this the answer is nowhere in here which you dummy encode as saying the CLS token is the correct answer so I'm which gives you also an indication of whether or not the question is unanswerable which is going to be important in later data sets for example natural questions has questions that are sort of unanswerable and having sort of a this this gives you like an immediate classifier so that's how I guess how you do it what's the best way to de-tokenize text I don't know give it to the tokenizer and say say decode right here okay so here we put everything together I'm okay this pad on right yeah max length stride return overflowing tokens true offset mapping is true are this is what you need to put to get these mappings padding with pad to the max length okay and now for each now we look offset mapping here we get a dictionary and we're going to get the offset mappings out of it I'm going to feel other in entries here calling start positions and end positions okay I guess we can skip over that but at the end we're going to end up with sort of a a dictionary that I guess we can look at it right here no okay this is a we can't we can't this is not a global variable alright but in any case we have up this one here features features is a dictionary no batch encoding but they should be the same ish as a dictionary does this work yeah exactly so we have input IDs which is or the samples I'm what T do you have weights and biases T obviously it's a some kind of berry T you can activate vim on colab mmm let me figure that out and I've knew I've known this but I somehow don't like I don't like I'm not too much too much of a fan of in bindings in other things I'll try where you tools settings there we go yes that done arm okay good we have we have been bindings alright not too big of a fan because I I don't have all my key bindings like my been mercy thought the same alright and finally finally will have a tokenized data set by applying the map function to data sets alright and then we instantiate the model from the checkpoint where's this where does this come from see now search for it and I don't know I don't know where it is right so I need to search like this and where is it it's on top here it's just not not too pleasant of an experience in bindings but continue so will load up the model like this from the checkpoint right then arm will have some training arguments right here yada yada yada push to hub no arm yeah exactly so okay we've instantiated our trainer and now we can just say train and if this is working out all we should be able to see this in our 1DB um dashboard there we go wait break out the tab here do we have an accelerator even um runtime type we do not alright let's try again runtime restart and run all so in the meantime we can think of what do we do in order to actually uh do what we came here to do namely um wait is this on a custom data set no it's on squad squad is a question answering data set so what we want to do is we want to actually take the squad data set and generate more questions from it so what we need to do is need to train a model to take in a um a context and an an answer and give us the question right this is is different so it's a bit like um we actually need to do generation of text and not just question answering and we can take that data set and feed it back into the question answering so how do we do that um we need some sort of a text generation model so we go onto the hub let's say here we have examples fine-tuning with custom data sets um sequence what's this sequence classification token classification question answering it's all along the along the same lines but what we need to do is a text generation right so um train and what do we want maybe a t5 or so or or maybe a gpt2 some some model like this we could we could generate so maybe we'll start real small with like a distil gpt or something and we'll fine tune it okay so let's take that can we fine tune the distil gpt probably or do we have to fine tune gpt and then um that's we already have this how to fine tune a model on yeah okay translation summarization speech recognition audio classification language modeling which one I guess the translation might be most appropriate to us no since since um what we want to do is we have like an input like it's a sequence to sequence task and can we already see anything no we're training we don't see anything yet let's see there we go where are my charts we only have that yet we didn't actually log anything yet okay but okay we have a GPU now that's good yeah see there's no no epoch yet I mean this is gonna take a while I have a feeling it's gonna take a while right it's just five hours look at that okay in the meantime we need to we need to figure out what to do so where do we need to hook in um where do we need to hook in why not question answering we're doing question answering we're just doing it a little bit weird where do we hook in in order to find so I feel the I feel the data set here is just fine right we get input ids which contains this contains the this contains the but do we really need to point if we give it if we give it start and end positions those are the labels input ids the point with these transformers is you can always you can give them right you can you can give them a part of the input as labels like what we had as the sequence ids before these are zeros and ones and usually you do zeros for like the for the first sentence and ones for the second sentence or here you do zeros for the question and ones for the context can we somehow abuse this to mark the answer probably not maybe what we need to do is we need to give it the context the answer and then produce the question but in an order aggressive way I'm late can someone shortly explain what we're trying to achieve so we're just we've just analyzed this notebook here that fine tunes question answering and we're seeing now that's just going to take way too long but the goal is to take this and actually produce questions new questions so we're going to train a date this a model that takes in the context then the answer and then gives us a question so that is that is the goal right here and for that we are looking at fine tuning fine tuning a translation model fine tuning a pre trained model common downstream tasks here we're going to fine tune a translation model but the tokenizer this is very very much into this is another sentence can we fine tune a T5 is that too big of a model T5 see people have done this before huh are they used they used this is an old version classification sentiment span extraction mmm summarization OK so we could potentially use this right here is full stack advantage OK attractive summer yep so this and that we got a GPU what are they doing here custom data set so we're outputting said he's target IDs does this still run I wonder let's just sort of let's just sort of stop this right here interrupt and let's give this a try too many sessions yeah just OK good stuff well this is this is training by hand so essentially all we need to do is we need to get the squad data set and we need to replace the summary we need to explain it replace the summary source with the context and answer and then the summary target with the question this should be OK ish I have done this already no sorry I need to log in briefly into one DB just putting someone something else in my control C in my clipboard OK so we got an error this might be an old an old thing sentence piece library we can do that yep done so now we just splice the two together first loader train OK this goes from a data frame probably easier if we do if we did the so we need these source mask and here we need source mask as well target IDs source IDs and source mask you also need a tokenizer so I feel we can copy over the data set from the last one OK good so now we we don't have the data set which is fine T5 for conditional where does this come from have you tried comment I have not tried comment I'm sorry T5 for conditional generation that's what we need with the language modeling head on top yes so input IDs position embeddings from a T5 tokenizer what about target returns a loss labels are there we go no this is sequence classification how can we compute the loss if we have batch size sequence length how do we compute a loss exactly if we don't have targets like your label so you just give OK I guess we can do that all right so here here's what we do we'll do a new collab as we had it here we'll just make our own and we'll click click click together whatever whatever we need so this is probably going to yell at us no OK do we have the GPU change runtime type GPU yes it's going to yell at us we're going to terminate the others and close I've just started a job and your channel is a common source of inspiration Thank you. I hope I'm not destroying that today because live streams are quite a bit more more boring but we'll try our best yes OK well so we get the data set and then we get this will just try that we'll try the T5 thing that they have here so this is for inference this is for yeah we can do this this is fine the Vim binding still here no they're not tools settings editor Vim so OK we still need sentence piece excellent favorite beer I don't I don't have a I don't drink beer so no idea sorry OK tokenizer we wait we can't get pretrained token this is impossible what's our tokenizer none our tokenizer is none how let's grab this put it down here and see my bindings don't work this is annoying how can this how can it be all right help me here why is this why is it none wise tokenizer none have you experienced imposter syndrome like long periods without published papers how did you go through yeah I think yeah this is quite unless you're like super stellar at academia this is very very common yeah it's it's like it it seems like everyone else knows more than you and you can't like you just can't let it get you down and also a lot of research fails but I mean I think for most people the answer is just to prevail there it might be a little bit tricky if you have like if you really have a wrong topic that where it's really hard to publish stuff yeah but oh OK oh I'm just seeing that no not again sorry need need to get quick power yeah see this previously happened on live stream where I was streaming and my battery just kept decreasing in power and now I even have the big charger here but when I did the Minecraft stream that was the case so even though it was plugged in it would just slowly decrease in in in battery over time so I might have to stop it might just break at some point and then I'm really sorry but yeah we'll see how long this lasts OK in any case did we figure out did we figure out why the tokenizer here is none maybe we have a wrong one so up to five small all right I mean this should be should be OK no maybe we can get an auto tokenizer all right instead of a an auto tokenizer instead of using the T5 class seems reasonable enough so like this let's take this put it here up that's better OK all right see none of this Vim stuff is working if I cut cut this and OK I can paste it but like I have I don't have my my zero buffer aren't you afraid that your search history will pop up when you type the search bar yes yes I'm afraid well I'm afraid I mean it will pop up but I'm not I'm not that interesting of a I don't think my searches are interesting enough to warrant big worry yeah I I'm rather yeah what are we working on we're trying to make a model that that gives us questions to answers which so let me update the text right here current goal just chatting also training model that OK all right so now let's look at our loss our loss should be a number prediction scores are something 1732 that's probably the vocabulary that's probably the length I guess because the input IDs what's the input IDs should be a tensor of length seven all right so and then we can use the second line right here in order to conditional generation from pre trained that's the same line that's the same line that's the same line and there we can generate stuff from it and the outputs here are so let's give it to the code done done done done done done done done list canopy interpreted as an integer how about that yep so there you go that is hello my dog is cute hello my dog is cute no shouldn't we sort of auto aggressively and shouldn't we be able to generate text from this I have no idea how this how this works generate inputs max length min length do sample really stopping let's let's put 10 here so cute and cute use batch decode yes true absolutely very cool all right so we can force it to to create something now we need a data set and will copy this from here will get the squad data set uh huh data sets we have data sets over here we have data sets cool we'll get the squad data set load data set squad let's just go with squad data sets is not defined right good let's just remind ourselves and mostly me how this works okay yeah so this is all this is all good I used to we used to all do the all of this by hand right like load the data and process pre process stuff and yada yada yada so, and this has been, it's gotten so much easier recently, it is, it is crazy. So we'll get the training data set, and this is an index of all right this is not just an iterable yeah. So, it's technically what we can do is, how does this map work right here. map. Right, we can we can do this. This map function. No. Yep. Do this map function. And what it does is you can give it a like takes a callable accepting a dict as an argument and iterates over the data set by calling the function with each example. So that's what we do. We'll map it. Or how do we, how do we then train. How do we train. Trainer dot train so we got the model, we have a data set, and how does the data set need to be structured for training, that's my, my only other question. So, like how do we how do we know what the, the loss is. IMDB. Tokenize function. So we tokenize it. But then what's the. How do we tell the model. What the label is. We just give it the data set, but is that just like inherent in this data set. Because here you see map. So. We return, we map the tokenize, tokenize the data sets. Small train data set. Right, this is what I don't get. So we simply extract the text. We tokenize it, which is cool, which is fine. Right. But then we only have the text. How does it know what the labels are. Labels. Now this is for evaluation. Report metrics. Report metrics. This must be inherent to the data set somehow. Auto return by tokenizer. OK. I see. Cool. So if, if we transform this right now. So let's make a function. Dev process data because why not because. I feel like implementing a Turing machine. Every function could be called process data. So this is going to be an example right here. We. We pass and at the end we'll just. Pass in the zero if data point right here. All right. So. If we do this, we'll just get our normal. Our normal thing. Now what we need to do is we need to get the context. The answer and the question. And for the context and the answer. Let's just concatenate them. Yeah. We'll do it. We'll do the cheapest. We'll do like the GPT three cheap trick right here. So. Process data of data. Yeah. That's what we're going to do. OK. So we'll do the input. Text is. Let's put the. Context right here. And. We'll do. We'll do GPT three style. We'll do. Context colon. Of this. And. We'll do. Answer colon. I know T5 does things differently a little bit. But we don't care. Example. Answers. We have multiple and we could have multiple answers, but we'll just take whatever the first one is. So. The answer. Text. Zero. Like that. So this is going to be our input. And then. This one right here is the prompt. And output text is going to be simply the question. All right. Now we need the tokenizer. The tokenizer. We have it somewhere up. Right. Good. Oops. Oops. Oops. See what did I do? Encode. The input text. So these are going to be the input IDs. Do we need a mask? We can give it two things. Can we also give the T5 thing two things? Maybe not. Tokenizer. Probably not. It's going to be fine. Right. This right here. Encode. Return tensors. PT will do that. Let's just do this once. OK. So the input IDs is going to be. Good. Could we put a second one here? Like what happens if we do this? Then. Are these two lists now? Let's see. This is the tensor bracket. Anyone see whether these are two lists? Probably not. Right. So if we do this, the last one is a 10 1. And then if we do this. The last one is not. Here is 10 1. And then there is 304 something something. OK. I mean, we could just do language modeling as such. Right. And we could then even let it figure out the answers. But that's kind of crappy because we would also train it to do the context. T5 is encoder. Yeah. T5 is encoder decoder. You must specify the decoder input plus pad plus plus label. Yeah. So the query, the queue here. Would probably be the part of the decoder. Like here, we're going from a T5. This is Pandas. This is validation down here. We have target IDs and source IDs for this one. Right. So input IDs. But we don't have... If input IDs, decoder input IDs is this. And LM labels is what language model labels. So here we have input IDs. The attention mask. Encoder outputs. See our decoder input IDs. That's, I guess, also what we want for the decoder. Provide for sequence to sequence training. Uses the pad token ID as the starting token for decoder input IDs generation. If... Take a look at T5 training. There we go. We could use those extra IDs, right? Unsupervised denoising training. Supervised training. Translate English to German. The house is wonderful. Labels. Return tensors. Easy enough. Easy enough. Thank you for letting me know. So we put our end of sentence here. Then we put... Let's do this. Yep. And our end of sentence here. Input IDs, labels. Output text. PyTorch tensors. Let's remove this one. And we should be good to go. So input IDs and labels. Excellent. Excellent. Where was it? Where was T5 training? Too many tabs open. Supervised training. OK, if we put this into the model now, do we get a loss? That's the question. If we get a loss, everything's fine. Everything's good. So where is our model? Model. Let's call this X because why not? My coding voice is very different. Yeah, because I can't keep up hours of shouting. So what if we put X into the model? That is not cool. OK. What if we put... ...if we unpack X into the model? That is better. Better. Better. Better. What is this? OK, let's call this Y. And let's look at type of Y. Investigate. Sequence to sequence language model output. Thank you. That is very helpful. What can I... Is it a dict? Yes, it is. OK, so we have a loss. We have a loss. I guess that is... ...why do you always wear shades? So people ask the question. Is branding. Is just branding. OK, so we have this. Very cool. Now... So if we do this, we can even see it. Now we can use our data set map. Process data. What do they do here? Map. I guess we can use... ...fatched and we can remove all the columns. No? We can probably remove all the columns. Module object is not subscript. Remove columns. Data sets. Ah, this is it. Mm-hmm. Mm-hmm. It's not happy. It's not very happy. List indices must be integers or slices. OK, we should do batched false. All right. I guess we should also drop... We should probably also drop some of the too long stuff. No? I mean, that's what they do here. They put a max length. Return overflowing tokens. Ah, we don't want that. We simply want the max length, right? So, no. Stop it. Stop it. OK, so tokenizer. See, now is the trouble, right? The max length is what? The max length is what for t5 small? Does anyone know? Let's see, t5. Or can I figure this out somehow? There should be this auto... There's this auto config, no? By hugging face? Can I load this somehow? Somehow. Here, like... There is... auto config from pre-trained t5 small. No? This should exist. Yeah, see? So here we have task-specific parameters. Model type, number of positions, number of layers, task-specific parameters, vocab size, translation, summarization, max length. Is this, though... Is this the max length of the encoder or the decoder? I mean, here it's 512. We can do the following. We can do... We can do max length here to something... I mean, what is... What is 512? Let's say minus 128. OK, and then... And then here we can do 128. Padding equals... How about that? So we pad everything max length, min length. 230. Yeah, thanks. Thank you. So... We'll see. We'll make everything such that it is at max 512 long. We can still change these things. We should probably make some variables somewhere, but... Yeah. So, good. Now that the dataset is mapping, what can we do? I guess all we have to do is give it to the trainer. No? Hugging face trainer. Mm-hmm. Compute loss, yeah, and then... Do we have examples right here? Sec2SecTrainer. Can we just get a trainer or do we need the specific one? Evaluate? No. Predict? No. Training arguments. Ooh, that is a... Model. We put in model. Yep. Train dataset, evaluate dataset. Yeah, this should be doable, no? Here, trainer, we just give it... This, and we're good. 384. Really? Hmm. We could make it even smaller, given that it's going to take a while. Also... We could just do this. Let's just cancel for now. How long does it take? What? But they did it. They did it up here. They did it here, no? They made this smaller dataset. Um... What do I need to do? No? Here. Here is a dataset. Okay, I guess they just took the training one. Like this. Okay. Excellent. So now we have a dataset and we should just be able to give this to a trainer. Yes, yes. Okay, so let's get some of them. Training arguments. Good. And let's also get some of them. Trainer itself. Good. Yeah, okay. That's where the 384 comes from. Thank you. Okay, model is model. This is that. And I guess... Okay, so... And... This and that. Good. So here we have... Train dataset and eval dataset. This seems to work so far. This is more like magic. How... Where is all of this... Like... Okay. There seems to be so much magic going on and this is... Okay, I see. Finally, finally we get something. Something that crashes. So... What did we do? Input shape. Too many values to unpack in the input shape. So where are we going wrong? Too big context? Nah. We have too many... We have too many things. So here we might be... We might want to inspect this X a little more. So these are our input IDs, right? So... Input IDs are right here. And if we do the shape of them, we get two numbers. Now this is complaining that if it looks at the input shape, right? So if we go to T5 modeling... Wow! This opens in a Colab directly. This is crazy. Okay, if we go to the input shape... This is the input embeddings. No, that's not true. See the input IDs... Oopsie. Dot size. Well, what is it? Aren't you going to tell me? Too many values to unpack? Well, what is it? So... Maybe we need to batch this ourselves somehow, right? Because... Here, as you can see, the shape... Where was it? Is 1 and 384. So this already assumes that it's kind of batched. And... I might be completely wrong right here. Unpack is about the input shape being only one object. Try printing the shape. Yeah. I can print. Can I print in here? But it seems the problem is the batching, no? So... Does this work? This seems weird. Like if this works, this is... This is magic. I've not developed in a Colab in so long. No, right? This does not work. So if... All changes saved, yeah? Ah, okay. Now it says too many values to... Yeah, it did not load, reload this thing. Can I... Sorry. Click the six frames. Yep. Well... It just tells me where it's happening. Which I know, right? I just... Compute loss. Models. Unpack the inputs. That's fine. I think it's the... The collate function is wrong. It assumes that what comes out of the data set is a single example. So it probably batches it together. Need to use auto reload too. Thank you. Let's try. Auto reload. Magic function not found. Too bad. It probably assumes that. So what we could do is we could... Simply tell it that this is not a batched thing. Right. I found at least two devices. Great. Now we're... Okay. Process data. Ah... How about that? No attribute detach. Yeah. Okay. Sounds good. We'll try again. Too many values to unpack. Still no. Yeah, it still thinks it's there. But it doesn't matter. We have bigger problems. We have bigger problems. Look at this thing. Now where was it? Here. Right. So we need to... Do we really need to get back... Which ones are on? You need to run auto reload to afterload. Okay. Okay. Okay. Okay. Okay. So see, I'm so unfamiliar with Colabs because I've just been using my Vim in my terminal. That I have no clue about the magic functions. Okay, so... There. Yeah, there you go. Exactly. So this is what I meant. It batches it to eight. It assumes that what comes out of the data set is already a batch. And therefore, it doesn't get it. So either we somehow tell the tokenizer or something that... Or we tell the collate function. Collater default. We'll batch our processed examples together. Let's see. There's the default one. Label IDs handles a list of values per object. Maybe this one, data collater for SICK to tokenizer. Dynamically pad the input as well as the labels. Maybe this one's good for us. So let's try this one. Yes, I'll make a video about my Vim config. Okay, all tensors need to be on the same device. Found at least two devices. How can I... Where is my data set? So how about... I look at... What's X? What is X.inputIDs? What's the device of that? CPU. Okay. So how do I put that on? And X is what? X is just... X is a data point. Now... Can I just tell HoggingFace to... Or do I have to actually say... Did the model have a device? Okay. So now what we'll have to do is we'll have to input IDs equals X input IDs to model device and labels equals X labels to model device. Yeah, okay. So now we got sort of the same... We got the same problem here. So we don't want to necessarily do this. We want to output this because after a while we could also overflow and then output several... Could we put several different ones? No, I guess we can continue doing this. And then here we'll just have to... Do something like that or expand. We'll just go none. Do a little unsqueeze. Nice. Okay. So now we still... Oh no. Now we have... We need to remove the print statement again, otherwise we'll just be bombarded by print statements, right? Okay, let's do this. Let's do that. Please save. Thank you. Okay. Good. Problem solved. Happy holiday. Thank you. Happy holiday to you too. Good. We're at the next error. Expected a sequence length of 384 at dimension 1. Got 517. What? What? We even limited it to... We limited this. Got 512 data collater. We're here. Label. Do we have label or label IDs? We should have label IDs, no? Because... Yeah. We should have label IDs. That seems reasonable. Still the same. Okay. So the problem is that here... What's f in features? What's features? What is it? List of dict f of k. So what's k? If k is not in label and v is not none... And here we say expected a sequence of length. Got... Okay. We can do this and we can print the length of that and let's see what happens. 384, 384, 384. And then we get like a long one. Why is it not respecting the max length? So this one... Let's see. Why is it not respecting the max length? Hmm... Yes, max length. Pad to max length. Max length. I'm just searching for this now. In all the things I have open. Yeah, I should be able to truncation true. Is that a thing? Okay. Let's see. Auto models. Tokenizer. Pre-trained tokenizer. Can I click on it? Please. Yes. No. This is version 2. Sure. There we go. Max length. It's called model max length, really. Truncation. Defaults to false. Well, that explains something. Let's in this case do true. How about now? 384, 384, 384, 384. This looks better. Okay. Now we just need to remove the print statements. Otherwise the file will be gone. Cool. Okay, so this isn't too long. This isn't five hours like before. So hopefully, hopefully something cool will happen. Truncation. You throw away everything after the first 384 tokens. Is that on purpose? Yes. We can still refine like it. Keep these tokens that it's handled in the... Yeah, the tokenizer can sort of split and, and... Tokenizer can stride over the document. But in first instance, I'm just happy with getting any sort of result. Can still be refined later, but we just want to train anything at all. Well, this is the fun part. So I think people sometimes ask, you know, about streaming and machine learning. And, you know, streaming video games is sort of the most like super entertaining, right? Because it's like stuff is happening. And then streaming coding is still okay if you can talk and code and read chat and so on at the same time. But then streaming machine learning is just sort of... Even if we make the data sets deliberately small like we did here, it is just... It is, it is... We just have to wait. And then what do we do during waiting? Could you show line data set equals? Data set equals... Well, we have data set is simply loading it here from... We simply load the squat data set. So that's kind of trivial. But then we have this mapping function here that tokenizes the text and puts it into sort of this framework context and answer. And we want to output the question. What do you do during wait normally? I watch... I love watching loading bars and screens. But usually I... I don't know. I code something else. I guess there is something else to code, which is... I'm making like a super... Sure, doing an AMA. Am I not doing an AMA during all of the live stream Swordfight? Yes, the XKCD meme. So I see if I have enough GPU left. That's a tight rope, no? Try to stream some reinforcement learning. Watch an agent not really working. Yeah, if you get the visualization set up correctly, that could be something. Also like smaller problems, right? Like way smaller problems is also something. This here I sort of wanted to try. And oh, we can look at our... We can look at the... This here, it should be... Yeah, yeah. Look at this. At least the CPU utilization is going on. That is something that's happening. We don't have metrics yet. We might want to log more intermittently. Training completed. Training completed. Excellent. We have zero logging. We did log, right? It said it logs to... It logs to report to... Didn't it say it would log to weights and biases? Did we even log in? WondB. We didn't log in. We didn't log in. Oh, we did. Somewhere. At some point, maybe. Am I planning to do the ML year recap video? Did I announce an ML year recap? I don't know. Alpha fold happened or something. Was it this year or last year? I have no idea. The paper of the year. No idea. It's really hard to predict what paper is going to have a big impact. For example, the foundation models paper. It can either have a really big impact or it can just be forgotten by everyone. There's no way to tell right now. What do you think about the recent hype in graph neural nets? I've not looked super much into graph neural nets. It seems they're pretty cool, but they also seem to be quite limited right now because they're essentially RNNs on a dynamic graph. Check the report. Yeah, so WondB has a trainer. So there should be it should be possible. Here, report to WondB. That's fine. With the training arguments. Good stuff. We might want to restart and run all. OK, now once the model has trained, we can then let it generate some stuff. I really don't think that it will produce something interesting right now. OK, good stuff. Reusing the data set. That's what we want. OK, here. Yes. This one. OK, as this is going to come in, we can already think about what we want to do. So what we want to do is we want to simply take the model that we have and let it generate something again. So let's just try. Let's just try this again. I think. No. No, like. Try this. And where was the generate? Here. And then we do batch decode. There we go. Aren't graph neural networks just a year behind NLP or attention? I'm not sure what a year behind. You mean like in a year they're going to be as popular? It could be. I've also seen like there's also a big history of such things just flopping like there being a lot of hype and then not much. It depends. It depends if sort of large breakthroughs, I guess, are being made in relevant domains. That's usually what fuels these things like transformers. I think BERT has transformers have existed. And I remember I remember people sort of the sentiment was, yeah, transformers for language, they kind of work, but they use so much memory and they're harder to train and so on. And people would still sort of be on the be on the fence and say we'd rather use LSTMs with attention. And I think only the advent of BERT really say it wasn't even the attention is all you need paper. It was the BERT paper that really fueled the transformer hype. So if there is something similar for graph neural networks in a relevant domain, so either in a domain that exists like but I don't I don't really see graph neural networks being that useful in vision or or NLP or something. So I'm not entirely sure. They are probably super useful in biology. But it's not like biology is that it's just not that accessible to most people. And they don't want to work in biology because if I'm good at like if I do something in NLP, like I can build a little app and so on. It's very tangible and very useful for most people. And same with vision. But with something like biological data, I mean, yeah, sure, I can download some bio data from from online. But then, you know what? What am I going to do with it? Are you familiar with PINNS? No, I'm not actually. Socks, 1 dB socks giveaway. Well, if you if I'm I don't know, I should wear them probably. Yeah. Sorry, I'm trying to keep up with chat. Most interesting meta learning paper. I'm not I'm not super I don't have a maybe not anyone, but I don't have a good overview over meta learning. According to a Luther graph, neural networks are largely NLP research from 2019 and 2020. If you want to achieve Soda, you'd have to use 2020. So you're saying that current NLP methods beat the graph neural network methods. So when people in the past said, hey, these graph neural networks are actually pretty good. Then all you had to do essentially is wait a year and then the field would have sort of caught up. I guess I hope that's what you're saying. OK, so now we're sort of kind of a bit ready. No. Yes. So here, let's say. Yes, we can generate a context. Hi. The dog was loud and. The answer is dog. Right. And then end of sentence. OK, and then we have. Yes. Does that does that work? OK, so input IDs and then we'll put them to. Device. Like this, like this. How about this? Ha ha. You have to. Either decoder input IDs. OK, so. And code, let's encode just the. I see. Ha. What am I going to put here? How do I how do I have T5 answer a question then? No. How? Will there be Yannick merge sometime soon? Yes. Yes. There is. There's merge. It's I mean, I'm still I'm still fleshing out the merge, right? I'm I'm still this is a changing thing. If you I guess you can have it today. But I'm just telling you this is it's going to change. So if you're into super rare stuff, maybe it's going to be deleted soon. It's also going to be augmented. But let me. Let me show you. Yeah, OK. It's up. So but as I said, this is subject to change. And and so on. But if you so you can either go here or you can go the URL is. Store.YKilcher.com. That's it. That should be a redirect to the it's a Teespring store. So and it's going to be integrated with super rare, super rare in the sense that I might decide something is just too dumb of an item and just remove it again because I need to thin it out a little bit. But, you know, no one stops you. I'm just taking like if you if you really let's say I remove this T-shirt, right? No one stops you from just taking the channel logo and slapping it on a T-shirt yourself. It's going to be literally exactly the same thing. So, you know, the rarity is the rarity is is only because people are usually too lazy to do that. So, yeah, I'm going to probably thin this out a little bit. But I've ordered some samples and I think I'll announce it once I have the samples ready. There's also one. So it's usually it's the same like it's the channel logo on various items. It's sort of the the channel model, which I don't usually highlight, but skill greater than destiny is kind of the I don't know. It's been sort of a battle cry since I was young. It yeah. It sort of it sort of says that you can if you if you dedicate yourself, you can overcome sort of the maybe the hurdles of life, whatever. It's a bit cheesy, but actually doesn't mean like if you take it literally, it doesn't even make sense. And because both are predetermined, like your skill is maybe predetermined. Sure, you can learn something, but and then your destiny is like ultimately predetermined. So if there is destiny, you can't change it. But maybe the paradoxical nature of it is what makes it good and appealing. And obviously there is the attention for you if you're sewing, if you enjoy that. And yeah, I think that's I'm thinking of a technology good technology, bad technology biased motif as well. But that's that if you want to want to do this. But as I said, it's not announced yet. It's going to change. And there's also there's there's more stuff coming out. In any case, let's like how do we how do we actually generate stuff from this? Like, okay, we have a language modeling head here. But what do we have here? We have an auto model also with with a language modeling head. Huh? So we should we should provide some inputs. So let's say, okay, how about we make our data set such that it always starts with a Q, right? Like this. And then we could put here we could put like Q. I'm not making NFTs. No, no, no. Unless they actually become more useful. I'm not I'm not going to make NFTs. Don't worry about that. So we got decoder input IDs. Yes, outputs. Wait, that's what that's what we that's what we did. Oh, OK. Oh, no, wait, is it really? Could we do that? Aha. OK. Well, good. Yes. Most definitely. Totally. It's a good model so far. OK. How do I the hugging face trainer? You know, how do I control its its logging? How do I control it? Log level, log level replica. Like how often how often it logs. Yeah, yeah. The logging frequency. Yes, that's what I want. Frequency. Hey, where is it? Can't find it. Where is it? OK, no. Logging steps. Does this exist? There we go. Logging steps. Number of update steps between two logs if logging strategy equals steps. OK. So this is steps. And we'll just set it to. Is this a trainer or is this the training arguments? Training arguments. OK. So in our training arguments. Wait, we might want to reprocess the data set, right? And here now let's do this because. Oh no, it's being tokenized like that. Sorry, sorry. Uh huh. OK. Training arguments, logging steps 10. Train. Yes, please. And now we should finally see something. Here. Hopefully, please. Runs. This is green, so this means it's going. Yes. Yay. And we are decreasing in loss. Excellent. Oh wait, I need to catch up in chat. For how long will you stream? Not for too much longer. So maybe last year during I remember during Christmas. You know, everything's kinda. And still during Christmas, right? And I remember PewDiePie streaming and I thought, oh, that's neat because, you know, nothing else is going on. So and. Yeah, so I thought I'd do the same thing and maybe someone. Well, appreciate it. Or just be sort of mildly entertained while they search for other content. Will it be delivered all over the planet? The merch? I think so. So I've deliberately I first wanted to go with a different like with what was it? Don't remember the other one, but there was another one that could be integrated with YouTube, but they are not shipping to India right now, which is kind of I thought kind of sucked. So I think the Teespring should be able to ship anywhere. Don't feed in the decoder input IDs manually. It makes the outputs go weird. Well, it won't let me do anything else. Yeah, so I have not done Hugging Face training in a long time from scratch. But this is just the first the first instance. So we should be. Yay. Yeah, we're we're nicely decreasing. No, not towards the end here. Is this one epoch? Three epochs. OK. Batch size of eight. Yeah, that works out. 125 per epoch. And then we'll see what happens. In 35 seconds. Cool. Dun dun dun dun dun. Do not forget to share your model. I don't think this model is necessarily the best to be shared. OK, so we still. We're still kind of. Yep. So the model isn't the best right now. What data set are using its squad. So we're using squad. But I think this would be sort of the process. A sort of the process to get this started. Right. I mean, what if I just don't do anything here? What does it do? It just gives me like an end of end of sentence immediately. Is this to be expected to it? Does the. Does this even need an end of sentence? Not sure. So the question does does the model is the model initializing. Like if I reset the model like like this. Then. OK, this is a lot of output. If I do that and then train again, what happens? Yeah, OK. No, I don't want to learn about the newest weights and biases updates because. I make ads for them. I should know about the newest features and updates. So this is better right now because before we just kept retraining the same model, right? And not reinitialize it. And now at least we're at least we're reinitializing it. See, this was like our global step was. We just kept resetting the same model. But now now. You see. Do we have is this the is the one that we currently have? No. We'll see in a bit once it sinks. But even if not, you can see here loss is going down quite nicely. Anyone know if one to be self-hosting is fairly seamless training on a local GPU server to watch progress. So the I think that like I might be wrong, but the self-hosting for one to be is is mostly a thing you do as a as a company. It's not it's not the same as like. It's not like, you know, here is our open source tool. And, you know, we also host it for you if you want. They sort of take the different approach. It's it's cloud first. And if you are if you're a business and you say, well, I really need the data to stay in my in my in my data centers in my own environment, then they'll give they'll they'll give you sort of a self-hosted environment. I don't know how easy it is, though, to get to one. If you just say, well, I'm an individual and I'm just. Maybe. Yeah. I don't know, but. I guess you can ask them. So soon we should have this done. And then we can see then we'll have a model that's kind of trained from scratch. That shouldn't be too bad. No. This is for research you're working on just trying out something. This is just trying out something I haven't done. I haven't actually done NLP coding in a while. So and. I just randomly wanted to. Or I just randomly thought that I might might as well live stream it. You know, maybe we're not we're not doing a whole lot here. We'll see. We'll see. So we got three hundred and seventy five steps. We're almost done. Do we get some metrics here in the meantime? Not yet. Maybe I need to give it a different name, though. Right. I think. Like because I'm always logging to the same name. That can't be too good. What am I working on right now? I've co-founded a startup, an NLP startup, ironically. Right. Which means that I work with a lot of smart people that all also know NLP. Therefore, I don't get to code NLP anymore. I get to code. I get to code mostly plumbing. OK, so. So as you can see, let me. Let me train. Yeah, let me get a data point from here and we'll take this right here. And we'll we'll make our. Prepare data. Of this one. How do we call it? Process data, of course. Is there any model for transformers that don't need N squared paper? No, but they do have pseudo code, I believe, in the paper itself. How does your day look like? It's a wild mess of whatever burns the most. It's I do not advise. I do not advise following my daily routine. OK, so let's call this X1. And let's input that into the model. So X1 input IDs. Not enough values to unpack. Where? How? Why? No. What? Come on. Come on, man. We'll do this. List. It needs to be a tensor. So. That. Beautiful. Beautiful. Uh. Uh. Uh. Our model is terrible. How and why? Right. It just. Baffles me. What if we don't train it at all? Like, what if what if we just load the model? And we'll just input this. How many steps do you usually train for an image classification task or an image reconstruction? Do you smaller or large batch sizes? I don't I don't have a general recipe. You just go with whatever is is appropriate. And it also depends really on the task. So for unsupervised learning, people do hundreds of epochs for ImageNet. I guess you can get away with a bit less. I think like 50 epochs or something like this. But also for. Yeah, it's it's I know C410 or so people use 50 to 250 epochs, which is like a lot. But then heavy augmentation. It's really depends fully on the task. Tell us about your startup. Sure. It's called Deep Judge. It's very cliche name. And we do NLP for the legal domain. And if you're allowed to work in the European Union, no. Well, if you if you are allowed to work in Switzerland and you want to work in Switzerland and your skills in engineering with an affinity for machine learning, you may you can happily apply. That is so. Yes. Let's see if Google finds us. Yes, of course, Google finds us. OK. That is us. Me wondering what I missed. You didn't miss anything. It's just me. It's just me dicking around in this. OK, so it seems like we haven't trained anything. If or I'm just I'm just doing this completely wrong. I might want to look up some more tutorials. I've just jumped into this completely. But what you can what we can do, what we can do is you can see that the this X1 that we have right here. So the first data point and this is usually a good thing to do if you wonder if your model works at all. So we take the training data set right. We look at this data point and the context is whatever architecturally the school has a Catholic character. The question is to whom did the Virgin Mary allegedly appear in da da da da da. And the answer is St. Bernadette. Whatever. Now, what we can do is we can completely overfit on that one data point. That that is that is what we're going to do. And for that, let's restart the runtime first and foremost. Then we'll run this these things. Run this. And when it comes to the data set, we are going to. So process data right here. And then when it comes to the data set, we're just going to select one data point. Right. We'll call that overfit data set. I'm going to take the training data point. I want to just get one single data point out of it. We won't even we won't even touch those right here. Data set is not defined. Of course, why would it be? Because I need to run this. OK, so da da. OK. So we'll get our overfit data set and then we'll just we'll just train on that overfit data set. And how where is the number of epochs? Oh, num train epochs is three. We'll just put this to. Um, num train epochs is, I don't know, like a hundred or so. We'll just overfit on this one data point heavily. And instead of the training data set being the training data set, we'll make train data set is overfit data set and eval data set. Is also the overfit data set. Overfit data set is not defined. That set. I see. So if this works out. Image net is 100 to 600 epochs. 50 isn't really possible. OK. Yeah. If your judge model doesn't work, claim it is a company secret. Training, wait, already completed? This is not, this is, that is shady. OK, let's try. See, OK, we're doing, I'm doing something terribly wrong. Or we'll just put the number of epochs to 10,000. Do we have output? Please. Do I need to rename? I should probably rename, right? Ah, see, OK, this is the output. Now we're continuing. Yeah, we're overfitting nicely. We tested out the extreme overfitting approach leading to super convergence. You talked a while back. You mean the grokking? Yeah, I think this will be a different, different domain right here, but I haven't, I haven't actually done or investigated grokking any time myself. It seems super interesting, though. Yeah. Change the name of 1db thing. Yeah, I should. I see no problem with training equals zero. Are you going to do a more classic paper review? Yes. Yeah, this is, this is also planned. The classics. Why do you look like Bezos? It's a, whenever the hair gets long, longer, it just gets on my nerves. Are you loading all the data? I am, but right now I'm just loading a single data point. So. Remove the end of sentence token. True, that could be. But this isn't this part of the. I've just seen this on like one example. I might be wrong. A great masculine jaw. What's the secret of the glasses? There's no, there's no secret. It's just that it was a, it was a maybe bad branding decision. And now it's I'm stuck with them. So. So see, but now you can you can see that we are, we are converging in. Yeah, look at that. I mean. What is what is this? Do we? We don't have validation stuff. OK, but training total. Training runtime. This one right here. I mean, look at that. That is just very low. Very, very small loss. So we got a thousand, a thousand steps. We might as well sort of interrupt. What is this clear output? No, we might as well. Interrupt a little bit. Hey, would you? Would you mind stopping? Just interrupting a bit. Stop. OK. And we'll just check the output. Maybe it's something better now. Ahh, still no, still no. Hmm. Are we doing something wrong? Unk, unk, unk, unk, unk, unk, unk. Am I doing something wrong? I'm probably doing something wrong. Are you hiring? Yes, we're hiring. But you need to be able to work in Switzerland, which is, it sucks and it's not available to most of the world because of legal reasons. But we have no choice really. So you need to be able to get a working permission. And yes, this still sucks, even though we've completely overfit, which means that we're, I'm doing something quite wrong. The question is what? What's so wrong? I may not use this in the exact correct way right here. So I could sample, I guess. So do sample is. But the argmax, it should be good if we overfit. The problem is that we might just remove these end of sentence tokens, right? Because like why? Let's restart. And this is the last try. If this doesn't work, we'll just take the loss. So we'll remove the end of sentence tokens like this. And let's hope the tokenizer puts them in itself. Like that. Good. And we'll do the 1000 epochs again. And there we go. OK, last run for today. Wrong input, yeah. Yeah, we can inspect train data set entries after process data play. That's a good idea. We should do that as well. I don't understand the meaning of the output. Can anyone explain? No, no one can explain. That's it's it's it's not the desired output. We're trying to predict text. And right now we're just it's garbage. But what we're trying to do is overfit. Let's see. I could change this. Also. Overfitting on one data point. We're just trying. We're just a bunch of researchers giving our best. Your input is from the full data set. But I mean, but I take I just select the first entry like I do select. I selected this entry. You're predicting the text from your full data set, but not from overfitting data set. Yeah, but the overfitting data set is taking. It's taking the. It's taking the first data point, right? The overfit data set selects exactly the first data set. The first data point. So it selects the zero of the train data set. So this should be the same. I can I can try to just input. I could try to input. Yeah, I see. Is there a reason you're overfitting? Yes, it's because it's a good way to see if you don't have like it's a good way to catch bugs. So what you want to do is you want to take a single data point and then overfit to it. And if you can do that, it means that your model is OK or well, it means your model can at least process data. That's essentially what it means. But for us right now. So let's let's call that X2. Overfit data set zero. Right. So X2 is now. It's a whole lot lot of zeros. Zero zero zero zero. No, it's actually it's actually OK. It's more than zeros. We can decode it. Tokenizer decode X2 input IDs. Let's see if that. Yeah, architecturally, the school has a Catholic character. Right. This is fine. This is and the answer, St. Bernadette. We might. Oh, this is here. Good. So this is done automatically. We don't have to do this. And then the pad. Maybe we'll leave away the pad because right now we're padding. Right. And we technically don't need to pad or we also grab the mask. Yep. Maybe the masking is is the problem and label IDs. Yeah, this is fine. And if we input now this into. Let's input this. So we'll take X2 right here. And the decoder IDs will input nothing. This has no attribute to. What was X2 again? This is. Just a list. We need to. OK. Can we do this? I'm just I'm just hoping here. Nope. No attribute shape. Yeah, we should. We should put it into a tensor. How to make a tensor from this, do I need to import Pytorch? Yeah, probably. Yep. So. OK. How? OK, does anyone know how to make an int tensor? Like this? Long. Yep. OK. Yeah, so the output seems to be correct, but we can't overfit. So I have. OK, let's say we have a last trick up our sleeves and that is we remove the padding. So the model will just have to. Take whatever we have it. So the padding. We don't pad. We just truncate. Dun dun dun. Alright. OK. How's it coming in? Excellent. Loss is decreasing. That is a good sign. And it's going to take another. 90 seconds. Does anyone know how Yannick deletes a chunk of text with one button? It's probably you mean like the entire line like this? It's a. Oh, no. It's a Vim command. It's capital S. Tim is calling me. I should. I should leave soon. We're making we're making the the final argument, the final things for the next machine learning street talk. Alright. Done soon. You mentioned me as an inspiration for your master's application to ETH. Nice. But I don't I don't think human resources is too happy with me. Given that they've received calls in the past about my behavior. So maybe maybe don't don't mention that too much. OK. OK. I mean, you know, no to sample through. Some things, some things wrong. Like something's clearly wrong. I don't know what, but I'm doing something wrong with the with the inputs right here. Because it's. Yeah. So I'm not sure what exactly what exactly they are. What exactly they need. But here fine tuning T5. A mixture of supervised. So here in supervised training. Right. That's what they do. They have input IDs. They have labels. I have labels and not label IDs. The function automatically creates the correct decoder input IDs. But you see, you see this. It seems weird because I'm not putting it and they. Yes, something is something is definitely wrong. So labels. Let's call it labels. Right. Oh, no. I was calling it labels all along. Label IDs, labels, label IDs, labels. Labels. Labels. Well. Decoder IDs. It's worth a try. I mean. We might just try it until here. Might be the parameter where you can auto add the end token. Yes. Yeah. True. Yeah. Here you generate. So. Let's try. And instead of. Yeah. Here I have labels too. Oh, that's what I just changed a second ago. I see. Sorry for being dumb. No, but here labels. Right. It's already running. OK. OK. What's your opinion on Jax? Well, given that it has a built in. If I understand, I have not looked into Jax, but if I understand correctly as a built in XLA compiler, which is really good if you want to do super custom stuff in PyTorch. Sorry on TPUs, which I don't. But I as far as I've seen so far, I do like sort of the. It's much more clean. So, you know, maybe if they put a bit more attention also on common usability, not just with TPUs. I guess it might be a cool language. Any good suggestion for a master's in NLP? I have no idea. Sorry. I've chosen my university because it was the only one in the country that was like it was that was that was a had the things that I wanted to or that I was interested in vaguely. See, got an unexpected labels. Why? OK. How? No. Decoder IDs. OK. OK. All right. Maybe now. Labels. Yeah, no. What was it before? Decoder IDs, right? Decoder input IDs. That's better. That is better. No, you don't think so. Catholic Catholic. And what did we want to whom did the Virgin Mary allegedly appear? It's better than before. No. A two. Yeah. We say do sample. True. Now it just evolves again. I might be imagining that this is better than before, but it has the end of in the main main of the main building. I might be imagining that this is better. Maybe also because we trained it for not as long. Oh, look at that. Look at that. Look at that. Yeah. OK. Decoder IDs, whatever that means. But if I don't do decoder input IDs, but decoder IDs, I get the correct question. So we can, in fact, overfit on a single data point. See, this is the output. It is the output. Yes. It's padded to the left for whatever reason. But it is the output. And what if I put like a cue here? Yeah. Excellent. So it turns out that the most challenging part about doing machine learning is using the correct keyword arguments in the hugging face library. Coincidentally, this would be something where like Copilot would be a killer at. I've been I've been using this for a while now. And it's this kind of stuff. It's really good. So, yes. So that was essentially this was where I wanted to get to today. We have a model that can take in context answer pairs and give us questions. The next step is to take this and make this train it on SQuAD, then generate a data set of more questions and answers. And then we can use that to train another question answering model. And at the end of it, that one should be better than our original one that we just trained on the data set alone, which is a little bit magic because we don't introduce new data. Right. We don't. But what we do, what we can do is, you know, we always need we need new question answer pairs. So the input to this model is going to be a context and an answer. And we let it produce a question. Now, the context we just take from the same data set. What is the answer? That is that is the question. Well, ironically, that is the question. So what we can do is we can use something like a maybe like a sequence tagger, maybe from from space here. So that recognizes nouns or noun phrases in the context. And then we'll just take these noun phrases as possible answers. So what these papers this is not my idea, right? These are papers that that are in question answering. What they do is they would find noun phrases. They would let this model that they've trained to produce questions, produce a question given the context and this possible answer. Then they use the original question answering model to check whether it actually gets it right. And that's how they generate new data. That's essentially what we're going to do. But yeah, this was it for for today's live stream. It took it took a bit longer, but it's good to refresh myself as well, because as I said, I've been doing mostly plumbing coding and not machine learning coding. So this is fun. And I hope I hope at least someone someone learned about the trick of overfitting on a single data point. Because I remember when I first when first someone told this to me, I was like, oh, wow, this is this is really helpful. And I'm sure most people know it by now, but it's still super helpful. All right. So in this case, thank you very much to everyone who's who's been here. I need to hop on a call with with Tim and Keith for the street talk. And yeah, I'll see you around. ```\\n\\nQuestion:\\nHow to build next-level Q&A with OpenAI\\n\\nAnswer:\\n\\nSources:\\n<title>: <url>\\n\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "message = \"\"\"\n",
        "Answer the question based on the context delimited by triple backticks and show answer sources with their titles and urls.\n",
        "\n",
        "Context:\n",
        "```{context}```\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\n",
        "Sources:\n",
        "<title>: <url>\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages=[('system', 'You are a helpful assistant that always answers questions.'), ('human', message)])\n",
        "print(prompt_template.invoke({'context': retrieved_texts[0], 'question': query_text}).messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHmFnVZH8-hS"
      },
      "source": [
        "# Creating a Question-answering RAG chatbot by chain rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "d1_3BjdRz0uU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {'context': retriever, 'question': RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSFTWBIg26jG",
        "outputId": "2dfb90d7-c0f5-4c9f-9ebe-a8813fa9562d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To build a next-level Q&A system with OpenAI, you can follow these steps:\n",
            "\n",
            "1. **Define the scope and goals**: Determine what kind of questions you want to answer and what kind of information you want to provide.\n",
            "2. **Choose a model**: Select a suitable OpenAI model, such as a language model or a question-answering model, depending on your specific needs.\n",
            "3. **Prepare the data**: Collect and preprocess the data that will be used to train and fine-tune the model.\n",
            "4. **Fine-tune the model**: Use the collected data to fine-tune the model and improve its performance on your specific task.\n",
            "5. **Integrate with a user interface**: Create a user-friendly interface that allows users to interact with the model and receive answers to their questions.\n",
            "6. **Test and evaluate**: Test the system and evaluate its performance to identify areas for improvement.\n",
            "7. **Continuously update and refine**: Continuously update and refine the system to improve its performance and accuracy.\n",
            "\n",
            "Some popular OpenAI models for building Q&A systems include:\n",
            "\n",
            "* **Language models**: Such as BERT, RoBERTa, and XLNet, which can be fine-tuned for question-answering tasks.\n",
            "* **Question-answering models**: Such as OpenAI's Question-Answering model, which is specifically designed for answering questions.\n",
            "\n",
            "Additionally, you can use techniques such as:\n",
            "\n",
            "* **Knowledge graph embedding**: To represent knowledge in a graph structure and improve the model's ability to reason and answer questions.\n",
            "* **Multi-task learning**: To train the model on multiple tasks simultaneously and improve its overall performance.\n",
            "* **Active learning**: To select the most informative samples for human annotation and improve the model's performance with limited labeled data.\n",
            "\n",
            "By following these steps and using the right techniques and models, you can build a next-level Q&A system with OpenAI that provides accurate and informative answers to users' questions.\n",
            "\n",
            "Sources:\n",
            "* Machine Learning Holiday Live Stream: https://youtu.be/WvTU-y8HyDg\n",
            "* Streamlit for ML #2 - ML Models and APIs: https://youtu.be/U0EoaFFGyTg\n",
            "* How-to Structure a Q&A ML App: https://youtu.be/4Jmq28RQ3hU\n"
          ]
        }
      ],
      "source": [
        "print(rag_chain.invoke(query_text).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GoAtl3u3ZCk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
